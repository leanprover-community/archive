[
    {
        "content": "<p>I'm interested in playing around with developing some Lean models (low budget, so not much chance of interesting success at this stage) and would like to avoid putting too much work into data extraction if it's already been done. A premade dataset would be ideal. I'm aware of the LeanDojo dataset and the MLFMF dataset - which looks a bit harder to use, but maybe more flexible. Are there any others which I've missed?</p>\n<p>I'm also curious whether any significant effort has been put into data augmentation. I'm not a Lean expert (nor a mathematician, I'm very much just an interested amateur here) but it seems to me that there are extensive opportunities to expand the size of these datasets through equivalence preserving transformations.</p>\n<p>For a trivial example, <code>rw [dropSlice_eq, drop_drop, add_comm]</code> can be split into separate <code>rw</code>s. Other examples might include:</p>\n<ul>\n<li>Rewrites (and similar) could be reordered when there's no dependency between them.</li>\n<li>Expressions in the context could be put into equivalent forms (and then a step inserted into the proof to undo this)</li>\n<li>Context statements could be reordered (as far as permitted by dependencies)</li>\n<li>Tactics which can be \"inlined\" into invocations of other tactics could be inlined in the dataset</li>\n</ul>\n<p>I'm sure I'm not the first person to think of any of this, but I'm wondering what the state of building a reusable high quality dataset with support for these types of augmentations is? In theory, this should be able to go some way towards mitigating the small dataset issues.</p>\n<p>I'm also interested in versions of proofs in which some or all premises are anonymised as a form of augmentation.</p>",
        "id": 406915979,
        "sender_full_name": "Sam",
        "timestamp": 1702115237
    },
    {
        "content": "<p>I should point out that although simple augmentations like the <code>rw</code> one could be done via string manipulation, to be most useful we would probably want the dataset to include contexts, so even these simple augmentations would need to be done within Lean itself.</p>",
        "id": 406916860,
        "sender_full_name": "Sam",
        "timestamp": 1702115505
    },
    {
        "content": "<p>It would also be interesting to have a dataset which includes term expansions for each tactic invocation. At the very least, I'd like to try training tactic expansion as an auxiliary learning task.</p>",
        "id": 406917372,
        "sender_full_name": "Sam",
        "timestamp": 1702115716
    },
    {
        "content": "<p>Some of your first questions about available tools and datasets might be answered in <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/How.20to.20run.20a.20simple.20ATP.20baseline.20for.20Lean\">#Machine Learning for Theorem Proving &gt; How to run a simple ATP baseline for Lean</a> but honestly this is moving so fast that I don’t know what all the available tools and datasets are for Lean 4, and how easy they are to work with.</p>",
        "id": 406932504,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702126917
    },
    {
        "content": "<p>As for tactic decomposition, this used to be more common especially in the pre-transformer works like <a href=\"https://arxiv.org/abs/1905.09381\">CoqGym</a> and <a href=\"https://proverbot9001.ucsd.edu\">ProverBot9001</a>.  The idea is that if you are not using a pure language model solution to generate tactics, then you need some sort of explicit list of tactics, and having fewer might be better.</p>",
        "id": 406932800,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702127071
    },
    {
        "content": "<p>For some other data augmentation see the <a href=\"https://arxiv.org/abs/2102.06203\">PACT</a> paper for some ideas, but I agree a lot more can be done in this space.</p>",
        "id": 406932863,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702127134
    },
    {
        "content": "<p>Also maybe see this <a href=\"https://proofassistants.stackexchange.com/q/2586/122\">PA.SE answer</a> which also talks about some of the augmentations you were thinking like anonymizing the goals. The key to making you own augmentations would probably be to learn some Lean metaprogramming through the <a href=\"https://github.com/leanprover-community/lean4-metaprogramming-book\">metaprogramming book</a>.</p>",
        "id": 406933541,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702127658
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>, those all look like useful resources!</p>\n<p>Do you know if there's interest in, or any effort towards, a community maintained dataset project? It seems that most paper releases do this themselves, and I suspect this slows the pace of progress significantly. I know that I personally am less likely to persue this interest in the short term given the amount of work it would take (including probably requiring me to learn Lean metaprogramming to relatively high proficiency) to get my ideal dataset together.</p>\n<p>It would be cool to have one single dataset, along with a standard tool capable of lots of different augmentations.</p>",
        "id": 406934128,
        "sender_full_name": "Sam",
        "timestamp": 1702128111
    },
    {
        "content": "<p>When I briefly looked at the metaprogramming stuff, it seemed like maybe LeanDojo's data extraction pipeline was a good starting point? But I'm not sure yet.</p>",
        "id": 406934218,
        "sender_full_name": "Sam",
        "timestamp": 1702128163
    },
    {
        "content": "<p>It feels like progress in the Lean ML space currently requires quite high proficiency in both Lean and ML, and that's quite a small set intersection. Personally, I lack Lean profiency beyond a superficial level. It would be cool to have the tooling good enough to open this area of research up to the wider ML community.</p>",
        "id": 406934382,
        "sender_full_name": "Sam",
        "timestamp": 1702128319
    },
    {
        "content": "<p>As for proficiently, that is tricky.  I recommend ML and ITP pair up to help with this.</p>",
        "id": 406934469,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702128373
    },
    {
        "content": "<p>As for general purpose tooling, I would also like to see more.  I personally want to play around with the dataset extractor and ML Aesop extensions mentioned in that other thread.  But I also think it is hard to make a one size fits all solution so the best hope is easy to modify code. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 406934644,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702128504
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"428422\">Sam</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406934218\">said</a>:</p>\n<blockquote>\n<p>When I briefly looked at the metaprogramming stuff, it seemed like maybe LeanDojo's data extraction pipeline was a good starting point? But I'm not sure yet.</p>\n</blockquote>\n<p>LeanDojo's data extraction pipeline and <a href=\"https://github.com/semorrison/lean-training-data\">lean-training-data</a> are the best (open-source) references so far.</p>",
        "id": 406934674,
        "sender_full_name": "Utensil Song",
        "timestamp": 1702128530
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"296911\">@Utensil Song</span> Is it easy to modify?  I’m personally trying to figure out whether to try that or lean-training-data , especially if I need to modify the code to return different types of information from the goal states.</p>",
        "id": 406935120,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702128778
    },
    {
        "content": "<p>I guess I should just look at both.</p>",
        "id": 406935149,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702128797
    },
    {
        "content": "<p>(Oh sorry, you mention both.)</p>",
        "id": 406935292,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702128852
    },
    {
        "content": "<p>Ah, lean-training-data looks interesting! That's one that I'd missed. It seems there's some interest in making that a community tool - the readme invites requests for additional features.</p>",
        "id": 406936021,
        "sender_full_name": "Sam",
        "timestamp": 1702129155
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"428422\">Sam</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406915979\">said</a>:</p>\n<blockquote>\n<p>...</p>\n<ul>\n<li>Expressions in the context could be put into equivalent forms (and then a step inserted into the proof to undo this)</li>\n<li>Context statements could be reordered (as far as permitted by dependencies)</li>\n</ul>\n</blockquote>\n<p>A alternative/complementary approach could be to investigate encodings that have the desirable properties like premise order invariance. These may require training exclusively on proofs data, which may be a disadvantage compared to pre-training on all of the internet. Some examples are:</p>\n<ul>\n<li>it seems that isarstep uses an encoding that has premise order invariance baked in (see fig 2 of <a href=\"https://arxiv.org/abs/2006.09265\">https://arxiv.org/abs/2006.09265</a>)</li>\n<li>tree encoding <a href=\"https://www.microsoft.com/en-us/research/publication/novel-positional-encodings-to-enable-tree-based-transformers/\">https://www.microsoft.com/en-us/research/publication/novel-positional-encodings-to-enable-tree-based-transformers/</a></li>\n</ul>",
        "id": 406936395,
        "sender_full_name": "Abhishek Anand",
        "timestamp": 1702129306
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"259905\">@Abhishek Anand</span> Yeah, for those particular examples baking invariances into the models can be a good option, although there are many more available augmentations where this doesn't apply.</p>\n<p>Actually though, I wonder whether baking these invariances in actually works out as being efficient. You get better inductive biases, but you lose the \"extra training data\" from training on all of the permuted version. There's probably a paper out there which answers this question.</p>",
        "id": 406937040,
        "sender_full_name": "Sam",
        "timestamp": 1702129557
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406935120\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"296911\">Utensil Song</span> Is it easy to modify?  I’m personally trying to figure out whether to try that or lean-training-data , especially if I need to modify the code to return different types of information from the goal states.</p>\n</blockquote>\n<p>Implementation-wise, they are very different.</p>\n<p>LeanDojo uses Lean meta-programming to extract everything they possibly care about to a trace IR, then processes the IR in Python, and works with other data pipeline tools.  lean-training-data focuses on using pure Lean meta-programming to directly extract different types of target datasets, which is potentially better for the specific tasks as it will inspect and transform things in Lean for that task.</p>",
        "id": 406937475,
        "sender_full_name": "Utensil Song",
        "timestamp": 1702129714
    },
    {
        "content": "<p>If we were to setup a data augmentation pipeline, there would be two general ways of doing it:</p>\n<ol>\n<li>Build tooling which can augment samples on request, and put this in your training loop in the normal way of augmentation. This is ultimately the most powerful, but getting it setup to run on the training machine might be a bit of faff, and I don't know whether it could be fast enough not to be a bottleneck. This seems somewhere between hard and impossible.</li>\n<li>Build a tool as above, and provide a pre-generated dataset a few orders of magnitude larger than base mathlib by baking in lots of augmentations. This is ultimately less powerful, but provides a much more accessible way of getting started.</li>\n</ol>",
        "id": 406938558,
        "sender_full_name": "Sam",
        "timestamp": 1702130163
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> would you consider data augmentation transforms to be in scope for lean-training-data?</p>",
        "id": 406939190,
        "sender_full_name": "Sam",
        "timestamp": 1702130431
    },
    {
        "content": "<p>1 doesn’t seem feasible to me since once you remove the data from Lean you have removed the information you need to augment it.  Unless like LeanDojo (from what it seems), you are extracting tons of metadata with the data you need, but even then it might be too hard.</p>",
        "id": 406940011,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702130693
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I was assuming the augmentation would be done as part of the extraction process. Essentially the pipeline would have to take some proof, feed it through an augmentor, and then extract multiple augmented versions.</p>\n<p>I agree that this seems almost impossible to do in the training loop after the fact.</p>",
        "id": 406940380,
        "sender_full_name": "Sam",
        "timestamp": 1702130861
    },
    {
        "content": "<p>Oh, I think I misunderstood 1.  You mean your tool will extract data from Lean in the format you need it (with whatever augmentations you request).  This seems best to me.  It doesn’t even have to be a tight integration with the ML pipeline, unless you are doing RL.</p>",
        "id": 406940389,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702130867
    },
    {
        "content": "<p>Yeah. Ideally of course you want an effectively infinite amount of data, which means an effectively infinite amount of augmentation - which is best achieved by doing it live. But that seems infeasible. The closest we could get to that would probably be re-running the augmented extraction every epoch.</p>",
        "id": 406940534,
        "sender_full_name": "Sam",
        "timestamp": 1702130979
    },
    {
        "content": "<p>Realistically, just having a 100X size dataset with pre-applied augmentations would be a great start. If you can stick something like that on huggingface, the barrier to entry goes way down.</p>",
        "id": 406940637,
        "sender_full_name": "Sam",
        "timestamp": 1702131034
    },
    {
        "content": "<p>lean-training-data's readme includes <code>TODO: Break out individual steps of rw and simp_rw, with intermediate goals. This is easy to do, just needs some plumbing.</code> which looks like a vague start in the right direction!</p>",
        "id": 406941690,
        "sender_full_name": "Sam",
        "timestamp": 1702131730
    },
    {
        "content": "<p>Related to all of this, it would also be cool to put community effort into a really good Lean tokenizer. ReProver just used bytes because tokenizing lean is hard, but I bet it's possible to do better.</p>",
        "id": 406942183,
        "sender_full_name": "Sam",
        "timestamp": 1702131927
    },
    {
        "content": "<p>As bare minimum, one could use a byte tokenizer + special tokens for common strings like tactic names.</p>",
        "id": 406942361,
        "sender_full_name": "Sam",
        "timestamp": 1702131977
    },
    {
        "content": "<p>You have to be careful.  If you use a custom tokenizer, you can’t use pretrained models.</p>",
        "id": 406946670,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702133804
    },
    {
        "content": "<p>Well, I haven't tried it, but I suspect in some cases moving to a custom tokenizer might be reasonable as a finetuning step.</p>",
        "id": 406947450,
        "sender_full_name": "Sam",
        "timestamp": 1702134321
    },
    {
        "content": "<p>So you would not use the original vocabulary embeddings, but just the internal weights?</p>",
        "id": 406947724,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702134516
    },
    {
        "content": "<p>Related to these augmentation discussions, we can also get a lot more data out of mathlib by introducing some other tasks such as</p>\n<ul>\n<li>Given some goal state and some future goal state (not necessarily on the proof path); what tactic sequence achieves that state?</li>\n<li>Given some goal state and some tactic sequence (not necessarily on the proof path); what goal state results?</li>\n</ul>\n<p>Although these aren't exactly augmentations, I think they have their place in the same tool. The method to generate them would be to apply random (but sensible) tactics to each goal state and extract. Obviously you don't want to train the model to generate those random tactics, but you can train it to understand their consequences.</p>",
        "id": 406947801,
        "sender_full_name": "Sam",
        "timestamp": 1702134596
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406947724\">said</a>:</p>\n<blockquote>\n<p>So you would not use the original vocabulary embeddings, but just the internal weights?</p>\n</blockquote>\n<p>Yeah, I think that should be possible within reason. Probably. I'm sure somebody's tried it in some domain.</p>",
        "id": 406947884,
        "sender_full_name": "Sam",
        "timestamp": 1702134645
    },
    {
        "content": "<p>It would certainly make sense to extend the PACT approach with whatever auxiliary data you want (if it fits in the context).  All your suggestions sound reasonable.  The key I think is to find the right data to improve the search overall. That might require not just auxiliary/augmented data, but different ways of searching entirely.</p>",
        "id": 406948162,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702134882
    },
    {
        "content": "<p>One thing to be cautious of (which weirdly I've never really seen discussed) is that more compact tokenization reduces computational capacity. Models do computation per token, so a higher information per token rate should in theory make the model weaker, although to some degree that may be offset by the task being easier.</p>\n<p>So if, for example, we tried to finetune the ByT5 ReProver model to use fewer but larger tokens, it may struggle because you've reduced it's internal capacity by a factor of mean_bytes_per_token.</p>\n<p>Weirdly I've never seen this information density per token idea discussed, but it seems like it must surely be true.</p>",
        "id": 406948229,
        "sender_full_name": "Sam",
        "timestamp": 1702134956
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406948162\">said</a>:</p>\n<blockquote>\n<p>It would certainly make sense to extend the PACT approach with whatever auxiliary data you want (if it fits in the context).  All your suggestions sound reasonable.  The key I think is to find the right data to improve the search overall. That might require not just auxiliary/augmented data, but different ways of searching entirely.</p>\n</blockquote>\n<p>Even outside of search, I'd love it if we could have a really strong Lean base model to build all of those search type models off of. That's why I'm so interested in extensive augmentation and auxiliary tasks designed to force a really strong internal world model.</p>",
        "id": 406948397,
        "sender_full_name": "Sam",
        "timestamp": 1702135094
    },
    {
        "content": "<p>My interest got renewed today after seeing the hype about Mamba potentially blowing things wide open in this space. If the early indications pan out, and we really do have a faster and stronger alternative to transformers, the barrier to building large strong base models just went down...</p>",
        "id": 406948633,
        "sender_full_name": "Sam",
        "timestamp": 1702135298
    },
    {
        "content": "<p>mamba?: <a href=\"https://arxiv.org/abs/2312.00752\">https://arxiv.org/abs/2312.00752</a></p>",
        "id": 406949216,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702135767
    },
    {
        "content": "<p>Yes, I’ve been saying for a while we need to do more with state space models.  Maybe it is time for me to follow through on that. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 406949338,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702135848
    },
    {
        "content": "<p>That's the one! I've only skimmed the paper so far and it's got lots of new concepts to get my head around, but it's picking up some hype for good reason. </p>\n<p><a href=\"https://old.reddit.com/r/MachineLearning/comments/18d65bz/d_thoughts_on_mamba/\">Somebody on the machinelearning subreddit</a> implemented it wrong by accident (throwing in a couple of extra feedforward layers in each block) and got some very impressive results on a toy character level shakespeare model. I ran <a href=\"https://colab.research.google.com/drive/1g9qpeVcFa0ca0cnhmqusO4RZtQdh9umY?usp=sharing#scrollTo=2lECw6S4N7cn\">their colab</a> and got very impressive results in 13 minutes of training on a T4 and went \"Oh, they might have something here...\"</p>",
        "id": 406949517,
        "sender_full_name": "Sam",
        "timestamp": 1702136005
    },
    {
        "content": "<p>I haven't looked into it enough to know if it's actually good, but that result definitely left me wanting to play with it! My first thought was that I'd love to try it on Lean, and then I hit the datasets problem and ended up here.</p>",
        "id": 406949639,
        "sender_full_name": "Sam",
        "timestamp": 1702136107
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406949338\">said</a>:</p>\n<blockquote>\n<p>Yes, I’ve been saying for a while we need to do more with state space models.  Maybe it is time for me to follow through on that. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>\n</blockquote>\n<p>If Mamba fully lives up to the hype, I'm afraid you may be 8 days too late to be the guy who defined the future of AI. <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 406949832,
        "sender_full_name": "Sam",
        "timestamp": 1702136305
    },
    {
        "content": "<p>It is not about the model but how you use it.  State space models have long contexts and the ability to checkpoint an initial segment of the conversation.  That could be used to incredible effect.  See the discussion in <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/HiPPO.20papers.20.28efficient.20sequence.20modeling.29\">#Machine Learning for Theorem Proving &gt; HiPPO papers (efficient sequence modeling)</a>.</p>",
        "id": 406950202,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702136646
    },
    {
        "content": "<p>Yeah, but early results are also showing them training faster/better than transformers for the same number of tokens and parameters, which is pretty cool if it scales!</p>",
        "id": 406950274,
        "sender_full_name": "Sam",
        "timestamp": 1702136729
    },
    {
        "content": "<p>Checkpointing isn't exactly exclusive to SSMs is it? You can do it via KV cache, but of course those are much larger.</p>",
        "id": 406950616,
        "sender_full_name": "Sam",
        "timestamp": 1702137025
    },
    {
        "content": "<p>Exactly, it is so much larger.</p>",
        "id": 406950807,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702137229
    },
    {
        "content": "<p>Yeah, part of what got me interested in using Mamba with Lean is some ideas I had for how to make efficient use of that checkpointing. My thinking is that it should make it easier to do multi task learning where at each step (say after each tactic) you branch into a few different learning tasks.</p>\n<p>Of course this can be done with a transformer based LLM and a custom mask, and there are some FlashAttention implementations that can make that masking somewhat efficient in some cases, but Mamba should be able to handle it more naturally I think.</p>",
        "id": 406951151,
        "sender_full_name": "Sam",
        "timestamp": 1702137573
    },
    {
        "content": "<p>It's worth noting that part of what makes Mamba so promising is that Tri Dao (of FlashAttention fame) collaborated on a very high performance CUDA kernel for it, which allows it to be very fast despite some of the pitfalls usually present with recurrent models. They use tricks to keep the checkpoints in SRAM which really makes this go fast.</p>\n<p>Those tricks probably start to lose their effectiveness if you interrupt the kernel to branch too often.</p>",
        "id": 406951799,
        "sender_full_name": "Sam",
        "timestamp": 1702137933
    },
    {
        "content": "<p>Actually it looks like the fast path used for training doesn't return states at all unfortunately. But that's just from scanning the python code that calls into the kernel - I should actually play with it.</p>\n<p><a href=\"https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L122\">https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L122</a></p>",
        "id": 406952612,
        "sender_full_name": "Sam",
        "timestamp": 1702138682
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"428422\">Sam</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What's.20state.20of.20public.20Lean.20datasets.20and.20augmentation.3F/near/406939190\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> would you consider data augmentation transforms to be in scope for lean-training-data?</p>\n</blockquote>\n<p>Sorry about the slow reply here; yes, this would definitely be in scope, and I'm strongly in favour of doing data augmentation in Lean rather than a post-processing in python. You have the Lean tactic monads available to do meaningful transformations.</p>\n<p>I am really hoping to put a lot of work into lean-training-data come January. :-)</p>",
        "id": 407335157,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702335960
    },
    {
        "content": "<p>There is also the Lean REPL, which gives some flexibility for do data augmentation \"post-hoc\". E.g. starting from a string you can run a partial declaration with a <code>sorry</code> in it in the REPL, get access to the tactic state, run tactics that modify the tactic state in some \"augmenting\" way, and then pretty print (perhaps using <code>extract_goal</code>) the new goal back to string for your \"offline\" store of data.</p>",
        "id": 407335363,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702336077
    },
    {
        "content": "<p>One thing that would be very useful I think is to have a mechanism (if it doesn’t already exist) in lean-training-data and lean-copilot for keeping track of the internal/hidden state of the search.  I have this impression that the data in lean-training-data and the tactic prediction API in lean-copilot are stateless.  A prediction can’t use the previous tactic history in the partial proof.  It also can’t pass hidden information forward, which is important for doing more complicated search algorithms, or for using RNNs in your search.</p>",
        "id": 407355279,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702346581
    },
    {
        "content": "<p>Also if one wants to do everything in Lean, there needs to be a way to store metadata along with new theorems/definitions as they are created.  Right now for example, I don’t think lean-copilot can do premise selection for new definitions.</p>",
        "id": 407355622,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702346678
    }
]