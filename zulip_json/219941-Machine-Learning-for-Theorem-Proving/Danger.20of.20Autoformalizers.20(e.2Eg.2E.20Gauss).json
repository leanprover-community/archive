[
    {
        "content": "<p>Have you considered the grave danger inherent in such powerful autoformalisers like this? Undergraduate maths students will no longer have an incentive to understand proofs, they'll just feed their questions to the autoformaliser and passively accept its results</p>",
        "id": 538951827,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757630100
    },
    {
        "content": "<p>Also, if autoformalisers aren't open-sourced, we may end up with an old joke where a commercial company claims to have solved something like the Riemann hypothesis but never produces an independently verifiable proof</p>",
        "id": 538952029,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757630227
    },
    {
        "content": "<p>The name Math, Inc is very ominous</p>",
        "id": 538952301,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757630419
    },
    {
        "content": "<p>tl;dr For the sake of its community's health and its future, we cannot afford mathematical proof becoming a commodity</p>",
        "id": 538953569,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757631436
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"598052\">@Jeremy Tan</span> we care a lot about making sure this technology is useful to mathematicians and moves mathematics as a human enterprise forward. historically, math advances when we can hide complexity, and autoformalization has the potential to allow us to work at higher levels instead of spelling out details all the time. a similar change happened when compilers were invented; it's true that the fraction of programmers familiar with low-level instructions and working really close to the machine dwindled, but moving to higher-level languages unlocked an entire universe of software. we're excited about what that could look like.</p>",
        "id": 538956201,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1757633354
    },
    {
        "content": "<p>I am not against Gauss or any autoformaliser; I am just warning you that such tools must be used correctly since they carry a lot of potential risk</p>",
        "id": 538956524,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757633610
    },
    {
        "content": "<p>I would certainly not want \"theorems for sale\"</p>",
        "id": 538956627,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757633697
    },
    {
        "content": "<p>May I suggest that meta-discussions like the last few messages above be moved to a different thread?  I am not sure that everyone is interested in such issues.  Thanks!</p>",
        "id": 538956682,
        "sender_full_name": "Ching-Tsun Chou",
        "timestamp": 1757633754
    },
    {
        "content": "<p>8 messages were moved here from <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Math.2C.20Inc.2C.20and.20Gauss.20discussion/with/538948739\">#Machine Learning for Theorem Proving &gt; Math, Inc, and Gauss discussion</a> by <span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span>.</p>",
        "id": 538957071,
        "sender_full_name": "Notification Bot",
        "timestamp": 1757634049
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"598052\">Jeremy Tan</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538956627\">said</a>:</p>\n<blockquote>\n<p>I would certainly not want \"theorems for sale\"</p>\n</blockquote>\n<p>There's an infinite supply of theorems out there, so I personally think any business in the \"market for theorems\" would go out of business quite quickly, either due to lack of demand, or oversaturation of the market with theorems, or (most probably) both.</p>",
        "id": 538958024,
        "sender_full_name": "Bryan Wang",
        "timestamp": 1757634928
    },
    {
        "content": "<p>Regarding autoformalisation (and automation in general), I too am quite pessimistic about such things, but I've quickly realised that in this day and age, we just can't stop people out there from trying to automate <em>anything</em>. But the mathlib community (and wider mathematical Lean community in general) put in a lot of effort to ensure that we have good-quality, well-organised, and (at least locally) understandable code in mathlib (this is something that I think is not advertised strongly enough to mathematicians and the public). If autoformalisation really takes off then I can see organisation and maintenance becoming the main job of formalisers, with autoformalisers being just one of many tools to speed up progress towards the frontiers of math. For what it's worth, there is no analogue of this and no interest in this (a central repository of all of math which is high-quality and well-organised) at all in traditional mathematics! And while undergrad education is a separate issue which deserves separate attention, even currently, traditional mathematicians have no issue routinely citing big results without understanding most of the details.</p>",
        "id": 538958182,
        "sender_full_name": "Bryan Wang",
        "timestamp": 1757635042
    },
    {
        "content": "<p>theorems for sale would be great. i would like to buy a theorem that proves the apps on my phone aren't going to steal my personal data</p>",
        "id": 538962273,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1757636972
    },
    {
        "content": "<p>I was about to make a post saying LLMs are unlikely to be able to do formal verification. But I woke up and saw this post. <span aria-label=\"eyes\" class=\"emoji emoji-1f440\" role=\"img\" title=\"eyes\">:eyes:</span> Gotta keep my eyes peeled</p>",
        "id": 538963699,
        "sender_full_name": "(deleted)",
        "timestamp": 1757637586
    },
    {
        "content": "<p>Unfortunately such dangers have already become a reality, if the concern is math students. Essentially all undergrads I teach , already try to use LLMs excessively for natural language proofs or to explain such proofs, no matter what you say to the students. </p>\n<p>It just isn't in a formalised setting like Lean 4, so really autoformalisation is an improvement of the status quo.</p>\n<p>It is like if a student always had a solution manual to everything, of sorts, its up to the student to practice self-discipline. And typically courses try to wash that out via paper invigilated exams.</p>",
        "id": 538967244,
        "sender_full_name": "Yan Yablonovskiy ðŸ‡ºðŸ‡¦",
        "timestamp": 1757640024
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"773147\">Yan Yablonovskiy ðŸ‡ºðŸ‡¦</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538967244\">said</a>:</p>\n<blockquote>\n<p>Unfortunately such dangers have already become a reality, if the concern is math students. Essentially all undergrads I teach , already try to use LLMs excessively for natural language proofs or to explain such proofs, no matter what you say to the students. </p>\n<p>It just isn't in a formalised setting like Lean 4, so really autoformalisation is an improvement of the status quo.</p>\n</blockquote>\n<p>Yes - and so I feel that, if anything, an inscrutable auto-generated Lean-verified proof of the Riemann hypothesis is orders of magnitude better than an inscrutable auto-generated natural language proof, because at least we can verify correctness. And at this point, there will always be people out there trying to do either of these (automate formal math, or automate natural language math), regardless of the dangers.</p>",
        "id": 538972622,
        "sender_full_name": "Bryan Wang",
        "timestamp": 1757644318
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"511228\">Huá»³nh Tráº§n Khanh</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538963699\">said</a>:</p>\n<blockquote>\n<p>I was about to make a post saying LLMs are unlikely to be able to do formal verification</p>\n</blockquote>\n<p>Who said that was unlikely to happen?</p>",
        "id": 538975610,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1757646860
    },
    {
        "content": "<p>As a math student before LLM era, I also was too lazy to study proofs. We all rely on things that other have proved, until the point we have to teach it ourselves (perhaps multiple times, before we actually understand it). Those who like to learn proofs will continue to learn proofs.</p>",
        "id": 538977887,
        "sender_full_name": "Yao Liu",
        "timestamp": 1757649127
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"598052\">Jeremy Tan</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538975610\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"511228\">Huá»³nh Tráº§n Khanh</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538963699\">said</a>:</p>\n<blockquote>\n<p>I was about to make a post saying LLMs are unlikely to be able to do formal verification</p>\n</blockquote>\n<p>Who said that was unlikely to happen?</p>\n</blockquote>\n<p>Me. Having seen repeated failures of GPT-5 and Claude Code. But too bad. In fact I was about to make a post saying that we should use AI to generate tools that can solve specific classes of problems, or to generate an auxiliary interactive theorem prover that is more limited and faster than Lean for rapid iteration</p>",
        "id": 538981631,
        "sender_full_name": "(deleted)",
        "timestamp": 1757652424
    },
    {
        "content": "<p>I'm not sure why theorems for sale is bad. I think it's a very good idea.</p>",
        "id": 538983413,
        "sender_full_name": "(deleted)",
        "timestamp": 1757653705
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"511228\">Huá»³nh Tráº§n Khanh</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538983413\">said</a>:</p>\n<blockquote>\n<p>I'm not sure why theorems for sale is bad. I think it's a very good idea.</p>\n</blockquote>\n<p>To paraphrase the famous six-word story: \"For sale: proven theorem, never understood\"</p>",
        "id": 538983727,
        "sender_full_name": "Chris Henson",
        "timestamp": 1757653938
    },
    {
        "content": "<p>One beauty of pure math is that the perceived value of a given theorem/result is directly proportional to (or at least strongly correlated with) the perceived difficulty of obtaining said result. Automation by nature decreases the difficulty of obtaining these results, and so the irony is that the better automation gets, the more it devalues (in the eyes of pure mathematicians) the results it can obtain. Therefore while formalisation and autoformalisation could change pure math in some other ways (e.g. changing what pure mathematicians value), I personally don't believe there is any <em>profitable</em> business to be found in pure math in the long run. I do agree though that perhaps applied math, on the other hand, could be a quite different story...</p>",
        "id": 538985579,
        "sender_full_name": "Bryan Wang",
        "timestamp": 1757655332
    },
    {
        "content": "<p>I don't think they refer to anything specific when saying \"theorems for sale\", but just a hint from Szegedy's posts, he wants to have formalized verification for superintelligence, whatever that means.</p>",
        "id": 538987904,
        "sender_full_name": "Yao Liu",
        "timestamp": 1757656919
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"934803\">Bryan Wang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538972622\">said</a>:</p>\n<blockquote>\n<p>Yes - and so I feel that, if anything, an inscrutable auto-generated Lean-verified proof of the Riemann hypothesis is orders of magnitude better than an inscrutable auto-generated natural language proof, because at least we can verify correctness.</p>\n</blockquote>\n<p>I would argue that an inscrutable machine-checked proof of even the Riemann hypothesis on its own is not worth much: the point of proof is to impart understanding, not a binary \"is this correct\" bit.</p>",
        "id": 539005238,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1757664266
    },
    {
        "content": "<p>(Though in practice, producing one will be hard without also producing a paper proof, which can hopefully be the basis for a clear exposition.)</p>",
        "id": 539005439,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1757664324
    },
    {
        "content": "<p>I actually think that any inscrutable proof will turn out to give some hints as to where to look for understanding. So it will always give more info than just the \"is this correct\" binary signal. At he very minimum, it gives the info that \"a proof of 37 GB exists\", and hence the motivation to find a smaller proof.</p>",
        "id": 539012442,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1757666607
    },
    {
        "content": "<p>And even more, the proof generated by Gauss is not inscrutable, it may not be clean, but once it's formalized, you can refactor. It's something developpers do everyday.<br>\nBecause in reality, all programmer write junk code. But more senior ones, guide them to refactor it so that it becomes understandable and maintainable.<br>\nAnd in the same vain, you can now ask LLMs to produce proofs and then ask them to refactor it with some senior mathematician insight.<br>\nI don't see anything wrong with this.</p>",
        "id": 539018103,
        "sender_full_name": "Alfredo Moreira-Rosa",
        "timestamp": 1757668532
    },
    {
        "content": "<p>I am worried that LLMs will spit out more \"not good\" code than more experienced users can refactor. (But part of the answer could be to make LLMs generate better code. And human insight, of course.)</p>",
        "id": 539018827,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1757668750
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"634338\">Michael Rothgang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/539005238\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"934803\">Bryan Wang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/538972622\">said</a>:</p>\n<blockquote>\n<p>Yes - and so I feel that, if anything, an inscrutable auto-generated Lean-verified proof of the Riemann hypothesis is orders of magnitude better than an inscrutable auto-generated natural language proof, because at least we can verify correctness.</p>\n</blockquote>\n<p>I would argue that an inscrutable machine-checked proof of even the Riemann hypothesis on its own is not worth much: the point of proof is to impart understanding, not a binary \"is this correct\" bit.</p>\n</blockquote>\n<p>I agree completely, my point was just to compare it to a similarly-inscrutable natural language proof <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span></p>",
        "id": 539020862,
        "sender_full_name": "Bryan Wang",
        "timestamp": 1757669444
    },
    {
        "content": "<p>I think if we had an inscrutable auto-generated Lean-verified proof of the Riemann hypothesis that would be amazing. I mean, can you imagine how many people would be poring over it trying to figure out what was going on? Even if you only value human understanding, it would be such a great source of direction for where to look.</p>\n<p>It would be like Renaissance scholars who learned Latin and Greek in order to interpret the wisdom of the ancients. Mathematicians could learn Lean in order to interpret the wisdom of the AIs.</p>",
        "id": 539095331,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1757690911
    },
    {
        "content": "<p>I've seen people claim it would be simultaneously amazing and terrible</p>",
        "id": 539118544,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1757694813
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"238605\">Kevin Lacker</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/539095331\">said</a>:</p>\n<blockquote>\n<p>I think if we had an inscrutable auto-generated Lean-verified proof of the Riemann hypothesis that would be amazing. I mean, can you imagine how many people would be poring over it trying to figure out what was going on? Even if you only value human understanding, it would be such a great source of direction for where to look.</p>\n<p>It would be like Renaissance scholars who learned Latin and Greek in order to interpret the wisdom of the ancients. Mathematicians could learn Lean in order to interpret the wisdom of the AIs.</p>\n</blockquote>\n<p>I would think translating from lean to informal would be fast and the hard part is that a superhuman math entity can probably reason about more complex object directly which may not be easily simplified by abstractions or general theory which may lead to some proof where it's inhuman to understand why you are studying various objects until they all fit together.</p>",
        "id": 539166558,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1757713425
    },
    {
        "content": "<p>Possibly the biggest problem/'danger' with autoformalization in the long term is how to integrate these massive projects, whose sizes may soon dwarf that of mathlib itself, into mathlib/the mathlib ecosystem in general. Perhaps this is something that the mathlib maintainers and community will have to think about in the not-so-distant future...</p>",
        "id": 539882254,
        "sender_full_name": "Bryan Wang",
        "timestamp": 1758051968
    },
    {
        "content": "<p>I started a discussion on a related topic at <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/Minimally.20exporting.20a.20theorem.20from.20a.20repository/with/539880441\">#general &gt; Minimally exporting a theorem from a repository</a> .  Hopefully there is some way to state the output of these projects as providing a term of some type <code>X:Prop</code> definable using some minimal subset of Mathlib, then one could then import an <code>axiom</code> of type <code>X</code> in another Mathlib-derived project, hopefully using either an identical or nearly identical minimal subset of Mathlib so that one can easily match types.</p>",
        "id": 539900591,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758060473
    },
    {
        "content": "<p>There seems to be something missing from the discussion of a hypothetical machine-generated proof of the Riemann hypothesis. The Riemann hypothesis, and certain other famous conjectures such as P != NP, are not just \"random hard problems.\" In both cases, there are known <em>barriers</em> to proving these conjectures. The strong expectation is that there doesn't exist a proof that simply plows through a massive inscrutable computation one trivial step at a time and emerges at the other end with the goal accomplished.</p>\n<p>By contrast, the proof that a <a href=\"https://en.wikipedia.org/wiki/Robbins_algebra\">Robbins algebra</a> is a Boolean algebra, or the similarly-flavored <a href=\"https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/\">Wolfram's theorem</a>, is exactly the kind of result where it's unsurprising for there to exist a proof that just \"chases equations\" until the proof is complete. In such cases, there may or may not be an \"idea\" underlying the proof.</p>\n<p>If we get access to a proof the Riemann hypothesis or P != NP, then an immediate question is going to be, how were the known barriers circumvented? I would say that with high probability, there is going to be an answer to that question, even if it takes a huge effort to extract it from the machine-generated mess of a proof. (A starting point would be to identify which definitions get repeatedly reused.) Once the idea is extracted, then it's going to reveal a new technique that can be used to solve other problems.</p>\n<p>There is a small probability that despite our best efforts, no such idea can be extracted, and that the proof really does turn out to be a Robbins-algebra-type of proof, where every line of the proof looks virtually indistinguishable from every other line. That might be disappointing from an aesthetic point of view, but IMO it would be a revolutionary discovery in its own right to find that the Riemann hypothesis has a short (i.e., at most exabyte or so) proof that <em>requires no new definitions</em>.</p>\n<p>Either way, I see it as a massive win for mathematics.</p>\n<p>Now, if the problems get resolved in the \"unexpected\" direction, then the win might not be as massive. If the computer just spits out a zero off the critical line, or we get some inscrutable Turing machine that solves SAT in time O(n^k) with k being Graham's-number, then we might not get much generalizable insight from the proof. But it would still be nice to know these things.</p>",
        "id": 553697364,
        "sender_full_name": "Timothy Chow",
        "timestamp": 1762284700
    },
    {
        "content": "<p>I think that a proof of P=NP, even with O(n^[something huge]), would still be at least a huge a win. For starters, we would expect that constant to be at least <em>greatly</em> reduced through effort. Second, it would tell us something very surprising about the structure of NP-hard problems, the insights would surely be relevant all over. In particular, whatever structure they expose could probably be used to accelerate/complement existing algorithms.</p>\n<p>For example, Reingold's proof that L = SL shows this by an O(n^(2^64)) algorithm. But the insights have still had application in other places, esp. graph theory</p>",
        "id": 553953008,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1762373346
    },
    {
        "content": "<p>Likewise, MIP*=RE doesn't have much computational content (<span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span> pardon the pun), but still offers plenty of insight.</p>",
        "id": 553957145,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1762374778
    },
    {
        "content": "<p>Perhaps it is worth pointing out that Robbins algebra result was not proved using the current LLM-based methods.  So far, has any LLM-based method produced a proof of an open problem or even a new proof of a known result?</p>",
        "id": 553957577,
        "sender_full_name": "Ching-Tsun Chou",
        "timestamp": 1762374920
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110637\">Ching-Tsun Chou</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/553957577\">said</a>:</p>\n<blockquote>\n<p>So far, has any LLM-based method produced a proof of an open problem or even a new proof of a known result?</p>\n</blockquote>\n<p>This was a <a href=\"https://mathoverflow.net/q/502120\">recent question on MathOverflow</a>. None of the examples listed there is as compelling as the Robbins problem. One could also maybe cite Ken Ono's experience with a private version of o4-mini that apparently autonomously solved what Ono thought was a Ph.D.-level problem about elliptic curves. I think that at the time Ono thought up the problem and supplied it to the machine, it was open, although I think he managed to figure it out himself fairly quickly.</p>",
        "id": 553958821,
        "sender_full_name": "Timothy Chow",
        "timestamp": 1762375413
    },
    {
        "content": "<p>Was this \"Ph.D.-level problem about elliptic curves\" a question whose answer was a number (rather than a proof)? My experience with these is that the machine can get lucky and vibe their way to the number whilst not really understanding why various intermediate claims they need along the way are true.</p>",
        "id": 553959566,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1762375709
    },
    {
        "content": "<p>Another issue with these number questions is that sometimes people keep asking the same question to the machine until it gets the number right; if it gets it right once then victory is declared and \"machine answers hard research problem\". This does not scale to a tool which is useful to mathematicians.</p>",
        "id": 553959748,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1762375792
    },
    {
        "content": "<p>No, it wasn't just a number. The question was, \"What is the 5th power moment of Tamagawa numbers of elliptic curves over Q?\" You can read about it in Scientific American, or read Ono's summary of the most relevant interaction <a href=\"https://www.facebook.com/photo/?fbid=9758357980879433&amp;set=a.164126083636052\">here</a>. Ono said that o4-mini autonomously searched the literature, found a paper, correctly rejected it as not relevant, searched again, found the right paper, identified an obstacle, got around the obstacle, and finally produced a correct formula. To make sure o4-mini \"understood\" the formula, he then asked it to do a numerical computation, which it did correctly.</p>",
        "id": 553963956,
        "sender_full_name": "Timothy Chow",
        "timestamp": 1762377375
    },
    {
        "content": "<p>Some people have expressed skepticism that the model may not have acted as autonomously as Ono's report makes it seem, but even if we accept the report at face value (which I'm inclined to do), it still appears to be a cherry-picked outlier at this point in time. I had a brief email exchange with Ono on the topic, and he said that ChatGPT Instant (which is free) still can't solve the problem. ChatGPT Thinking (subscription) can, but probably because it has access to a record of the original conversation with Ono.</p>",
        "id": 553968592,
        "sender_full_name": "Timothy Chow",
        "timestamp": 1762379175
    },
    {
        "content": "<p>[rephrased]  It truly annoys me that I had to log into Facebook to read a math discussion.</p>",
        "id": 553968651,
        "sender_full_name": "Ching-Tsun Chou",
        "timestamp": 1762379211
    },
    {
        "content": "<p>See <a href=\"#narrow/stream/270676-lean4/topic/Community.20Guidelines/near/235207529\">#butterfly</a> to learn what this emoji means.</p>",
        "id": 553972780,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1762381136
    },
    {
        "content": "<p>3 messages were moved from this topic to <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/How.20have.20you.20guys.20found.20GPT.205.3F/with/554048853\">#Machine Learning for Theorem Proving &gt; How have you guys found GPT 5?</a> by <span class=\"user-mention silent\" data-user-id=\"240862\">Oliver Nash</span>.</p>",
        "id": 554049368,
        "sender_full_name": "Notification Bot",
        "timestamp": 1762423956
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/553959566\">said</a>:</p>\n<blockquote>\n<p>Was this \"Ph.D.-level problem about elliptic curves\" a question whose answer was a number (rather than a proof)? My experience with these is that the machine can get lucky and vibe their way to the number whilst not really understanding why various intermediate claims they need along the way are true.</p>\n</blockquote>\n<p>It is good at problems which answer is a number. It can do well with FrontierMath questions</p>",
        "id": 554090249,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762436101
    },
    {
        "content": "<p>There are cases that it can get the right number but the reasoning is wrong</p>",
        "id": 554090740,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762436236
    },
    {
        "content": "<p>You can listen to Ken Ono describing his interaction with o4-mini <a href=\"https://www.youtube.com/watch?v=vGmrcm1aD6g&amp;t=790s\">here</a>. See also this <a href=\"https://x.com/littmath/status/1931403214613598252\">tweet by Daniel Litt</a>.</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"vGmrcm1aD6g\" href=\"https://www.youtube.com/watch?v=vGmrcm1aD6g&amp;t=790s\"><img src=\"https://uploads.zulipusercontent.net/079675698f606844f51d1824b1236513b7a1a13e/68747470733a2f2f692e7974696d672e636f6d2f76692f76476d72636d31614436672f6d7164656661756c742e6a7067\"></a></div>",
        "id": 554155851,
        "sender_full_name": "Timothy Chow",
        "timestamp": 1762451878
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"478409\">Timothy Chow</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/554155851\">said</a>:</p>\n<blockquote>\n<p>You can listen to Ken Ono describing his interaction with o4-mini <a href=\"https://www.youtube.com/watch?v=vGmrcm1aD6g&amp;t=790s\">here</a>. See also this <a href=\"https://x.com/littmath/status/1931403214613598252\">tweet by Daniel Litt</a>.</p>\n</blockquote>\n<p>Thank you</p>",
        "id": 554223688,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762480494
    },
    {
        "content": "<p><a href=\"https://arxiv.org/pdf/2407.15360\">https://arxiv.org/pdf/2407.15360</a></p>",
        "id": 554294579,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762513401
    },
    {
        "content": "<p>There are people trying to make sense what happening inside Transformer doing simple arithmetic</p>",
        "id": 554294704,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762513438
    },
    {
        "content": "<p>Maybe there is a reason explain why the AI model can reach the correct answer in FrontierMath questions</p>",
        "id": 554302258,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762515762
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"874606\">Anh Nguyá»…n</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/554302258\">said</a>:</p>\n<blockquote>\n<p>Maybe there is a reason explain why the AI model can reach the correct answer in FrontierMath questions</p>\n</blockquote>\n<p>By training on problems/papers, they can develop their own ways of navigating through a problem</p>",
        "id": 554304230,
        "sender_full_name": "Anh Nguyá»…n",
        "timestamp": 1762516434
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"448405\">Alex Meiburg</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Danger.20of.20Autoformalizers.20.28e.2Eg.2E.20Gauss.29/near/553953008\">said</a>:</p>\n<blockquote>\n<p>I think that a proof of P=NP, even with O(n^[something huge]), would still be at least a huge a win.</p>\n</blockquote>\n<p>It would win you $1 million. What's not so well known is that, according to the <a href=\"https://www.claymath.org/wp-content/uploads/2022/03/millennium_prize_rules_0.pdf\">Millennium Prize Rules</a> (in particular, Rule 5), disproving the Riemann Hypothesis won't necessarily win you the $1 million. Only P = NP and Navier-Stokes are guaranteed to win you the $1 million either way they are resolved.</p>",
        "id": 554354015,
        "sender_full_name": "Timothy Chow",
        "timestamp": 1762530444
    }
]