[
    {
        "content": "<p>Two new papers dropped (submissions to ICLR 2023) on the topic of auto-formalization of statements and proofs in ITPs using off-the-shelf pre-trained models (like Codex and Minerva).</p>",
        "id": 301531452,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664485393
    },
    {
        "content": "<ul>\n<li><a href=\"https://openreview.net/forum?id=pKu077C57fH\">Towards a Mathematics Formalisation Assistant using Large Language Models</a><ul>\n<li>Lean 3 and 4</li>\n<li>Tried translating 120 informal theorem statements to Lean 4</li>\n<li>Tried translating 13 natural language proofs to Lean 3</li>\n<li>Some novel features include<ul>\n<li>input dependent prompts (similar to retrieval augmented language models)</li>\n<li>using the Lean 3 to Lean 4 syntax translator (from mathport?) and the Lean 4 elaborator to filter candidate theorem translations</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>",
        "id": 301532728,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664485908
    },
    {
        "content": "<ul>\n<li><a href=\"https://openreview.net/forum?id=SMa9EAovKMC\">Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs</a><ul>\n<li>Isabelle</li>\n<li>Focus on miniF2F problems</li>\n<li>Gives an end-to-end solution which takes in informal statement and formal statement and then generates a proof (by first generating an informal proof and a formal Isabelle proof sketch, and finally applying hammer-like tactics to fill in the missing steps in the sketch).</li>\n</ul>\n</li>\n</ul>",
        "id": 301532735,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664485911
    },
    {
        "content": "<p>First, I want to say that I think both papers are very nice.  They really show the potential of out-of-the-box large pre-trained language models to write proofs in ITPs.  And like the recent <a href=\"https://arxiv.org/abs/2205.12615\">auto-formalization paper,</a> these techniques could be immediately <a href=\"https://github.com/zhangir-azerbayev/lean-chat\">coded up</a> and given to Lean, Isabelle, Coq users to play with.</p>",
        "id": 301886403,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678738
    },
    {
        "content": "<p>But I'm also a bit concerned by a particular element in the second paper (\"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs\").  This paper shows that one can solve many more proofs using LLMs to first make an informal proof, then convert that to a formal proof sketch, and finally fill in the holes with hammer-like tactics.  Since this paper uses large pre-trained language models, I think it should at least have a small discussion about leakage.  This is even more important since many problems in the miniF2F benchmark are famous and have been discussed on the internet repeatedly.  In particular, there is a lot of discussion on the 1959 IMO p1 which the model got correct.  (This is both the first IMO problem ever, and the one which some consider the easiest ever).</p>",
        "id": 301886408,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678755
    },
    {
        "content": "<p>Of course, some types of leakage are worse than others.  For example, if Codex had the miniF2F repo in its training data, then any problems with an Isabelle proof in that repo would be much easier for the model.  (Now Isabelle proofs were just added recently to miniF2f, and the Codex code-davinci-002 data is from Jun 2021, so I don't think there is this type of leakage, but this should be discussed in the paper.)  Also, some of these proofs could also occur in other repos or in other ITPs.  Finally, it is guaranteed that at least some of these theorems are in Minerva's and Codex's training data in their natural language form.</p>",
        "id": 301886409,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678761
    },
    {
        "content": "<p>Now this problem isn't unique to this paper.  Most of the benchmarks on miniF2F use pre-trained language models specifically pre-trained on math-heavy data.  This is very much desirable as it is an easy way to instill the model with \"math and code awareness\", but it also means that the miniF2F benchmark with it's heavy use of competition problems, many years old, should be thought of as not purely a reasoning benchmark, but an auto-formalization benchmark.  This means that models like sledgehammer which don't even have the ability to memorize human-written proofs are at a disadvantage, especially for famous problems which have made their way into the training data (in natural language form).  A cynical view (to play the devil's advocate) would be that this disadvantage might be reduced heavily if evaluated on problems which are guaranteed to be novel.</p>",
        "id": 301886451,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678767
    },
    {
        "content": "<p>I do think this is a tricky matter.  The majority of formalization in mathematics is formalizing stuff which has already been published, some if it decades old and ubiquitous on the Internet.  It is quite advantageous to have a model like this one, which given a theorem, automatically retrieves and reconstructs a number of informal proofs of that theorem, and uses those to make a number of formal proof sketches which are tested to see if they can be automatically filled in.</p>",
        "id": 301886456,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678773
    },
    {
        "content": "<p>By the way, the first paper (Towards a Mathematics Formalisation Assistant using Large Language Models) goes very much out of their way to test on problems unlikely to be in the training data of Codex.</p>",
        "id": 301886463,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678776
    },
    {
        "content": "<p>I agree with Jason's assessment that the leakage problem should be addressed so that the readers are aware of this possibility. But to qualify it: the leakage, if any, is more of a Minerva problem (informal prover) than a Codex (autoformaliser) problem since the miniF2F/Isabelle partition contains not many proofs. For the DSP paper in particular, an ablation study shows that if one simply lets Codex write down the full formal proof, the performance is pretty bad. This to me suggests that pure memoisation does not work out-of-the-box.</p>",
        "id": 301935561,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1664717435
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I think the problem you are mentioning is indeed more specific to Minerva, where it's true that the majority of the test problems can be found on Google. But here, the main point of the paper was more to show that if an informal proof is available (whether it's written by a human or a by model), then you can use it to guide a formal prover. So the first application here is not necessarily to prove new problems, but more to facilitate the formalization / translation in Lean when informal proofs exist (which is the case for all theorems on mathlib I imagine).</p>",
        "id": 302018823,
        "sender_full_name": "Guillaume",
        "timestamp": 1664785511
    },
    {
        "content": "<p>After some research and analysis, I conclude that it is not likely that the abilities of large language models in the DSP paper can be attributed to memorisation, although I agree entirely with <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> that this is a very tricky matter and should be treated with care. My reasons for the conclusion are:</p>\n<ol>\n<li>\n<p>For coming up with informal proofs, Minerva and Codex are used. In section 5 of the <a href=\"https://arxiv.org/abs/2206.14858\">Minerva paper</a>, the authors analysed and discussed the possibility of memorisation by rephrasing the problems or changing the numbers/variables in the problems, plus a few more text-similarity examinations. They did not find convincing evidence that Minerva benefits from memorisation on their evaluation dataset. Codex is trained on code data and is less likely to remember mathematical proof data (it also performs inferiorly to Minerva models).</p>\n</li>\n<li>\n<p>For the autoformalisation of proof sketches, the specific Codex version (code-davinci-002) was trained on data up to June 2021, at which point the miniF2F benchmark was not publicly released yet! Also there is very little formalisation on the internet in general on these high-school maths competition problems. So it seems highly unlikely to me that Codex should remember anything about miniF2F.</p>\n</li>\n</ol>\n<p>I hope this is useful for the discussion of memorisation/data contamination with respect to this paper.</p>",
        "id": 302448403,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1664973854
    },
    {
        "content": "<p>I want to mention we had done an analysis in the Minerva paper about memorization, that is worth reading if you are concerned about this. We showed Minerva did a lot more than just memorization. In addition, Minerva attained great scores on new math exams this year (2022 UK, Polish math exams).</p>",
        "id": 304017408,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1665749629
    },
    {
        "content": "<p>I just looked at Section 5 of the Minerva paper.  I feel like it is stating two things:</p>\n<ul>\n<li>The MATH dataset wasn't in the training data for MInerva so memorization is impossible.</li>\n<li>Minerva does about the same with small changes to a question (it has not seen before).</li>\n</ul>\n<p>But does this say anything about competition level problems which may be in the dataset for PaLM or Minerva?  Those are not small changes from other problems.  Those are very different questions from each other.  Also, I don't think you looked at what happens when a problem does end up in Minerva's training data. (IMHO section 5.2 should have taken problems which were found in Minerva's training data, not problems that Minerva happened to get correct.)  Maybe the accuracy shoots up really high to almost 100%.</p>\n<p>Here are a few follow up hypotheticals:</p>\n<ul>\n<li>Do you think that Minerva would have done better or the same on the 2010-2020 polish math exams as the 2022 polish math exam, especially if those exams and their solutions made it into the training data for PaLM or Minerva (in English)?</li>\n<li>Do you think that Minerva would do the same on 1959 IMO p1 if it was or was not in the Minerva training data?</li>\n<li>Do you think the results in this ICLR paper which use Minerva would come out the same if the mini-F2F problems where replaced with different but similar caliber problems which are nowhere on the internet?</li>\n</ul>\n<p>I would also love to see the techniques of this ICLR paper applied to the 2022 Polish math exam.  That would be a good follow up and a cleaner experiment.  (But I'm not asking the authors to do that.  I know I'm being a Reviewer <a href=\"https://github.com/leanprover-community/mathlib/pull/2\">#2</a> right now.  I think the autoformalization part of the paper stands on its own.)</p>",
        "id": 304027964,
        "sender_full_name": "Jason Rute",
        "timestamp": 1665752886
    },
    {
        "content": "<p>Here is the arXiv version of one of the papers mentioned above:<br>\n<a href=\"https://arxiv.org/abs/2210.12283\">https://arxiv.org/abs/2210.12283</a></p>",
        "id": 306120667,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1666731529
    },
    {
        "content": "<p>The first paper is now public as both a <a href=\"https://arxiv.org/abs/2211.07524\">preprint</a> and a <a href=\"https://mathai2022.github.io/papers/17.pdf\">note for MATH-AI</a>.  Moreover, the authors <span class=\"user-mention\" data-user-id=\"439607\">@Ayush Agrawal</span>, <span class=\"user-mention\" data-user-id=\"266304\">@Siddhartha Gadgil</span>, <span class=\"user-mention\" data-user-id=\"248811\">@Navin Goyal</span>, <span class=\"user-mention\" data-user-id=\"250372\">@Ashvni Narayanan</span>, and <span class=\"user-mention\" data-user-id=\"303675\">@Anand Rao</span>  just released their LeanAid tool which is like LeanChat, but seems to have more powerful integration with Lean including checking if a translation typechecks and using Lean's environment docstrings to do adaptive prompting.  They include a repo and a demo video.</p>\n<ul>\n<li><a class=\"stream-topic\" data-stream-id=\"270676\" href=\"/#narrow/stream/270676-lean4/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#lean4 &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#general &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.203\">#general &gt; LeanAide translation: Natural language to Lean 3</a></li>\n</ul>",
        "id": 312985378,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669783210
    },
    {
        "content": "<p>Here is also another MATH-AI paper along these lines:</p>\n<ul>\n<li><a href=\"https://mathai2022.github.io/papers/20.pdf\">ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems</a> by <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span>, <span class=\"user-mention\" data-user-id=\"257492\">@Bartosz Piotrowski</span>, and <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span></li>\n</ul>",
        "id": 312985812,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669783557
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/More.20papers.20on.20autoformalization/near/312985378\">said</a>:</p>\n<blockquote>\n<p>The first paper is now public as both a <a href=\"https://arxiv.org/abs/2211.07524\">preprint</a> and a <a href=\"https://mathai2022.github.io/papers/17.pdf\">note for MATH-eAI</a>.  Moreover, the authors <span class=\"user-mention silent\" data-user-id=\"266304\">Siddhartha Gadgil</span> <span class=\"user-mention silent\" data-user-id=\"303675\">Anand Rao</span>  just released their LeanAid tool which is like LeanChat, but seems to have more powerful integration with Lean including checking if a translation typechecks and using Lean's environment docstrings to do adaptive prompting.  They include a repo and a demo video.</p>\n<ul>\n<li><a class=\"stream-topic\" data-stream-id=\"270676\" href=\"/#narrow/stream/270676-lean4/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#lean4 &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#general &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.203\">#general &gt; LeanAide translation: Natural language to Lean 3</a></li>\n</ul>\n</blockquote>\n<p>One clarification: we use docstrings extracted from <code>mathlib</code>in addition to those from the environment (the latter is not currently used by default in Lean 4). This is a large set so allows good prompt engineering. These were extracted by running docgen and using the intermediate JSON file (with some cleanup).</p>",
        "id": 312986067,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1669783750
    },
    {
        "content": "<p>This is extremely cool!</p>",
        "id": 313016801,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1669800351
    },
    {
        "content": "<p><a href=\"https://arxiv.org/abs/2302.12433\">ProofNet paper now on arXiv</a> - nice work <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span> <span class=\"user-mention\" data-user-id=\"257492\">@Bartosz Piotrowski</span> Hailey Schoelkopf <span class=\"user-mention\" data-user-id=\"121918\">@Edward Ayers</span> <span class=\"user-mention\" data-user-id=\"246919\">@Dragomir Radev</span>  <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span></p>",
        "id": 338362449,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1677512624
    },
    {
        "content": "<p>So is \"Yale College\" different from \"Yale university\"? Different authors have different affiliations (but all the relevant ones have <code>@yale.edu</code> email addresses)</p>",
        "id": 338372564,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1677514976
    },
    {
        "content": "<p>page 2: the Sylow theorem example at the top of the page has an example of a group of order 312, and the proof starts \"since the order of G is 351, ...\". Is this supposed to be a demonstration of how woefully bad language models are still, or is this a typo?</p>",
        "id": 338373260,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1677515145
    },
    {
        "content": "<p>The next characters are <code>351 = 3^2.13</code> (which is again crazy) and then <code>G has 3-Sylow subgroup of order 9</code> (which is wrong for the initial group of cardinality <code>312</code> as this is not divisible by <code>9</code>)...</p>",
        "id": 338374862,
        "sender_full_name": "Sebastien Gouezel",
        "timestamp": 1677515514
    },
    {
        "content": "<p>Undergraduates are technically members of the college, and both the college and the department of computer science are part of the university.</p>",
        "id": 338441105,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1677528944
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/More.20papers.20on.20autoformalization/near/338373260\">said</a>:</p>\n<blockquote>\n<p>page 2: the Sylow theorem example at the top of the page has an example of a group of order 312, and the proof starts \"since the order of G is 351, ...\". Is this supposed to be a demonstration of how woefully bad language models are still, or is this a typo?</p>\n</blockquote>\n<p>This is my mistake. I copied the example from the dataset into the figure, and made some silly errors while doing so.</p>",
        "id": 338442590,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1677529396
    },
    {
        "content": "<p>Thank you!<br>\nas a tiny remark: the Pythia arxiv link doesn't seem to lead to the intended paper</p>",
        "id": 338654213,
        "sender_full_name": "Fabian Glöckle",
        "timestamp": 1677607225
    },
    {
        "content": "<p>Just noticed <em>Draft, Sketch, Prove</em> shares the same acronym as <em><a href=\"https://github.com/stanfordnlp/dsp\">Demonstrate, Search, Predict</a></em> from Stanford two months later :)</p>",
        "id": 344575328,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1679793265
    },
    {
        "content": "<p>eh I might go back to norse mythology names if acronym collision is so frequent</p>",
        "id": 344643392,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1679831332
    },
    {
        "content": "<p>Re ProofNet, I was just scrolling through the paper and realized that case study 1 is actually wrong. Exercise 10.7.4 asks you to prove that if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi><mo>⊆</mo><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">M\\subseteq R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8193em;vertical-align:-0.136em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">⊆</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span> is an ideal which contains all non-units of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span>, then <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> is the unique maximal ideal. The issue is that of course this fails if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi><mo>=</mo><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">M=R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span>. A human would understand that implicitly, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> has to be a <em>proper</em> ideal, or that it must contain only non-units of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span> as well (or else <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> trivially cannot be maximal). However, the formalization does not contain this subtlety:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">theorem</span> <span class=\"n\">exercise_10_7_10</span> <span class=\"o\">{</span><span class=\"n\">R</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"bp\">*</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">ring</span> <span class=\"n\">R</span><span class=\"o\">]</span> <span class=\"o\">(</span><span class=\"n\">M</span> <span class=\"o\">:</span> <span class=\"n\">ideal</span> <span class=\"n\">R</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">hM</span> <span class=\"o\">:</span> <span class=\"bp\">∀</span> <span class=\"n\">x</span> <span class=\"o\">:</span> <span class=\"n\">R</span><span class=\"o\">,</span> <span class=\"n\">x</span> <span class=\"bp\">∉</span> <span class=\"n\">M</span> <span class=\"bp\">→</span> <span class=\"n\">is_unit</span> <span class=\"n\">x</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"n\">is_maximal</span> <span class=\"n\">M</span> <span class=\"bp\">∧</span> <span class=\"bp\">∀</span> <span class=\"o\">(</span><span class=\"n\">N</span> <span class=\"o\">:</span> <span class=\"n\">ideal</span> <span class=\"n\">R</span><span class=\"o\">),</span> <span class=\"n\">is_maximal</span> <span class=\"n\">N</span> <span class=\"bp\">→</span> <span class=\"n\">N</span> <span class=\"bp\">=</span> <span class=\"n\">M</span> <span class=\"o\">:=</span> <span class=\"gr\">sorry</span>\n\n<span class=\"kd\">example</span> <span class=\"o\">:</span> <span class=\"n\">false</span> <span class=\"o\">:=</span>\n<span class=\"o\">(</span><span class=\"n\">exercise_10_7_10</span> <span class=\"o\">(</span><span class=\"bp\">⊤</span> <span class=\"o\">:</span> <span class=\"n\">ideal</span> <span class=\"n\">ℤ</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">))</span><span class=\"bp\">.</span><span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"n\">ne_top</span> <span class=\"n\">rfl</span>\n</code></pre></div>\n<p>This could be seen as a one-off mistake, but I am tempted to read into it more (especially since it was specifically selected as a successful case study by the authors). I wonder if it would be good to train models to either provide a proof <em>or</em> a counterexample, so that situations like this are still valuable. It also highlights the subtlety of translating text to formal mathematics -- sometimes you need a good grasp of the content already in order to translate the exercise correctly.</p>",
        "id": 349258602,
        "sender_full_name": "Praneeth Kolichala",
        "timestamp": 1681445325
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"437861\">Praneeth Kolichala</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/More.20papers.20on.20autoformalization/near/349258602\">said</a>:</p>\n<blockquote>\n<p>Re ProofNet, I was just scrolling through the paper and realized that case study 1 is actually wrong. Exercise 10.7.4 asks you to prove that if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi><mo>⊆</mo><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">M\\subseteq R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8193em;vertical-align:-0.136em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">⊆</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span> is an ideal which contains all non-units of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span>, then <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> is the unique maximal ideal. The issue is that of course this fails if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi><mo>=</mo><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">M=R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span>. A human would understand that implicitly, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> has to be a <em>proper</em> ideal, or that it must contain only non-units of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span></span></span></span> as well (or else <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span> trivially cannot be maximal). However, the formalization does not contain this subtlety:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">theorem</span> <span class=\"n\">exercise_10_7_10</span> <span class=\"o\">{</span><span class=\"n\">R</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"bp\">*</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">ring</span> <span class=\"n\">R</span><span class=\"o\">]</span> <span class=\"o\">(</span><span class=\"n\">M</span> <span class=\"o\">:</span> <span class=\"n\">ideal</span> <span class=\"n\">R</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">hM</span> <span class=\"o\">:</span> <span class=\"bp\">∀</span> <span class=\"n\">x</span> <span class=\"o\">:</span> <span class=\"n\">R</span><span class=\"o\">,</span> <span class=\"n\">x</span> <span class=\"bp\">∉</span> <span class=\"n\">M</span> <span class=\"bp\">→</span> <span class=\"n\">is_unit</span> <span class=\"n\">x</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"n\">is_maximal</span> <span class=\"n\">M</span> <span class=\"bp\">∧</span> <span class=\"bp\">∀</span> <span class=\"o\">(</span><span class=\"n\">N</span> <span class=\"o\">:</span> <span class=\"n\">ideal</span> <span class=\"n\">R</span><span class=\"o\">),</span> <span class=\"n\">is_maximal</span> <span class=\"n\">N</span> <span class=\"bp\">→</span> <span class=\"n\">N</span> <span class=\"bp\">=</span> <span class=\"n\">M</span> <span class=\"o\">:=</span> <span class=\"gr\">sorry</span>\n\n<span class=\"kd\">example</span> <span class=\"o\">:</span> <span class=\"n\">false</span> <span class=\"o\">:=</span>\n<span class=\"o\">(</span><span class=\"n\">exercise_10_7_10</span> <span class=\"o\">(</span><span class=\"bp\">⊤</span> <span class=\"o\">:</span> <span class=\"n\">ideal</span> <span class=\"n\">ℤ</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">))</span><span class=\"bp\">.</span><span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"n\">ne_top</span> <span class=\"n\">rfl</span>\n</code></pre></div>\n<p>This could be seen as a one-off mistake, but I am tempted to read into it more (especially since it was specifically selected as a successful case study by the authors). I wonder if it would be good to train models to either provide a proof <em>or</em> a counterexample, so that situations like this are still valuable. It also highlights the subtlety of translating text to formal mathematics -- sometimes you need a good grasp of the content already in order to translate the exercise correctly.</p>\n</blockquote>\n<p>Thanks for spotting this! We are preparing a conference submission that will hopefully rectify any errors. </p>\n<p>I haven't decided whether or not to convert everything in the paper to Lean 4 before the conference submission. If I do, we can run sagredo on negations of theorem statements to catch errors. </p>\n<p>If I don't end up doing that, do you think something like <code>tidy</code> would help catch silly errors?</p>",
        "id": 350213383,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1681591058
    },
    {
        "content": "<p>I doubt tidy on negations will get very far. If someone were to systematically improve <code>slim_check</code> by running it on the negations of things in mathlib, that would be awesome. There is a lot of low-hanging fruit there.</p>",
        "id": 350272174,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681633637
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span> , I just noticed an error in the proof of the first example on ProofNet. Could this be due to an error in the problem source?</p>\n<p>Exercise ID: <code>Dummit-Foote|exercise_4_5_14</code></p>\n<div class=\"codehilite\" data-code-language=\"TeX\"><pre><span></span><code>nl<span class=\"nb\">_</span>statement\nProve that a group of order 312 has a normal Sylow <span class=\"s\">$</span><span class=\"nb\">p</span><span class=\"s\">$</span>-subgroup for some prime <span class=\"s\">$</span><span class=\"nb\">p</span><span class=\"s\">$</span> dividing its order.\n\nnl<span class=\"nb\">_</span>proof\n<span class=\"k\">\\begin</span><span class=\"nb\">{</span>proof<span class=\"nb\">}</span>\nSince <span class=\"s\">$</span><span class=\"nb\">|G|</span><span class=\"o\">=</span><span class=\"m\">351</span><span class=\"o\">=</span><span class=\"m\">3</span><span class=\"nb\">^{</span><span class=\"m\">2</span><span class=\"nb\">}.</span><span class=\"m\">13</span><span class=\"s\">$</span>, <span class=\"s\">$</span><span class=\"nb\">G</span><span class=\"s\">$</span> has <span class=\"s\">$</span><span class=\"m\">3</span><span class=\"o\">-</span><span class=\"s\">$</span>Sylow subgroup of order <span class=\"s\">$</span><span class=\"m\">9</span><span class=\"s\">$</span>,\nas well as <span class=\"s\">$</span><span class=\"m\">13</span><span class=\"o\">-</span><span class=\"s\">$</span>Sylow subgroup of order <span class=\"s\">$</span><span class=\"m\">13</span><span class=\"s\">$</span>.\nNow, we count the number of such subgroups.\nLet <span class=\"s\">$</span><span class=\"nb\">n_{</span><span class=\"m\">13</span><span class=\"nb\">}</span><span class=\"s\">$</span> be the number of <span class=\"s\">$</span><span class=\"m\">13</span><span class=\"o\">-</span><span class=\"s\">$</span>Sylow subgroup and <span class=\"s\">$</span><span class=\"nb\">n_{</span><span class=\"m\">3</span><span class=\"nb\">}</span><span class=\"s\">$</span> be the number of  <span class=\"s\">$</span><span class=\"m\">3</span><span class=\"o\">-</span><span class=\"s\">$</span>Sylow subgroup.\nNow <span class=\"s\">$</span><span class=\"nb\">n_{</span><span class=\"m\">13</span><span class=\"nb\">}</span><span class=\"o\">=</span><span class=\"m\">1</span><span class=\"o\">+</span><span class=\"m\">13</span><span class=\"nb\">k</span><span class=\"s\">$</span> where <span class=\"s\">$</span><span class=\"m\">1</span><span class=\"o\">+</span><span class=\"m\">13</span><span class=\"nb\">k|</span><span class=\"m\">9</span><span class=\"s\">$</span>. The choices for <span class=\"s\">$</span><span class=\"nb\">k</span><span class=\"s\">$</span> is <span class=\"s\">$</span><span class=\"m\">0</span><span class=\"s\">$</span>.\nHence, there is a unique <span class=\"s\">$</span><span class=\"m\">13</span><span class=\"o\">-</span><span class=\"s\">$</span>Sylow subgroup and hence is normal.\n<span class=\"k\">\\end</span><span class=\"nb\">{</span>proof<span class=\"nb\">}</span>\n</code></pre></div>\n<p>Corrected verison: $|G| = 312=2^3\\cdot 3 \\cdot 13$ and $351=3^3\\cdot 13$.</p>\n<p>I haven't checked the other exercises yet, but this could have small impact on the accuracy of the formalized proof.</p>\n<hr>\n<p>EDIT: Sorry, I have just noticed that this has already been pointed out.</p>",
        "id": 350319845,
        "sender_full_name": "RexWang",
        "timestamp": 1681653401
    }
]