[
    {
        "content": "<p>How about training Facebook Transcoder on Lean 3 to Lean 4?  It is open source.  <br>\n<a href=\"https://github.com/facebookresearch/TransCoder\">https://github.com/facebookresearch/TransCoder</a></p>",
        "id": 361651408,
        "sender_full_name": "Lars Ericson",
        "timestamp": 1685225291
    },
    {
        "content": "<p>At this point since 75% of mathlib is translated, it would possibly be better to do an aligned translation.  Also given that they are 75% done it would likely be more academic than practical.  Iâ€™d like to see this on translating say Isabelle to Lean.  But ideally we need better tools which can translate a whole development including a whole project with definitions, theorems etc.</p>",
        "id": 361651765,
        "sender_full_name": "Jason Rute",
        "timestamp": 1685225523
    },
    {
        "content": "<p>I guess it's too close at this point, latest report is</p>\n<table>\n<thead>\n<tr>\n<th>Status</th>\n<th>Count</th>\n<th>Percent</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Unported files</td>\n<td>702/2995</td>\n<td>(76.6% of total)</td>\n</tr>\n<tr>\n<td>Unported lines</td>\n<td>261216/1031509</td>\n<td>(74.7% of total)</td>\n</tr>\n<tr>\n<td>Longest unported chain</td>\n<td>22/115</td>\n<td>(80.9% progress)</td>\n</tr>\n</tbody>\n</table>\n<p>so there is still a lot to do but not an impossible amount, so practically speaking not worth the effort.</p>\n<p>Also the number of lines of Lean 3 code in existence is several orders of magnitude smaller than what was used to train for mainstream leanguages, so...never mind.  It was just a thought.  (This is from Google Bard but I trust it well enough, at least on this topic.):</p>\n<table>\n<thead>\n<tr>\n<th>Language Pair</th>\n<th>Translator Name</th>\n<th>Number of Lines of Code</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Python to JavaScript</td>\n<td>Facebook Transcoder</td>\n<td>100 million</td>\n</tr>\n<tr>\n<td>Java to C++</td>\n<td>Google Translate</td>\n<td>50 million</td>\n</tr>\n<tr>\n<td>C# to Ruby</td>\n<td>Microsoft Translator</td>\n<td>25 million</td>\n</tr>\n<tr>\n<td>PHP to Go</td>\n<td>DeepL</td>\n<td>10 million</td>\n</tr>\n<tr>\n<td>Swift to Kotlin</td>\n<td>Yandex Translate</td>\n<td>5 million</td>\n</tr>\n</tbody>\n</table>\n<p>The Google Bard queries to get this were</p>\n<ul>\n<li>What open source code translation models are similar to Facebook Transcoder?</li>\n<li>Please make a table with the language pair, the translator name, and the number of lines of code used to translate.</li>\n<li>Can you give me that in Zulip markdown?</li>\n</ul>",
        "id": 361662076,
        "sender_full_name": "Lars Ericson",
        "timestamp": 1685232810
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"364351\">Lars Ericson</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Facebook.20Transcoder.20for.20Lean.204.20port/near/361662076\">said</a>:</p>\n<blockquote>\n<p>(This is from Google Bard but I trust it well enough, at least on this topic.)</p>\n</blockquote>\n<p>Really?  I have a hard time believing both google translate is used for programming languages, and if it were that the amount of training data would be public knowledge.</p>",
        "id": 361673019,
        "sender_full_name": "Jason Rute",
        "timestamp": 1685242375
    },
    {
        "content": "<p>None of these appear to be translators for code.</p>",
        "id": 361675754,
        "sender_full_name": "Jason Rute",
        "timestamp": 1685245107
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> you are right the table is mostly a hallucination except for <a href=\"https://ai.facebook.com/blog/deep-learning-to-translate-between-programming-languages/\">Facebook Transcoder</a> which is legit.  GPT hallucinations can be <a href=\"https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\">very hazardous to a career</a>.  In the case of Lean, the risks are that the resulting translations are harder to port than the current output of <code>mathport</code>, or that whoever tried the project discovers that there  isn't enough training data to get useful output.</p>",
        "id": 361752075,
        "sender_full_name": "Lars Ericson",
        "timestamp": 1685279280
    },
    {
        "content": "<p>To me the solution is obvious: don't trust an LLM to give factual information. Stick to interpretive and creative enterprises, or things where the facts are checked as part of the work (e.g. theorem proving)</p>",
        "id": 361762689,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1685283031
    }
]