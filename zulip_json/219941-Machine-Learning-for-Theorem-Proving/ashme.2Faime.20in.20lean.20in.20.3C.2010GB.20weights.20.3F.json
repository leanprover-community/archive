[
    {
        "content": "<p>[Context: I'm still not convinced using LLM to generate tactics is the best/right way to solve IMO/Putnam problems.]</p>\n<p>Question: Are there any systems that:</p>\n<ol>\n<li>uses lean4 to solve Ashme/AIME problems</li>\n<li>program + all weights is &lt;= 10 GB ?</li>\n</ol>\n<p>Now, one might point out: ASHME is multiple choice, AIME is 0 - 999. Neither involves writing proofs. Why would you want to use Lean4?<br>\nAnswer: it forces correct reasoning from step n to step n+1. Even if the final answer is 0-999 or multiple choice, it is useful for us to force every intermediate step to be mathematically correct.</p>\n<p>Question: are there systems, using Lean4, a mix of good old fashioned symbolic AI + very small weight neural network, that can solve ASHME + AIME problems at &gt; 90% accuracy?</p>\n<p>The intuition here is I'm particularly interested in systems that \"leans heavily\" on lean4 tactics, good old fashioned symbolic ai, and less reliance on neural networks / LLMs.</p>\n<p>Thanks!</p>",
        "id": 562431901,
        "sender_full_name": "TongKe Xue",
        "timestamp": 1765189699
    },
    {
        "content": "<p>I'm working on one such system and I'll try to get it published next year</p>",
        "id": 562588040,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1765246123
    },
    {
        "content": "<p>How would this work given that Mathlib itself is already ~32GB? You need to load this for premises no? Unless you prove every theorem ground up.</p>",
        "id": 562682786,
        "sender_full_name": "Gerben Koopman",
        "timestamp": 1765286577
    },
    {
        "content": "<p>I agree with your feeling though, that next-token generation seems ill-suited to generating tactics. It's just that the alternatives I can think of are also not better. But curious to see what other people came up with.</p>",
        "id": 562702253,
        "sender_full_name": "Gerben Koopman",
        "timestamp": 1765291261
    },
    {
        "content": "<ol>\n<li>Great question. Sorry for not being clear. You get all of MathLib, CSLib, any other public open source Lean4 db of formalized lemmas/theorems for free.</li>\n<li>The 10GB limit was meant mainly to focus on \"weights/blobs of f32s\".</li>\n<li>Taking a neural network's weights and \"encoding\" them somehow in a Lean4 db of theorems would be considered cheating. :-)</li>\n</ol>\n<p>One reason I consider this might be possible is this:</p>\n<ol>\n<li>IMO style problems often require some form of <code>inspiration</code>, i.e. a step that says <code>Consider the object/strategy XYZ</code> where <code>XYZ</code> is a somewhat complicated expr that we need to come up with <code>out of thin air</code>.</li>\n<li>ASHME/AIME problems are \"more direct\"; after formalization, many of them just look like plug &amp; play standard formulas, the 2d/3d geometry problems can often solved by \"coordinates\", combinatorics problems by case analysis, etc ...</li>\n</ol>\n<p>Concretely, a problem that shows up on IMO but never on ASHME/AIME is something like this: \"Consider [this weird 2 person game]. State which player has a winning strategy. Prove it.\"</p>\n<p>So a solution to this involves this \"inspiration step\" of \"wtf is the strategy; what are the invariants after each player makes their move\"; this coming up of this expr/strategy from thin air. In contrast, the ASHME/AIME feels a bit more \"bruteforce-able\".</p>\n<p>Thus, I too am curious what others are doing with this.</p>",
        "id": 562785840,
        "sender_full_name": "TongKe Xue",
        "timestamp": 1765310291
    },
    {
        "content": "<p>Why would this be meaningful? (I'm not doubting) I just want to know the motivation.</p>\n<p>Lean4 is not exactly the most optimal frontend of interaction. The room to optimize is huge. I believe you can cooptimize NN and software and make &lt;1B LLM work for these problems. And you didn't mention time complexity at all so it could try as many times as possible.</p>\n<p>For things like image editing, I could imagine things like privacy concerns that motivates minizing the models. For math itself, what would be the motivation?</p>",
        "id": 565347993,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1766647160
    },
    {
        "content": "<p>You're right. I failed to state time complexity.</p>\n<p>I'd like to have something that can solve IMO/Putnam problems on a &lt; $10k server, at around 5-10 mins / problem. I don't think this is realistic as of right now, but maybe something that could solve ASHME/AIME on a &lt; $10k server, at around 5-10 mins / problem is possible.</p>\n<p>Suppose such a system existed, it would be interesting to use it to synthesize formally verifiable code.</p>\n<p>I think it is very \"shaky foundations\" to build \"on top\" of remote blackbox APIs.</p>",
        "id": 565391518,
        "sender_full_name": "TongKe Xue",
        "timestamp": 1766697707
    },
    {
        "content": "<p>It has a long way to go from there to formally verifiable code.</p>\n<p>Code is quite complex.</p>\n<p>As much as people would like to boast, there's quite a lot of ground to cover for code generation before it's useful for general programming.</p>\n<p>Code verification is drastically different from math verification and Lean4 didn't really progress a lot for code verification from Rocq, etc. Currently people like to translate things to lean then verify it in lean then export it. That's an interesting use case but is quite insufficient for general stuffs.</p>\n<p>I'm waiting to see Vst-scale projects in Lean4.</p>",
        "id": 565392344,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1766699197
    },
    {
        "content": "<p>And I don't think cost plays a factor here. These costs are like nothing compared with modern LLM use cases. I think the real bottleneck isn't in the mlsys of neural networks.</p>",
        "id": 565392498,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1766699452
    },
    {
        "content": "<p>I'm actually working on such a system that do minif2f-level problems efficiently. But it's mostly a research interest rather than hoping it's of any immediate huge impact. I would say the room to optimize is quite large. I'm just trying to grab the low-hanging fruit now that it still hasn't attracted many MLsys people to work on that.</p>",
        "id": 565392584,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1766699604
    }
]