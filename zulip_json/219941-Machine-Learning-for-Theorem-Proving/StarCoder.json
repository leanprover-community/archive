[
    {
        "content": "<p>Open-source LLM trained on code. 80 programming languages, including Lean (I think Lean 3)<br>\nPaper: <a href=\"https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view\">https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view</a><br>\nCode: <a href=\"https://huggingface.co/bigcode\">https://huggingface.co/bigcode</a></p>",
        "id": 356036363,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1683288751
    },
    {
        "content": "<p>Please don't take this wrong, just jumping to the facts.</p>\n<p>From - Table 1: Overview of the training data for StarCoder.</p>\n<p>Language  Percentage<br>\nlean             0.012            </p>\n<p>Yes 1% would be 1.0</p>\n<p>Just weeks ago checked StarCoder with regards to Prolog so knew to look for this detail.</p>\n<p>Language  Percentage<br>\nprolog        0.001  </p>\n<p><span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span></p>",
        "id": 356044432,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1683290624
    },
    {
        "content": "<p>Yeah, 0.012% is not a lot. That’s 0.09 GB, so just 90 MB. Not sure how well it’ll perform, but it’s open source, so weights can be fine-tuned.</p>",
        "id": 356058109,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1683293221
    },
    {
        "content": "<p>Has anyone tried StarCoder? I tried to get started (which is actually pretty easy), but the downloads time out every time.</p>",
        "id": 356414358,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1683438409
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"614995\">Frederick V</span> has marked this topic as resolved.</p>",
        "id": 356508663,
        "sender_full_name": "Notification Bot",
        "timestamp": 1683473001
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"614995\">Frederick V</span> has marked this topic as unresolved.</p>",
        "id": 356509021,
        "sender_full_name": "Notification Bot",
        "timestamp": 1683473092
    },
    {
        "content": "<p>Even though starcoder's training set is only 0.012% Lean, that's the same ballpark as models we've already seen exhibit nontrivial Lean ability, like gpt-j, code-davinci-002, and gpt-3.5/4. Note that <a href=\"https://arxiv.org/abs/2210.03057\">language models are really good at generalizing to low-resource languages</a>.</p>\n<p>Of course finetuning will improve performance a lot. In two weeks, Tim Dettmers is releasing code that will allow you to <a href=\"https://twitter.com/Tim_Dettmers/status/1654917326381228033?s=20\">finetune starcoder on a single gpu</a>.</p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/Tim_Dettmers/status/1654917326381228033?s=20\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/260254623a465f8fdd0e05c9fd5419fb992469cd/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313436353135313838343331303638333635322f7634475f3537456e5f6e6f726d616c2e6a7067\"></a><p><a href=\"https://twitter.com/karpathy\">@karpathy</a> Super excited to push this even further:\n- Next week: bitsandbytes 4-bit closed beta that allows you to finetune 30B/65B LLaMA models on a single 24/48 GB GPU (no degradation vs full fine-tuning in 16-bit)\n-  Two weeks: Full release of code, paper, and a collection of 65B models</p><span>- Tim Dettmers (@Tim_Dettmers)</span></div></div>",
        "id": 357095943,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1683665244
    },
    {
        "content": "<p>Let me highlight more recent developments in models for coding and software development:</p>\n<ul>\n<li><a href=\"https://twitter.com/hardmaru/status/1669898590435835906\">WizardCoder</a>, surpassing open-source SOTA by ~20% on HumanEval pass@1. Generated/synthetic data is the future. Not sure how much Lean data is there, but in their repo it's said</li>\n</ul>\n<blockquote>\n<p>At present, our core contributors are preparing the 65B version and we expect to empower WizardLM with the ability to perform instruction evolution itself, aiming to evolve your specific data at a low cost.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html\">Large sequence models for software development activities</a> from Google (see also <a href=\"https://ai.googleblog.com/2023/05/resolving-code-review-comments-with-ml.html\">Resolving code review comments with ML</a>)</li>\n</ul>\n<blockquote>\n<p>The novelty of DIDACT is that it uses the process of software development as the source of training data for the model, rather than just the polished end state of that process, the finished code. By exposing the model to the contexts that developers see as they work, paired with the actions they take in response, the model learns about the dynamics of software development and is more aligned with how developers spend their time.</p>\n</blockquote>\n<p>On the other hand, it seems leanprover-community doesn't have access to <a href=\"https://githubnext.com/projects/copilot-for-pull-requests\">GitHub Copilt for PRs</a> yet.</p>",
        "id": 367254966,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1687038289
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224323\">Junyan Xu</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/StarCoder/near/367254966\">said</a>:</p>\n<blockquote>\n<p>Let me highlight more recent developments in models for coding and software development:</p>\n<ul>\n<li><a href=\"https://twitter.com/hardmaru/status/1669898590435835906\">WizardCoder</a>, surpassing open-source SOTA by ~20% on HumanEval pass@1. Generated/synthetic data is the future. Not sure how much Lean data is there, but in their repo it's said</li>\n</ul>\n<p>A couple of possible issues with this evaluation are (1) the initial prompts used for generating more prompts could be leaking data from HumanEval (2) ChatGPT is used to generate more prompts and could have knowledge of HumanEval which is another source of data leakage. Another issue with this methodology is that it seems upper bounded by how good the LLM used to generate the responses to the prompts is.</p>\n</blockquote>",
        "id": 367469726,
        "sender_full_name": "Sid",
        "timestamp": 1687129258
    },
    {
        "content": "<p>(I assume the last part of your message is not intended to be quoted)</p>",
        "id": 367660519,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1687184839
    }
]