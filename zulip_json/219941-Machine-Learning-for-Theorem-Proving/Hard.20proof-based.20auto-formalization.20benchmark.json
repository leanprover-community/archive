[
    {
        "content": "<p>I'm making a thread just for my proof-based autoformalization benchmark proposal.  It is the first benchmark mentioned in my slides <a href=\"https://docs.google.com/presentation/d/1Eukgx0bzCoKi4bhKOFGUJb9nJNBD0JdYnZliWN7NfnE/edit?usp=sharing\">here</a>.  (One can still use <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Talk.3A.20Preparing.20for.20the.20next.20stage.20in.20autoformalization/with/514543941\">#Machine Learning for Theorem Proving &gt; Talk: Preparing for the next stage in autoformalization</a> to discuss my talk in general.)</p>",
        "id": 514687585,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790278
    },
    {
        "content": "<p>This benchmark has a lot of interest and it would be good to get it fleshed out more.  First, I should mention I will likely not be able to participate myself much at all.  (In short, this isn't compatible with my job and I don't have a lot of personal time.)  But nonetheless, I'm excited for others interests.</p>",
        "id": 514687593,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790285
    },
    {
        "content": "<p>To me the most important aspects of this benchmark are the following:</p>\n<ul>\n<li><strong>Tests real-world Lean autoformalization.</strong> There are plenty of Lean proof benchmarks.  This should be focused on auto-formalization even if in some ways it is a secondary emergent property from this benchmark.</li>\n<li><strong>It is scored by proof correctness only.</strong>  This avoids all the issues most common with other autoformalization benchmarks where scoring is much more ambiguous and challenging. </li>\n<li><strong>It is hard.</strong>  The main benchmark should have proofs that contain lemmas or definitions in many of the proofs.  When formalized into Lean it will be more than just a small proof block (or a horribly long proof block if it is).</li>\n<li><strong>It avoids near future data leakage.</strong>  If you make the benchmark out of problems which will be formalized soon (what <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> calls the rising tide of Mathlib) then there is a real danger that these problems will be formalized and contaminate the benchmark.  So no work-a-day theorems, common theorems, famous theorems, etc.  Pick really non-standard stuff.  The subjects can be standard, and maybe should be, since they need to be formalized using existing mathlib definitions (or short non-mathlib definitions).</li>\n<li><strong>It is very carefully vetted.</strong>  The main issue with Lean/Rocq/Isabelle benchmarks are that they contain a lot (like  about a quarter in some cases) of misformalizations.  This can make problems too easy or impossible (and in this case, not fit the proof).  So problems need to be triple-checked for correctness.  Assumptions and edge cases need to be carefully vetted.</li>\n</ul>",
        "id": 514687599,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790292
    },
    {
        "content": "<p>Things I don't feel as strongly about:</p>\n<ul>\n<li>Should there be an easier trial version to start?  This would be a version with no lemmas or definitions in the proofs.  The proofs would be short and simple.  The two obvious candidates are MiniF2F and ProofNet.  (PutnamBench could work too but may be more work since the proofs are probably in PDF format as pointed out to me by <span class=\"user-mention\" data-user-id=\"890706\">@Zeyu Zheng</span> .)</li>\n<li>How do we avoid copyright issues?  Probably the best solution is to link to papers or write proofs from scratch, but don't copy paper text.</li>\n<li>How to set up the tiers?  I envisioned three tiers.  Tier 1 was written by hand and self-contained.  Tier 2 was self-contained papers.  Tier 3 was non-self-contained papers.  Maybe this is too many or not the right levels.  (<span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> pointed out that you could make a Tier 3 into a Tier 2 by including the prerequisite lemmas as assumptions to the main theorem.)</li>\n<li>Where to store stuff?  The simplest would be to make a repo.</li>\n<li>How to organize people and work?  Let's discuss here.</li>\n<li>Do we want training data or just test/benchmark data?  Training data is great if easy, but not necessary.  If for some reason one can easily come up with many examples, sure generate training data too.  But if not, then a testing benchmark is sufficient.</li>\n<li>How many examples?  I was envisioning 100 in each tier (if doing tiers), but that is open for discussion.  Fewer may be fine as it is so hard.</li>\n<li>Should we avoid images and scanned PDFs for now?  If so, then best to find public arxiv papers with LaTeX available (or our own written examples).  (Again, we only need a link to the arxiv, not to duplicate the text in the benchmark.)</li>\n</ul>",
        "id": 514687609,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790301
    },
    {
        "content": "<p>If you are interested in participating, maybe mention it here.</p>",
        "id": 514687633,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790317
    },
    {
        "content": "<p>Here is Kim's proposal.  (She said there is an intentional mistake.)</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span>\n\n<span class=\"kn\">open</span><span class=\"w\"> </span><span class=\"n\">ArithmeticFunction</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">sigma</span><span class=\"o\">)</span>\n<span class=\"kn\">open</span><span class=\"w\"> </span><span class=\"n\">Real</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">exp</span><span class=\"w\"> </span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"n\">eulerMascheroniConstant</span><span class=\"o\">)</span>\n\n<span class=\"kn\">namespace</span><span class=\"w\"> </span><span class=\"n\">LagariasElementaryStatement</span>\n\n<span class=\"kn\">local</span><span class=\"w\"> </span><span class=\"kn\">notation</span><span class=\"w\"> </span><span class=\"s2\">\"γ\"</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">eulerMascheroniConstant</span>\n<span class=\"c1\">-- There is an intentional elementary error in the statement of the problem here,</span>\n<span class=\"c1\">-- as a test of the review proceess!</span>\n\n<span class=\"sd\">/--</span>\n<span class=\"sd\">This is the statement (without proof) of Theorem 1 from</span>\n<span class=\"sd\">G. Robin, Grandes valeurs de la fonction somme des diviseurs et hypothèse de Riemann,</span>\n<span class=\"sd\">J. Math. Pures Appl. 63 (1984), 187-213.</span>\n<span class=\"sd\">-/</span>\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">Robin1</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">    </span><span class=\"n\">RiemannHypothesis</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">≥</span><span class=\"w\"> </span><span class=\"mi\">5041</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">sigma</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">exp</span><span class=\"w\"> </span><span class=\"n\">γ</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span>\n\n<span class=\"sd\">/--</span>\n<span class=\"sd\">This is a statement from Lagarias, without proof,</span>\n<span class=\"sd\">that follows from Proposition 1 in Section 4 of the same paper.</span>\n<span class=\"sd\">-/</span>\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">Robin2</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">    </span><span class=\"bp\">¬</span><span class=\"w\"> </span><span class=\"n\">RiemannHypothesis</span><span class=\"w\"> </span><span class=\"bp\">→</span>\n<span class=\"w\">      </span><span class=\"bp\">∃</span><span class=\"w\"> </span><span class=\"n\">β</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">β</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"n\">β</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"bp\">∃</span><span class=\"w\"> </span><span class=\"n\">C</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">C</span><span class=\"w\"> </span><span class=\"bp\">∧</span>\n<span class=\"w\">        </span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">∃</span><span class=\"w\"> </span><span class=\"n\">n'</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">n'</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"n\">sigma</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"n\">exp</span><span class=\"w\"> </span><span class=\"n\">γ</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">C</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">^</span><span class=\"w\"> </span><span class=\"n\">β</span>\n\n<span class=\"sd\">/--</span>\n<span class=\"sd\">This the statement, without proof, of Lagarias's</span>\n<span class=\"sd\">\"An Elementary Problem Equivalent to the Riemann Hypothesis\",</span>\n<span class=\"sd\">Amer. Math. Monthly 109 (2002), 534-543.</span>\n<span class=\"sd\">-/</span>\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">Lagarias</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">    </span><span class=\"n\">RiemannHypothesis</span><span class=\"w\"> </span><span class=\"bp\">↔</span><span class=\"w\"> </span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\"> </span><span class=\"bp\">≤</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">sigma</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">harmonic</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">exp</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">harmonic</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">log</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">harmonic</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span>\n\n<span class=\"sd\">/--</span>\n<span class=\"sd\">This is the intended benchmark goal,</span>\n<span class=\"sd\">using https://doi.org/10.1080/00029890.2002.11919883 as the source material.</span>\n<span class=\"sd\">-/</span>\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">LagariasGivenRobin</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">    </span><span class=\"n\">Robin1</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">Robin2</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">Lagarias</span>\n\n<span class=\"kn\">end</span><span class=\"w\"> </span><span class=\"n\">LagariasElementaryStatement</span>\n</code></pre></div>",
        "id": 514687869,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790523
    },
    {
        "content": "<p>It also seems to have an arxiv version: <a href=\"https://arxiv.org/abs/math/0008177\">https://arxiv.org/abs/math/0008177</a></p>",
        "id": 514688023,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790644
    },
    {
        "content": "<p>Thanks for sharing the aspects on autoformalization benchmarking. This is great! I'd be interested in participating. </p>\n<ul>\n<li>I think the dynamic aspect of the potential benchmarking is great (we have some recent work on dynamic benchmarking on testing other aspects such as knowledge components of LLMs). </li>\n<li>For testing vs. training, yeah it depends on the difficulty of acquisition of data. They can also come from the same pipeline, but for testing data we do more review and manual validation to ensure the quality, and for training we can rely on more automated data extraction and annotation pipeline with more tolerance to noise.</li>\n<li>Images and PDFs are very interesting too. I've been thinking about this during the ICERM workshop and I think we can get more data from different sources with multimodal LLMs (the quality is another question, but could be good based on my working with vision-language generation).</li>\n</ul>",
        "id": 515081306,
        "sender_full_name": "Joe Zhou",
        "timestamp": 1745937310
    },
    {
        "content": "<p>I did a little digging and the search term \"<a href=\"https://arxiv.org/search/?query=%22self-contained+proof%22&amp;searchtype=all&amp;source=header\">self-contained proof</a>\" gives some interesting hits on Arxiv.  Some of these theorems are famous (or maybe even in mathlib already), so they may not make a good benchmark.  But it is a good start to the exploration of what is out there.  Some samples:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2405.04588\">The Wedderburn-Artin Theorem</a></li>\n<li><a href=\"https://arxiv.org/abs/2408.10282\">A combinatorial proof of Cramer's Rule</a></li>\n<li><a href=\"https://arxiv.org/abs/2405.06300\">$L^p$ estimate of the heat equation on a bounded domain</a></li>\n<li><a href=\"https://arxiv.org/abs/2206.05801\">An Elementary Proof of the Local Kronecker-Weber Theorem</a></li>\n</ul>",
        "id": 515188795,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745978286
    },
    {
        "content": "<p>Another possible source of problems are counterexamples, like in the book counterexamples in topology (although I'm sure there are many in other subjects besides Topology).</p>\n<p>Advantages:</p>\n<ul>\n<li>Easy to state.  (There exists an X which is not Y.)</li>\n<li>They involve complicated one-off constructions.</li>\n<li>They are often self-contained.</li>\n<li>They aren't the sort of thing I see in Mathlib right now, so they might be safe from human formalizers getting in the way.</li>\n<li>Could find a bunch of examples by taking two related concepts X and Y in Mathlib, and asking an expert if there is an X which is not a Y or a related theorem.</li>\n</ul>\n<p>Disadvantages</p>\n<ul>\n<li>They could be of interest to someone (not in Mathlib, but in the Lean community) who wants to formalize such things, so they may not be perfectly immune to the rising tide of formalization.</li>\n<li>We might need to write up the proofs ourselves.</li>\n</ul>",
        "id": 515190419,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745979212
    },
    {
        "content": "<p>A potential source of problems can be extracted from online databases, e.g. the <a href=\"https://topology.pi-base.org/theorems\">pi-base topology database</a> or <a href=\"https://groupprops.subwiki.org/wiki/Main_Page\">the groupprops wiki</a>. That would require manually writing up proofs. <br>\nEvery claim in such databases could be a potential theorem. E.g. for pi-base, every pair (space, property) and every pair (property, property) could be a potential problem (prove an implication or its negation), usually on the easier side. <br>\nThough something like this might be better suited for a (larger) benchmark without corresponding paper proof, since writing/finding a paper proof for these might be time-consuming.</p>",
        "id": 515262360,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1746008154
    },
    {
        "content": "<p>A lot of classic counterexamples might in general be \"welcome\" in some form to <a href=\"https://github.com/leanprover-community/mathlib4/tree/master/Counterexamples\">https://github.com/leanprover-community/mathlib4/tree/master/Counterexamples</a> . Not saying they're at high risk of being added to Mathlib soon, but, it's possible.</p>",
        "id": 515336009,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1746027138
    },
    {
        "content": "<p>I don't think the Counterexamples folder is suited for a large collection of not-super-interesting counterexamples, so a typical <code>X</code> in pi-base should probably not go in that folder, unless it has a mathematically interesting idea. (I'm not speaking on behalf of the maintainers here, just my opinion.)</p>",
        "id": 515336744,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1746027305
    },
    {
        "content": "<p>I'd be interested in participating, and I've already been starting on <a href=\"https://arxiv.org/pdf/1810.11158\">formalizing one of my early papers from grad school</a>. Some questions:</p>\n<ul>\n<li>While the theorems here are purely theoretical, they are about idealized neural networks, so they might be considered \"CS\" rather than strictly \"math\". Is this acceptable?</li>\n<li>The theorems would require maybe a few dozens of lines of code to make auxiliary definitions for the Wasserstein distance and the feedforward network evaluation function. Is it still self-contained if I add these definitions to the file?</li>\n<li>I don't really expect this paper itself to be in mathlib soon, but perhaps Wasserstein distance could be. Is this a problem from the perspective of the results getting easier over time due to more API coming out on aspects of the problem, or form the perspective of a new definition with perhaps slightly different API coming out to conflict with the one in the benchmark?</li>\n<li>This paper has a bunch of independent theorems, but the most interesting/difficult are in section 2. Should I formalize all the theorems or just the main ones?</li>\n</ul>",
        "id": 515433118,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1746065716
    },
    {
        "content": "<p>Maybe bachelor / master theses are a good source of problems. They are often much more self-contained than published papers, and are often also not the kind of results that we will put in Mathlib soon.</p>",
        "id": 515490354,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1746099692
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span>:</p>\n<ul>\n<li>(Mathematical) CS would be perfectly acceptable.  Diversity was one of my initial goals.</li>\n<li>Adding auxiliary definitions is fine.  I do that in the examples in my talk.</li>\n<li>As for mathlib, the worry I have is just that others will formalize our examples and then those formalizations will either leak into the training data or the problems will become trivial since that same theorem is in the library.  So I'm not just worried about mathlib, but about any formalization project which formalizes the whole proof.  Do you plan to formalize your proof and put it on github?</li>\n<li>I haven't given a lot of thought into independent theorems.  I was initially thinking one theorem per paper.  (If we have a training data set, which we might not, it may make sense to have multiple theorems per paper.  For testing, I don't see the point in having more than one.  And certainly we shouldn't have the same paper in both training and test, even if different theorem statements.)  So I would say just pick the one theorem you best think fits this dataset.</li>\n</ul>",
        "id": 515502709,
        "sender_full_name": "Jason Rute",
        "timestamp": 1746105165
    },
    {
        "content": "<p>Ok, here is my proposal. If there's a bug, it's likely to be in the measure theory component, since that's the part of Mathlib I'm least familiar with</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span>\n\n<span class=\"kn\">open</span><span class=\"w\"> </span><span class=\"n\">MeasureTheory</span><span class=\"w\"> </span><span class=\"n\">Classical</span>\n\n<span class=\"sd\">/--</span>\n<span class=\"sd\">This is an autoformalization goal based on this [paper](https://proceedings.neurips.cc/paper_files/paper/2018/file/9bd5ee6fe55aaeb673025dbcb8f939c1-Paper.pdf).</span>\n<span class=\"sd\">[arXiv version here](https://arxiv.org/abs/1810.11158)</span>\n<span class=\"sd\">-/</span>\n\n<span class=\"c1\">-- Defintion 1</span>\n<span class=\"kn\">noncomputable</span><span class=\"w\"> </span><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">WassersteinDistance</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">α</span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"n\">MetricSpace</span><span class=\"w\"> </span><span class=\"n\">α</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"n\">MeasurableSpace</span><span class=\"w\"> </span><span class=\"n\">α</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">μ</span><span class=\"w\"> </span><span class=\"n\">ν</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Measure</span><span class=\"w\"> </span><span class=\"n\">α</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ENNReal</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">  </span><span class=\"n\">sInf</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"bp\">∃</span><span class=\"w\"> </span><span class=\"n\">π</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Measure</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">α</span><span class=\"w\"> </span><span class=\"bp\">×</span><span class=\"w\"> </span><span class=\"n\">α</span><span class=\"o\">),</span><span class=\"w\"> </span><span class=\"n\">π</span><span class=\"bp\">.</span><span class=\"n\">fst</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"n\">μ</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"n\">π</span><span class=\"bp\">.</span><span class=\"n\">snd</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"n\">ν</span><span class=\"w\"> </span><span class=\"bp\">∧</span>\n<span class=\"w\">            </span><span class=\"k\">if</span>\n<span class=\"w\">              </span><span class=\"n\">MeasureTheory</span><span class=\"bp\">.</span><span class=\"n\">Integrable</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"k\">fun</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">xy</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">_</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">dist</span><span class=\"w\"> </span><span class=\"n\">xy</span><span class=\"bp\">.</span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"n\">xy</span><span class=\"bp\">.</span><span class=\"m\">2</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"n\">π</span>\n<span class=\"w\">            </span><span class=\"k\">then</span>\n<span class=\"w\">              </span><span class=\"o\">(</span><span class=\"n\">ENNReal</span><span class=\"bp\">.</span><span class=\"n\">ofReal</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"bp\">∫</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">xy</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">_</span><span class=\"o\">),</span><span class=\"w\"> </span><span class=\"n\">dist</span><span class=\"w\"> </span><span class=\"n\">xy</span><span class=\"bp\">.</span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"n\">xy</span><span class=\"bp\">.</span><span class=\"m\">2</span><span class=\"w\"> </span><span class=\"bp\">∂</span><span class=\"n\">π</span><span class=\"o\">))</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"n\">x</span>\n<span class=\"w\">            </span><span class=\"k\">else</span>\n<span class=\"w\">              </span><span class=\"bp\">⊤</span>\n<span class=\"w\">        </span><span class=\"o\">}</span>\n\n<span class=\"kn\">inductive</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"kt\">Type</span><span class=\"w\"> </span><span class=\"kn\">where</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">single</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Matrix</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">cons</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"n\">hidden_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Matrix</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">hidden_dim</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">tail</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">hidden_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">ReLU</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">k</span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">max</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">)</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"bp\">.</span><span class=\"n\">eval</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">}</span>\n<span class=\"w\">    </span><span class=\"o\">(</span><span class=\"n\">input</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">input_dim</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">single</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"bp\">.</span><span class=\"n\">mulVec</span><span class=\"w\"> </span><span class=\"n\">input</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">cons</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">tail</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"bp\">@</span><span class=\"n\">tail</span><span class=\"bp\">.</span><span class=\"n\">eval</span><span class=\"w\"> </span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">ReLU</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">a</span><span class=\"bp\">.</span><span class=\"n\">mulVec</span><span class=\"w\"> </span><span class=\"n\">input</span><span class=\"o\">))</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"bp\">.</span><span class=\"n\">node_count</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"o\">:</span>\n<span class=\"w\">    </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℕ</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">single</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">o</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">cons</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"bp\">_</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">tail</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">tail</span><span class=\"bp\">.</span><span class=\"n\">node_count</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"bp\">.</span><span class=\"n\">layer_count</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"o\">:</span>\n<span class=\"w\">    </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℕ</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">single</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">1</span>\n<span class=\"w\">  </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">cons</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"n\">o</span><span class=\"w\"> </span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">tail</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">tail</span><span class=\"bp\">.</span><span class=\"n\">layer_count</span>\n\n<span class=\"c1\">-- The uniform measure on [0,1]^d</span>\n<span class=\"kn\">noncomputable</span><span class=\"w\"> </span><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">UniformCube</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Measure</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">dim</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">volume</span><span class=\"bp\">.</span><span class=\"n\">restrict</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"k\">fun</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"n\">dim</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"bp\">≤</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"bp\">≤</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">)</span>\n\n<span class=\"c1\">-- Theorem 4 of the paper (2.4 of the arXiv version), the intended goal.</span>\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">dimension_expansion_distance_lower_bound</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span>\n<span class=\"w\">  </span><span class=\"bp\">∃</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"bp\">∧</span>\n<span class=\"w\">    </span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">nn</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">NeuralNetwork</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"o\">,</span>\n<span class=\"w\">      </span><span class=\"k\">let</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">nn</span><span class=\"bp\">.</span><span class=\"n\">node_count</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span>\n<span class=\"w\">      </span><span class=\"k\">let</span><span class=\"w\"> </span><span class=\"n\">L</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">nn</span><span class=\"bp\">.</span><span class=\"n\">layer_count</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span>\n<span class=\"w\">      </span><span class=\"k\">let</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">input_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span>\n<span class=\"w\">      </span><span class=\"k\">let</span><span class=\"w\"> </span><span class=\"n\">d</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">output_dim</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span>\n<span class=\"w\">      </span><span class=\"o\">(</span><span class=\"n\">WassersteinDistance</span>\n<span class=\"w\">        </span><span class=\"o\">((</span><span class=\"n\">UniformCube</span><span class=\"w\"> </span><span class=\"n\">input_dim</span><span class=\"o\">)</span><span class=\"bp\">.</span><span class=\"n\">map</span><span class=\"w\"> </span><span class=\"n\">nn</span><span class=\"bp\">.</span><span class=\"n\">eval</span><span class=\"o\">)</span>\n<span class=\"w\">        </span><span class=\"o\">(</span><span class=\"n\">UniformCube</span><span class=\"w\"> </span><span class=\"n\">output_dim</span><span class=\"o\">))</span><span class=\"bp\">.</span><span class=\"n\">toReal</span>\n<span class=\"w\">      </span><span class=\"bp\">≥</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"o\">((</span><span class=\"n\">Real</span><span class=\"bp\">.</span><span class=\"n\">exp</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">L</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Real</span><span class=\"bp\">.</span><span class=\"n\">exp</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">))</span><span class=\"w\"> </span><span class=\"bp\">^</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"bp\">-</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">L</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">d</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">))</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n<span class=\"w\">  </span><span class=\"gr\">sorry</span>\n</code></pre></div>",
        "id": 516372382,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1746517543
    },
    {
        "content": "<p>(<span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span> your theorem can be proven by <code>use 0; simp</code>)</p>",
        "id": 517427124,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1747024576
    },
    {
        "content": "<p>Nice catch - seems the stipulation that <code>k</code> be positive is absent from my paper as well. Kudos for catching what me, my advisor, and apparently the NeurIPS reviewers did not.</p>",
        "id": 517690339,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1747092859
    },
    {
        "content": "<p>This sort of stuff is really going to be the main issue with making this benchmark.  Edge cases, common lean mistakes, and other correctness pitfalls will require a very careful review process.</p>",
        "id": 517690906,
        "sender_full_name": "Jason Rute",
        "timestamp": 1747093109
    },
    {
        "content": "<p>Also, problems like yours with lots of new definitions are going to be even more likely to be formalized incorrectly.</p>",
        "id": 517691223,
        "sender_full_name": "Jason Rute",
        "timestamp": 1747093257
    },
    {
        "content": "<p>But it seems like a good problem overall if you can get it to be correct.</p>",
        "id": 517692207,
        "sender_full_name": "Jason Rute",
        "timestamp": 1747093644
    },
    {
        "content": "<p>I am worried about access to your paper.  Is there an arxiv version with LaTeX?</p>",
        "id": 517692265,
        "sender_full_name": "Jason Rute",
        "timestamp": 1747093667
    },
    {
        "content": "<p>Sure, it's <a href=\"https://arxiv.org/abs/1810.11158\">here</a>, though the theorem numbering is different due to NeurIPS style guidelines</p>",
        "id": 517692428,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1747093732
    },
    {
        "content": "<p>I'll add another comment</p>",
        "id": 517692445,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1747093740
    },
    {
        "content": "<p>Also your paper has a lot of background lemmas requiring you to go back to other papers.  Again, it depends if we want these papers to be self-contained or not (or to make them self-contained by providing the background lemmas as hypotheses).</p>",
        "id": 517692738,
        "sender_full_name": "Jason Rute",
        "timestamp": 1747093869
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Hard.20proof-based.20auto-formalization.20benchmark/near/517692207\">said</a>:</p>\n<blockquote>\n<p>But it seems like a good problem overall if you can get it to be correct.</p>\n</blockquote>\n<p>The issue is, even if I had fully formalized the theorem, it wouldn't have caught Thomas Zhu's issue, I would have blithely formalized the correct value of k, used that, and remained unaware there was a shortcut.</p>",
        "id": 517692867,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1747093922
    },
    {
        "content": "<p>WRT lemmas, I think the only external lemma that this particular proof requires is A.2 giving the formula for the maximum number of orthants a k-dimensional plane embedded in R^n can intersect. I cited a textbook for it, but it's also the sort of thing I could imagine being in mathlib (though I can't actually find it).</p>",
        "id": 517694088,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1747094424
    },
    {
        "content": "<p>Perhaps the volume of an n-dimensional unit ball would count too - is that in mathlib?</p>",
        "id": 517694218,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1747094468
    },
    {
        "content": "<p><a href=\"https://leanprover-community.github.io/mathlib4_docs/find/?pattern=InnerProductSpace.volume_ball#doc\">docs#InnerProductSpace.volume_ball</a> yup :)</p>",
        "id": 517694554,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1747094583
    }
]