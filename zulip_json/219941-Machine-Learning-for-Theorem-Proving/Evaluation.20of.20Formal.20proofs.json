[
    {
        "content": "<p>Someone please help me with this, <br>\nIf I have generated a formal proof, how do I evaluate if it is correct or not, can you provide an example also</p>",
        "id": 452235057,
        "sender_full_name": "Siva Gollapalli",
        "timestamp": 1721272113
    },
    {
        "content": "<p>Unless you give us more information about your background and what you've tried already, I think the best response is that you should read the learning resources at</p>\n<p><a href=\"https://leanprover-community.github.io/learn.html\">https://leanprover-community.github.io/learn.html</a></p>",
        "id": 452235696,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1721272536
    },
    {
        "content": "<p>I have used the machine translation evaluation metrics like BLEU but it seems, that is not that good for the proof evaluation.<br>\nIn MiniF2F we don't have ground truth formal proofs right, so how are evaluating the proofs generated by our models</p>",
        "id": 452242849,
        "sender_full_name": "Siva Gollapalli",
        "timestamp": 1721276558
    },
    {
        "content": "<p>There are a few ways.  BLEU is absolutely not the right way.  The point of Lean is that it can check your work.  Please take some time to at least learn a bit more about lean.  If you understand how lean works it will make it easier to explain how to check things from the command line.</p>",
        "id": 452244344,
        "sender_full_name": "Jason Rute",
        "timestamp": 1721277102
    },
    {
        "content": "<p>(For those who don’t know, BLUE is a similarity metric used in machine translation scoring.)</p>",
        "id": 452244489,
        "sender_full_name": "Jason Rute",
        "timestamp": 1721277158
    },
    {
        "content": "<p>I'd suggest reading some basic tutorials so you understand how the proof checking interaction works for humans. After that it's easy to either pipe a proof to the <code>lean</code> executable and look at exit codes or error messages, or to use the <a href=\"https://github.com/leanprover-community/repl\">https://github.com/leanprover-community/repl</a>.</p>",
        "id": 452329260,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1721305007
    },
    {
        "content": "<p>So your first concern is whether to use Lean 3 or Lean 4.  The original MiniF2F and Facebook’s fixed version are in Lean 3, but Lean 3 has been replaced with Lean 4 and this year’s papers have been using Lean 4.  Note, GPT-4 I think still likes to output syntax closer to Lean 3 and needs some coaching to get Lean 4 syntax.</p>\n<p>The next question is which version of MiniF2F to use.  For Lean 3, use the Facebook version.  For Lean 4, I guess the yangkai11 repo is the current de facto standard, but it has many errors.  Don’t use the Hoskinsons Center’s huggingface version!</p>\n<p>The rest is assuming you are using Lean 4.  I assume you aren’t using a language agent and just have an LLM that outputs whole proofs.  That will make your life easier for benchmarking.  Then you have many options.  The simplest is to have a file for each problem, replace <code>sorry</code> with the proof, and then just to Lean from the command line on that file and check for errors.  I don’t remember the command off-hand, but I could find it.  You will need to make sure you have installed Mathlib appropriately (including with compiled binaries).</p>\n<p>Finally, there are a few types of proofs that are bad.  <code>sorry</code> is cheating and Lean will warn you.  There are other tactics that are also cheating and Lean won’t warn you.  This may not come up, but ideally, you would also print the axiom list with <code>#print axioms theorem_name</code> to be sure no weird axioms got in.</p>\n<p>I highly recommend Machine Learning researchers either partner with a Lean expert or take the time to understand Lean well.</p>",
        "id": 452351191,
        "sender_full_name": "Jason Rute",
        "timestamp": 1721310992
    },
    {
        "content": "<p>Also, maybe some other ML researcher is willing to share their checking code.  Many of them are doing the same thing (checking LLM whole proof suggestions for MiniF2F), so it wouldn't hurt to have a good template to get started.</p>",
        "id": 452358011,
        "sender_full_name": "Jason Rute",
        "timestamp": 1721312479
    },
    {
        "content": "<p>It just depends when they scraped the input. Claude 3.5 does full Lean4, I think, for example.</p>",
        "id": 452389497,
        "sender_full_name": "Ralf Stephan",
        "timestamp": 1721320472
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Evaluation.20of.20Formal.20proofs/near/452351191\">said</a>:</p>\n<blockquote>\n<p>Don’t use the Hoskinsons Center’s huggingface version!</p>\n</blockquote>\n<p>If you feel strongly about this, is it worth contacting the Hoskinson center and suggesting it be taken down / have a warning attached? This might be better resolved privately between you and them, since asking people to throw away their work is not something you want a dogpile to form around.</p>",
        "id": 452457482,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1721346621
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Evaluation.20of.20Formal.20proofs/near/452351191\">said</a>:</p>\n<blockquote>\n<p>Note, GPT-4 I think still likes to output syntax closer to Lean 3 and needs some coaching to get Lean 4 syntax.</p>\n</blockquote>\n<p>Yes, this is still true when using ChatGPT.</p>",
        "id": 452548763,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1721387626
    }
]