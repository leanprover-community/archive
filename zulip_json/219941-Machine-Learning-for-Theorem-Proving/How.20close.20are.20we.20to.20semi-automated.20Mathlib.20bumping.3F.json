[
    {
        "content": "<p>As we are discovering in <a class=\"stream-topic\" data-stream-id=\"458659\" href=\"/#narrow/channel/458659-Equational/topic/Bump.20to.20v20.2E0-rc5/with/517715804\">#Equational &gt; Bump to v20.0-rc5</a> , doing a Mathlib bump from a version that is more than a month out of date is surprisingly painful when the codebase becomes sufficiently large and complex.  On the other hand, out of all the challenging autoformalization tasks out there, the task of converting a proof in \"Lean + Mathlib version n\" to a proof in \"Lean + Mathlib version n+h\" (for smallish values of h) seems like the lowest hanging fruit.  How close are we to being able to develop tooling to accelerate this particular task?  It seems that all the existing Lean repositories might already have some suitable training data for this task that could be scraped for this.</p>",
        "id": 517753571,
        "sender_full_name": "Terence Tao",
        "timestamp": 1747120359
    },
    {
        "content": "<p>To begin with, 80% of the work (deprecations and imports) can be done without any AI help, but by a simple script. There exist various versions of scripts for each of these (deprecated lemmas, deprecated modules, fixing missing imports); centralising them into one would be great.</p>",
        "id": 517753855,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1747120488
    },
    {
        "content": "<p>(I don't know the answer about the remaining 20%. Certainly, not all of these repeat: if a Lean core change happens, there are probably little past changes which are exactly similar to learn from. That said, these changes should require relatively little manual work.)</p>",
        "id": 517754119,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1747120588
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"634338\">@Michael Rothgang</span> If we can integrate those tools, we can then try layering an LLM on top of them which we feed the Lean GitHub diff of Lean vA.B.C -&gt; vX.Y.Z to handle the remaining 20%. Let’s say it goes one error at a time, tries to resolve it, and then submits a pull request. We will probably have to automatically show it previous changes the LLM made related to the current problem it is working on. This all seems doable. I know in certain cases Lean version changes can make updating metaprogramming code difficult (this was an issue LeanDojo ran into), but bumping proofs should be fine. What do you think?</p>",
        "id": 517887812,
        "sender_full_name": "Justin Asher",
        "timestamp": 1747157636
    },
    {
        "content": "<p>That seems like the natural thing to try, for throwing a LLM at it. I'm personally a bit sceptical if LLM can do well for those --- the <em>root causes</em> will often be different, and you won't be trying to diagnose those, right? I might be wrong, of course.</p>",
        "id": 517888283,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1747157789
    },
    {
        "content": "<p>I like the long-term vision of <code>lean exe autobump</code>, where one writes automatic migrations for each breaking change in mathlib, and Lean applies that --- ideally, that's fully type-safe and correct. (It means spending effort on these migrations, though.)</p>",
        "id": 517888398,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1747157833
    },
    {
        "content": "<p>Migrations as a concept have long pre-dated LLMs. For changes of names and default tactic configs, there is usually a simple translation from the old tactic + config to new tactic + config. These should be reasonably simple (although in lean, parsing the end of one application of a tactic accurately can be tricky). For example if the new default behaviour of <code>simp</code> excluded <code>decide</code>, then a migration would replace every existing application of <code>simp</code> with <code>simp +decide</code>. But there are downsides to simple syntactic replacement.</p>",
        "id": 518072046,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747230622
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"634338\">Michael Rothgang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/How.20close.20are.20we.20to.20semi-automated.20Mathlib.20bumping.3F/near/517888398\">said</a>:</p>\n<blockquote>\n<p>I like the long-term vision of <code>lean exe autobump</code>, where one writes automatic migrations for each breaking change in mathlib, and Lean applies that --- ideally, that's fully type-safe and correct. (It means spending effort on these migrations, though.)</p>\n</blockquote>\n<p>See also <a class=\"stream-topic\" data-stream-id=\"287929\" href=\"/#narrow/channel/287929-mathlib4/topic/leanup.20design.20discussion/with/412831357\">#mathlib4 &gt; leanup design discussion</a></p>",
        "id": 518188281,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1747272249
    },
    {
        "content": "<p>Thanks for linking - that was what I meant :-)</p>",
        "id": 518214162,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1747289726
    },
    {
        "content": "<p>IMHO given that we've seen AI agents come up from non-trivial proofs from scratch, it's certain that we could automate the proof repair component of bumping an upstream repository. I believe it just requires somebody with sufficient motivation to build such a tool. I also believe that what is needed is not a \"proof of concept\" but rather a production-ready tool.</p>",
        "id": 518236012,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1747297196
    },
    {
        "content": "<p>I guess one idea would be that you train on the mathlib bump diff.</p>",
        "id": 518237020,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1747297477
    },
    {
        "content": "<p>My guess is that there wouldn't be nearly enough such diffs for this to be more useful than just throwing a general purpose Lean proving agent at the problem.</p>",
        "id": 518245301,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1747299682
    },
    {
        "content": "<p>A few such diffs may be useful in a prompt.</p>",
        "id": 518273678,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1747308083
    },
    {
        "content": "<p>The problem of proof repair is still a serious research question. I wonder if it will be that simple and produce consistently correct results</p>",
        "id": 518274197,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747308258
    },
    {
        "content": "<p>I guess I'm just repeating myself in different words but I'm convinced that it is <em>not</em> a serious research question to build a very useful tool.</p>",
        "id": 518274897,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1747308478
    },
    {
        "content": "<p>I think it's more of a difficult engineering problem than an interesting research question</p>",
        "id": 518276444,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1747308895
    },
    {
        "content": "<p>Based on the recent literature, I disagree (about the non-research nature of practical proof repair tools). But the problem in lean upgrades is a much more classic and simple one that can be solved by migration rules applied systematically to syntax trees. Ideally you don't actually want any non local changes to be made in any migration. For example, you don't want an old proof in mathlib replaced by a new one. You merely want pieces of the old proof such as definition names or tactic calls to be altered in a systematic way. In other words, modern AI tools might actively make things more complicated than they need to be, especially when the same changes need to be made across dozens or hundreds of files systematically</p>",
        "id": 518280417,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747310000
    },
    {
        "content": "<p>Less talk, more code.</p>",
        "id": 518281104,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1747310178
    },
    {
        "content": "<p>I think version migration across Mathlib releases definitely seems like one of the more tangible and data-rich proof engineering challenges—especially since breakages are often local but still nontrivial to resolve. In practice, many of these edits involve small but semantically meaningful adjustments: adapting to renamed declarations, modified type signatures, reorganized imports, or even subtle changes in tactic behavior.</p>\n<p>To study this kind of task more systematically, we've started working on a benchmark called <strong>APE-Bench I</strong>, which focuses on instruction-guided, file-level proof edits mined from real Mathlib4 commit history. While it doesn't explicitly target version bumps (yet), many of the tasks in the benchmark are exactly the kinds of localized changes that arise during library migration or maintenance.</p>\n<p>The long-term plan is to extend this into project-scale benchmarks (multi-file, version-aware) and eventually support tools that can assist with version migration workflows more directly. Ideally, this would culminate in LLM-based agents capable of proposing and validating edits in-context. We’re also exploring packaging this as a service layer to support interactive or automated maintenance. Paper link here for anyone interested: <a href=\"https://arxiv.org/abs/2405.06308\">https://arxiv.org/abs/2405.06308</a></p>",
        "id": 518503500,
        "sender_full_name": "Huajian Xin",
        "timestamp": 1747389755
    },
    {
        "content": "<p>I'm glad to see some effort put into this.  <a href=\"#narrow/channel/458659-Equational/topic/Thoughts.20and.20impressions.20thread/near/518164935\">In another thread</a>, <span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> pointed out that once one stops maintaining a project by continually keeping Mathlib up to date, a previous successful Lean formalization project may gradually become increasingly incompatible with the latest version of Mathlib, if the proofs are not upstreamed to Mathlib in a timely fashion.  This sort of \"Lean proof rot\" is not desirable in the long-term, so anything that reduces the friction of bumping would be welcome.</p>",
        "id": 518685122,
        "sender_full_name": "Terence Tao",
        "timestamp": 1747407451
    },
    {
        "content": "<p>Do you think for each bump there will always be a moderate-sized set of rules that can be used to fix the code?  Assuming there's some basic intelligence in applying the rules (e.g.: in <span class=\"user-mention\" data-user-id=\"466334\">@Shreyas Srinivas</span>'s example, if it works without error than keep it as <code>simp</code> but if it fails then try <code>simp +decide</code>).<br>\nIf so then what level of automation is needed for building the migration rules:</p>\n<ul>\n<li>Built manually and explicitly (could be listed in PRs and then automatically collected together)</li>\n<li>Built interactively (for each error, the user is prompted to suggest a fix, which is then broadly applied)</li>\n<li>Learned to generalize from seeing examples of a few fixes</li>\n<li>Task an LLM or proof search system to fix the errors and generate rules</li>\n</ul>",
        "id": 518984313,
        "sender_full_name": "Ralph Furman",
        "timestamp": 1747624026
    },
    {
        "content": "<p>I absolutely think a tool doing \"build manually\" would work. (Search for <code>leanup</code>, Mario Carneiro sketched a tool like this.) Now, somebody \"just\" needs to implement that, and all PR authors need to add migrations for their breaking changes/we need a way to detect that these are happening.</p>",
        "id": 519012880,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1747638447
    },
    {
        "content": "<p>Is there a WIP repo for the leanup project?</p>",
        "id": 519091299,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747656989
    },
    {
        "content": "<p>I think I made the repo but did not put anything in it. Most of the work done on leanup is in the zulip thread</p>",
        "id": 519099558,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1747658805
    },
    {
        "content": "<p>Is it really a bad idea to manipulate terms at the level of TSyntax with sufficient context?</p>",
        "id": 519102198,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747659343
    },
    {
        "content": "<p>I know it is brittle</p>",
        "id": 519102218,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747659347
    },
    {
        "content": "<p>But I don't see stronger alternatives</p>",
        "id": 519102331,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747659363
    },
    {
        "content": "<p>it's not really significantly easier to get TSyntax than full elaborator results if you want a 100% correct answer, because the elaborator can change the parser</p>",
        "id": 519103887,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1747659678
    },
    {
        "content": "<p>On the other end of the spectrum, if I take a fully elaborated proof term <code>Expr</code> and change it,  while leaving the surface tactic syntax untouched, I haven't really \"migrated\" the proof have I?</p>",
        "id": 519104387,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747659785
    },
    {
        "content": "<p>at what point of the elaboration should the change be made?</p>",
        "id": 519104473,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747659806
    },
    {
        "content": "<p>For reference : <a href=\"https://dl.acm.org/doi/10.1145/3453483.3454033\">https://dl.acm.org/doi/10.1145/3453483.3454033</a> EDIT: Arxiv : <a href=\"https://arxiv.org/abs/2010.00774\">https://arxiv.org/abs/2010.00774</a></p>",
        "id": 519105875,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1747660096
    },
    {
        "content": "<p>Has anyone considered some new tools being released, like <a href=\"https://docs.anthropic.com/en/docs/claude-code/overview\">Claude Code</a>, OpenAI's <a href=\"https://openai.com/index/introducing-codex/\">Codex</a>, or Google's <a href=\"https://jules.google/\">Jules</a>, for tasks related to bumping (or ATP in general)? I have not used coding agents yet, and I am curious to hear other peoples' takes. I know that, for instance, Jules runs the code in a VM before submitting a PR, and since I do not yet have access, I am uncertain whether this would work for Lean 4 code (I am presuming not).</p>",
        "id": 519233163,
        "sender_full_name": "Justin Asher",
        "timestamp": 1747691826
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/How.20close.20are.20we.20to.20semi-automated.20Mathlib.20bumping.3F/near/519104387\">said</a>:</p>\n<blockquote>\n<p>On the other end of the spectrum, if I take a fully elaborated proof term <code>Expr</code> and change it,  while leaving the surface tactic syntax untouched, I haven't really \"migrated\" the proof have I?</p>\n</blockquote>\n<p>No, that was never the suggestion. The idea is to use elaborator information to migrate syntax to syntax and pretty print it, or even better, construct diffs and apply them so that you don't muck up the formatting of the rest of the file while applying migrations</p>",
        "id": 519366283,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1747742471
    },
    {
        "content": "<p>This is essentially the functionality of <code>lake exe refactor</code>, except <code>leanup</code> repackages it into a form where the migrations are applied from a database based on where your code is and where it wants to be</p>",
        "id": 519366539,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1747742537
    },
    {
        "content": "<blockquote>\n<p>To begin with, 80% of the work (deprecations and imports) can be done without any AI help, but by a simple script. There exist various versions of scripts for each of these (deprecated lemmas, deprecated modules, fixing missing imports); centralising them into one would be great.</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"634338\">@Michael Rothgang</span> + others, could you link some of these? i haven't been able to easily find stuff online :(</p>",
        "id": 535004514,
        "sender_full_name": "agniv",
        "timestamp": 1755543173
    },
    {
        "content": "<p>Sure: firstly, there are</p>\n<ul>\n<li>deprecation warnings on renamed lemmas (which stay around for at least six months, so as long as you bump e.g. at most 3 Lean versions at a time, you'll get to use them)</li>\n<li>deprecated modules for files renamed or moved</li>\n<li>for mere import changes, you currently have to (1) add <code>import Mathlib</code> at the beginning, then use <code>#min_imports</code> at the end of the file to minimise imports again</li>\n</ul>\n<p>These are just about warnings, not yet about auto-fixing them.</p>",
        "id": 540389995,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1758267076
    },
    {
        "content": "<p>Updating deprecations: most of it can be done with <a href=\"https://github.com/adomani/UpdateDeprecations\">UpdateDeprecations</a> (see <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/update_deprecations/with/444364161\">#general &gt; update_deprecations</a>  and <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/Auto-replace.20deprecated.20aliases.3F/with/513551155\">#general &gt; Auto-replace deprecated aliases?</a>, and also <a class=\"stream-topic\" data-stream-id=\"144837\" href=\"/#narrow/channel/144837-PR-reviews/topic/.2313483.20update_deprecations/with/453230418\">#PR reviews &gt; #13483 update_deprecations</a>). That tool is in the prototype stage, but may be helpful already. Building a robust tool to do this would be very welcome.</p>",
        "id": 540390399,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1758267212
    },
    {
        "content": "<p>(And yes, that would just be engineering work - but very useful one.)</p>",
        "id": 540390439,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1758267226
    },
    {
        "content": "<p>I don't think there is tooling for auto-replacing deprecated modules. This shouldn't be too hard to add to UpdateDeprecations.</p>",
        "id": 540390501,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1758267254
    },
    {
        "content": "<p>Anything that can automate notation changes, say,  <code>\\sum x in Foo</code> to <code>\\sum x \\in Foo</code>?</p>",
        "id": 540391440,
        "sender_full_name": "Jason Rute",
        "timestamp": 1758267606
    },
    {
        "content": "<p>There definitely was a custom deprecation warning, telling you how to fix the code. I believe that was not automated.</p>",
        "id": 540392379,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1758267949
    },
    {
        "content": "<p>(Making linters suggest the right replacement is tricky, currently. But we're slowly getting there, IIUC.)</p>",
        "id": 540392437,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1758267970
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"634338\">Michael Rothgang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/How.20close.20are.20we.20to.20semi-automated.20Mathlib.20bumping.3F/near/540390439\">said</a>:</p>\n<blockquote>\n<p>(And yes, that would just be engineering work - but very useful one.)</p>\n</blockquote>\n<p>For anything repetitive / mechanical, claude code or openai codex should be able to handle it cleanly.  Where it might start to break down is proof repair.  That said, I've heard good things about gpt-5-pro if you have budget.</p>",
        "id": 541113560,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1758667943
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/How.20close.20are.20we.20to.20semi-automated.20Mathlib.20bumping.3F/near/519233163\">said</a>:</p>\n<blockquote>\n<p>am uncertain whether this would work for Lean 4 code (I am presuming not).</p>\n</blockquote>\n<p>Afaik, all recent frontier models are all fairly well versed in lean 4 itself.   However, they do hallucinate some more obscure std things and some mathlib symbols despite being trained on it, especially when proofs get complex and you start going past transformer sliding windows.  For that type of work, you'll want a good mcp installed.   </p>\n<p>I can't recommend enough using agentic search like o3-search or gpt5-search to find updated resources.  I know math can move slowly, but things are starting to accelerate and change quite rapidly.</p>",
        "id": 541116026,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1758669457
    }
]