[
    {
        "content": "<p>I was super compelled by <a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/?utm_source=x&amp;utm_medium=social&amp;utm_campaign=&amp;utm_content=\">AlphaProof</a>, and this got me thinking: are people trying similar things for AI program generation and verification in Lean?  </p>\n<p>I'm specifically compelled by AlphaProof's automatic generation of a bunch of proofs that were subsequently used to continuously train its language model.  Does anyone know of specific people or research groups that are attempting something similar in Lean, but with an emphasis on code generation? (Obv function generation is equivalent to theorem proving in lean, but I'm interested in cases where people are generating algorithms to accomplish specific tasks, not just mathematical theorems).</p>\n<p>Also, if anyone here happens to be involved in this, I'd love to hear about your efforts!</p>",
        "id": 459832982,
        "sender_full_name": "Daniel Geisz",
        "timestamp": 1723356987
    },
    {
        "content": "<p>I don't think there is nearly enough done on this topic.  Here are some things I know from the past few months:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2406.11915\">miniCodeProps: a Minimal Benchmark for Proving Code Properties</a></li>\n<li><a href=\"https://arxiv.org/abs/2406.08467\">DafnyBench: A Benchmark for Formal Software Verification</a></li>\n<li><a href=\"https://arxiv.org/abs/2406.14408\">FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving</a></li>\n</ul>",
        "id": 459946039,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723411314
    },
    {
        "content": "<p>I think the space of <em>humans</em> using Lean to write verified algorithms (if that's what you mean to ask) is itself quite sparse; for instance, I think it's currently quite hard to write any proofs about lean's <code>for</code> loops due to lack of related library theorems.</p>",
        "id": 459947673,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1723412887
    },
    {
        "content": "<p>The space of humans using <em>anything</em> to write verified algorithms or programs is already quite niche... Lean really isn't the go-to tool for that either, as much as I'd like it to be.</p>",
        "id": 459981525,
        "sender_full_name": "Adomas Baliuka",
        "timestamp": 1723433243
    },
    {
        "content": "<p>Yeah, I mean that totally makes sense.  Idk though... it might be starry-eyed thinking but it seems like Lean would be a really interesting environment to train language models to iteratively autogenerate and verify algorithmic code, AlphaProof style.  Excited to dig into this more :)</p>\n<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>, thanks for pointing out those links!</p>",
        "id": 459990506,
        "sender_full_name": "Daniel Geisz",
        "timestamp": 1723436346
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"433922\">@Daniel Geisz</span> consider changing your proof assistant. Verification has been the motivation behind the development of older proof assistants like Coq or Isabelle. I do not know what <span class=\"user-mention\" data-user-id=\"306713\">@Lasse Blaauwbroek</span> and <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> are planning for the Tactician (for Coq), but \"program verification\" training data should abound there. I am working in Isabelle in the early steps towards a Tactician-like environment for Isabelle. The objective is to create tools for Isabelle users, which are mostly verification people like me.</p>",
        "id": 460035562,
        "sender_full_name": "Jonathan Julian Huerta y Munive",
        "timestamp": 1723453688
    },
    {
        "content": "<p>Lean is still young compared to these other proof assistants, but in the past year there has been a very conscious effort to make it more suitable for software verification as well. Of course it can't compare to the ecosystem that the others are offering (yet). All I'm saying with this message is that there is no reason the Lean support for verification right now is indicative of what it will be in the (nearish) future</p>",
        "id": 460072746,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1723465767
    },
    {
        "content": "<p>I agree. Perhaps the verb \"change\" that I used might be misinterpreted here. I never intended to say that a project in Lean would be futile. We need projects in many proof assistants. Rather, I wanted to say that, if one widens the scope of the question, one might find more people/research groups doing \"machine learning for code-generation/verification\", specially since other ITPs have a &gt;30-year old history and the ideas are probably older than that.</p>",
        "id": 461939055,
        "sender_full_name": "Jonathan Julian Huerta y Munive",
        "timestamp": 1723490815
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> are there particular people or orgs behind the push to make lean better for software verification? Trying to get a feel for the state of things</p>",
        "id": 462395222,
        "sender_full_name": "Daniel Geisz",
        "timestamp": 1723658895
    },
    {
        "content": "<p>It is one of the long term focus points of the FRO, and you can find some aspects of it on the two-year FRO roadmap <a href=\"https://lean-fro.org/about/roadmap/\">https://lean-fro.org/about/roadmap/</a><br>\nConcretely, there is active work on integrating with SAT solvers, the <code>omega</code> tactic was developed with software verification as one of the motivations, and the std library is being developed, because before verifying software, you need to write the software...</p>",
        "id": 462396454,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1723659457
    }
]