[
    {
        "content": "<p>I just found the paper <a href=\"https://arxiv.org/abs/2503.21934\">Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad</a>.</p>",
        "id": 509534062,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1743536451
    },
    {
        "content": "<p>It is written by a group of researchers that use the latest public LLMs (reasoning models) to try to solve the 2025 USAMO, and the LLMs did terribly (&lt;2 points out of 42 on average).</p>",
        "id": 509534320,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1743536549
    },
    {
        "content": "<p>This supports the argument (that I've made) that to do complicated mathematical reasoning, you need a trusted verifier (e.g. Lean).<br>\nNot sure how strong this argument is. A counterargument is that it's easy to make a LLM output a bad answer, but maybe with some new techniques/insights you can get significantly better results.</p>",
        "id": 509535147,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1743536856
    },
    {
        "content": "<p>The LLM answers I saw to Putnam problems in December were also abysmal. In contrast the IMO achievement by DeepMind getting a respectable score on the IMO was with Lean. So I am also a proponent of the argument that a trusted verifier is a necessary tool to move these things beyond where they are now.</p>\n<p>I think the video at <a href=\"https://epoch.ai/frontiermath/tier-4\">https://epoch.ai/frontiermath/tier-4</a> is quite illuminating too. Daniel Litt points out that the reason that LLMs can sometimes get hard \"what is this number\" questions right is that it can sometimes be much easier to guess the answer than to prove that your guess is correct. The \"FrontierMath philosophy\" of asking for questions for which you necessarily have to do some hard mathematics correctly and yet the answer is a number, is like asking mathematicians to come up with some kind of amazing compression algorithm, where a very small amount of data (the final answer, a number) is supposed to represent a proof that the model must have done a large amount of deep reasoning accurately. This is a hard problem to solve (which is why Epoch AI are offering $7500 to anyone who can give an example I guess).</p>\n<p><del>Another trick I saw recently coming from the \"guess the right number\" proponents is a test which was run at \"pass@3200\" which apparently is code for \"let the model guess the number 3200 times and it is deemed to get the answer right if it gets it right at least once\". </del>(I retract this claim: I suspect I was thinking about <a href=\"https://arxiv.org/pdf/2502.00212\">https://arxiv.org/pdf/2502.00212</a> which is not about \"guess a number\")</p>\n<p>However there are problems with getting models to write Lean code, for example there is not enough data to train on. It would be nice if we had more details about what DeepMind did but there is still no paper on AlphaProof and instead we have articles like <a href=\"https://on.ft.com/4i6C8Gc\">https://on.ft.com/4i6C8Gc</a> , which does not bode well from the point of view of an academic. Again in the EpochAI video, Sergei Gukov stresses the importance of autoformalization; if we can make great progress here then perhaps the idea of getting LLMs to write Lean code will become closer to a reality, as there will be far more data to train on.</p>",
        "id": 509561507,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1743547922
    },
    {
        "content": "<p>Where did you see ‚Äúpass@3200‚Äù?</p>",
        "id": 509562051,
        "sender_full_name": "Jason Rute",
        "timestamp": 1743548173
    },
    {
        "content": "<p>Also what is this $7500 reward?</p>",
        "id": 509562107,
        "sender_full_name": "Jason Rute",
        "timestamp": 1743548217
    },
    {
        "content": "<p>The $7500 fee per question is mentioned on the Epoch AI Tier 4 page <a href=\"https://epoch.ai/frontiermath/tier-4\">https://epoch.ai/frontiermath/tier-4</a> .  It might be hard for me to find the pass@3200 reference, the tab is long closed. It made me laugh out loud.</p>\n<p>Another paper where LLMs with no formal abilities perform abysmally, this time on a relatively straightforward mathematical task, is documented here <a href=\"https://arxiv.org/abs/2502.07087\">https://arxiv.org/abs/2502.07087</a> .</p>",
        "id": 509563873,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1743549068
    },
    {
        "content": "<p>3200: Oh maybe it was this? <a href=\"https://arxiv.org/abs/2502.00212\">https://arxiv.org/abs/2502.00212</a> although now looking at it, this is asking for formal proofs not numbers, so pass@3200 is far more sensible. I have retracted my claim above.</p>",
        "id": 509564130,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1743549211
    },
    {
        "content": "<p>Yeah, it is unfortunate that pass@n means very different things (at least morally) in formal proofs vs. almost anything else (python code, natural language math, etc.)</p>",
        "id": 509564459,
        "sender_full_name": "Jason Rute",
        "timestamp": 1743549378
    },
    {
        "content": "<p>At the recent marking weekend for intermediate (ages 14-16) olympiads in the UK, one of the markers tried a ChatGPT reasoning model (whatever you can get without paying for a subscription) on problem 1 from <a href=\"https://ukmt.org.uk/wp-content/uploads/2025/03/Cayley_2025_Paper.pdf\">this paper</a> (so the easiest problem on a paper for 14-year-olds). It insisted that only digits 0 and 1 are valid in two-digit positive integers (despite understanding that the numbers were in decimal), and doubled down when told that it was wrong.</p>",
        "id": 509570905,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1743552886
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> While we do not have a paper on AlphaProof, there is one on <a href=\"https://arxiv.org/pdf/2502.03544\">AlphaGeometry2</a>. I am not sure whether the approach of building specialized symbolic engines by hand is particularly effective when general LLMs are becoming increasingly intelligent. However, having LLMs automate building symbolic engines would be neat.</p>",
        "id": 509576995,
        "sender_full_name": "Justin Asher",
        "timestamp": 1743555996
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"266253\">@Joseph Myers</span> While I understand the concern, I cannot imagine newer free reasoning models like Gemini 2.5 Pro having this issue. I remember playing around with ChatGPT a year or two ago, having a lot of problems like this, but now they have mostly subsided. I expect the error rate to go down close to zero, but never completely to zero, which is why we need formal methods.</p>",
        "id": 509577266,
        "sender_full_name": "Justin Asher",
        "timestamp": 1743556152
    },
    {
        "content": "<p>I always ask LLMs to prove that a sequentially compact metric space is compact. They have always failed me, which I find hilarious because this has to be present in the training data.</p>",
        "id": 509595311,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1743566307
    },
    {
        "content": "<p>Ouch, yeah, I'm surprised they are still so bad at that one. Claude 3.7 just told me \"since each point is contained in only finitely many open sets in a metric space\"...</p>",
        "id": 509604304,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1743571785
    },
    {
        "content": "<p>ChatGPT-o1 is similarly bad. It doesn't make an outright false statement, it is just wrong: at some point it says \"In particular, if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>U</mi><msup><mi>i</mi><mo>‚àó</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">U_{i^*}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6183em;\"><span style=\"top:-2.786em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mbin mtight\">‚àó</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> was among the previously chosen sets, eventually you should not find infinitely many <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><msub><mi>n</mi><mi>k</mi></msub></msub></mrow><annotation encoding=\"application/x-tex\">x_{n_k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6864em;vertical-align:-0.2559em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2559em;\"><span></span></span></span></span></span></span></span></span></span> in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>U</mi><msup><mi>i</mi><mo>‚àó</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">U_{i^*}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6183em;\"><span style=\"top:-2.786em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mbin mtight\">‚àó</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>.\" but there is no reason for <code>U_{i^*}</code> to have been used in the construction it gave. I suspect you could salvage a proof that sequentially compact implies countably compact from the argument it gives.</p>",
        "id": 509605223,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1743572330
    },
    {
        "content": "<p>Gemini 2.5 at first appears to produce a correct proof. It appeals to the (true!) fact that a metric space is compact if and only if it is complete and totally bounded. However, when asked to prove that, it then relies on the claim that a sequentially compact metric space is compact, and doesn't notice the circularity.</p>",
        "id": 509606055,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1743572807
    },
    {
        "content": "<p>I guess it won't be surprising to hear that I too believe our best chances for automatic theorem proving include using a proof checker, such as Lean.</p>\n<p>However I don't think we should entirely dismiss the alternatives such as LLM (or LLM + tool use). I am very pleased to see this paper, which reminds us of reality, but I think we should bear in mind the USAMO problems are sufficiently hard that there is not much room for a signal. This is also true for humans: if you take a bunch of mathematicians who haven't trained in Olympiad math, many will perform dismally on USAMO problems, even though they are quite capable mathematicians.</p>\n<p>It's clear that the performance on easier benchmarks like AIME are all ludicrously inflated (partially by unintentional / unavoidable leakage) but these models are clearly able to do some sort of reasoning. The constant hype is annoying but there is also progress being made.</p>",
        "id": 509640779,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1743584727
    },
    {
        "content": "<p>Glad to see more evaluations on proofs (vs numbers) in harder math questions for LLMs. Personally feel like the most representative of true ability would be to create maybe ~1000 proof type questions from all areas of math which are not incredibly standard but fairly natural (which is very difficult for these numerical answer question imo) and hand grade them for each model [as they have done here afaict]. I feel the knowledge of lean/the outdatedness of mathlib knowledge/generally coding ability translated into lean syntax confounds the math evaluations too much in the regard of math ability evaluations of pure formal math.</p>",
        "id": 509653172,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743588343
    },
    {
        "content": "<p>By the way, it seems that Gemini 2.5 pro has gotten 25% score on the usamo benchmark of this thread.</p>",
        "id": 509719573,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743605490
    },
    {
        "content": "<p>It will be quite difficult (and very expensive) to consistently mark the results of several LLMs on 1000 questions. Humans really need to do the marking here.</p>",
        "id": 509727698,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1743607299
    },
    {
        "content": "<p>I guess it'll be expensive but it depends on how important we think this data is. One way could be to form an informal group of a few hundred mathematicians who each come up with a private problem or two on their own and their entire (voluntary) commitment would be to benchmark any new SOTA model on that one question. I think the value of this would be to generate data to measure how helpful newer models for professional mathematicians is across different disciplines (maybe we can find some areas where it is much more helpful than others)</p>",
        "id": 509731577,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743608228
    },
    {
        "content": "<p>Looks to me as if Gemini-2.5 got it correct (did not check very carefully though): <a href=\"https://g.co/gemini/share/15e2ef3107b4\">https://g.co/gemini/share/15e2ef3107b4</a></p>",
        "id": 509741821,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1743610989
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"662620\">Andy Jiang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509731577\">said</a>:</p>\n<blockquote>\n<p>I guess it'll be expensive but it depends on how important we think this data is. One way could be to form an informal group of a few hundred mathematicians who each come up with a private problem or two on their own and their entire (voluntary) commitment would be to benchmark any new SOTA model on that one question. I think the value of this would be to generate data to measure how helpful newer models for professional mathematicians is across different disciplines (maybe we can find some areas where it is much more helpful than others)</p>\n</blockquote>\n<p>Many of us have our own benchmarks, and with mine Gemini-2.5-pro is pretty good (<a class=\"message-link\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/benchmarking.20Gemini.202.2E5.20pro/near/509345860\">#Machine Learning for Theorem Proving &gt; benchmarking Gemini 2.5 pro @ üí¨</a> ) though can be better.</p>\n<p>Honestly I don't think any benchmark will convince all people. Keeping an updated of collections that any good mathematician should be able to solve but the current best LLMs cannot lets one keep checking for progress. As was pointed out,  olympiad problems are not such problems.</p>",
        "id": 509743157,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1743611355
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"266304\">Siddhartha Gadgil</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509741821\">said</a>:</p>\n<blockquote>\n<p>Looks to me as if Gemini-2.5 got it correct (did not check very carefully though): <a href=\"https://g.co/gemini/share/15e2ef3107b4\">https://g.co/gemini/share/15e2ef3107b4</a></p>\n</blockquote>\n<p>That's not bad, but it appealed to the Lebesgue number lemma, which also has a nontrivial proof from sequential compactness. (Perhaps it could prove that though, if asked; the proof is on Wikipedia after all <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span>)</p>",
        "id": 509746385,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1743612189
    },
    {
        "content": "<p>In any case, that's the closest I've seen an LLM come to achieving this.</p>",
        "id": 509746623,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1743612246
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"266304\">Siddhartha Gadgil</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509743157\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"662620\">Andy Jiang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509731577\">said</a>:</p>\n<blockquote>\n<p>I guess it'll be expensive but it depends on how important we think this data is. One way could be to form an informal group of a few hundred mathematicians who each come up with a private problem or two on their own and their entire (voluntary) commitment would be to benchmark any new SOTA model on that one question. I think the value of this would be to generate data to measure how helpful newer models for professional mathematicians is across different disciplines (maybe we can find some areas where it is much more helpful than others)</p>\n</blockquote>\n<p>Many of us have our own benchmarks, and with mine Gemini-2.5-pro is pretty good (<a class=\"message-link\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/benchmarking.20Gemini.202.2E5.20pro/near/509345860\">#Machine Learning for Theorem Proving &gt; benchmarking Gemini 2.5 pro @ üí¨</a> ) though can be better.</p>\n<p>Honestly I don't think any benchmark will convince all people. Keeping an updated of collections that any good mathematician should be able to solve but the current best LLMs cannot lets one keep checking for progress. As was pointed out,  olympiad problems are not such problems.</p>\n</blockquote>\n<p>In my opinion the purpose would not really be to convince people. I don't think it's controversial that the models are improving at math. I think it would be good to provide a more robust measure of the progress. So at least we can agree on what level of problems it can solve on average.</p>",
        "id": 509763549,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743617512
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"197836\">Jireh Loreaux</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509746623\">said</a>:</p>\n<blockquote>\n<p>In any case, that's the closest I've seen and LLM come to achieving this.</p>\n</blockquote>\n<p>I don't think an LLM reciting a proof of sequential compactness =&gt; compactness (which probably occurred in its training data for hundreds/thousands of times) is really indicative of its reasoning ability. If anything it could just be the model pre-training giving a higher weight to Wikipedia data; I would also suspect it can be easily solved with the \"Search\" function turned on for ChatGPT.</p>",
        "id": 509763776,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1743617589
    },
    {
        "content": "<p>As I indicated before, it's certainly present in the training data many times over.<br>\n<span class=\"user-mention silent\" data-user-id=\"197836\">Jireh Loreaux</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509595311\">said</a>:</p>\n<blockquote>\n<p>I always ask LLMs to prove that a sequentially compact metric space is compact. They have always failed me, which I find hilarious because this has to be present in the training data.</p>\n</blockquote>\n<p>But the point is this: if it <em>can't</em> successfully recite a (valid!) proof that it's seen thousands of times, then it's certainly not reasoning successfully because it would just be able to look at that proof and realize it's correct. So, this isn't a <em>sufficient</em> condition for reasoning, but a <em>necessary</em> one.</p>\n<p>And I've never seen an LLM even manage that (although the Gemini 2.5 link above is the closest I've seen).</p>",
        "id": 509774394,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1743621005
    },
    {
        "content": "<p>Sorry, I didn't want to contradict what you said! I agree not being able to prove it certainly indicates a problem. I just think Gemini 2.5 being closer to a proof doesn't necessarily mean it's better at reasoning.</p>",
        "id": 509780932,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1743623227
    },
    {
        "content": "<p>Agreed.</p>",
        "id": 509791429,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1743627067
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509727698\">said</a>:</p>\n<blockquote>\n<p>It will be quite difficult (and very expensive) to consistently mark the results of several LLMs on 1000 questions. Humans really need to do the marking here.</p>\n</blockquote>\n<p>I think this is pretty much the same difficulty as checking the result of a potential future AI's long claimed (informal) solution to some unsolved problem: a large amount of human checking to know whether what the AI has produced is correct or not. (An AI might eventually get a reputation for producing reliable proofs, but a lot of human checking would be needed to get there. And a good reputation for one version of an AI wouldn't necessarily carry over to the next version of that AI.)</p>\n<p>Getting the AIs to produce formal proofs avoids that difficulty. But producing 1000 correct formal statements from all areas of mathematics, to be able to better benchmark AI prover performance without hand grading, is also hard (especially if you validate the statements by producing formal proofs of them all)! It would, however, have the value of adding a lot of material to mathlib (if genuinely covering \"all areas\") that's of value independent of any utility of benchmarks.</p>",
        "id": 509814348,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1743638426
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> </p>\n<blockquote>\n<p>And a good reputation for one version of an AI wouldn't necessarily carry over to the next version of that AI.</p>\n</blockquote>\n<p>Once we have one AI that can do math well, future models will likely be trained with that first AI checking their work, particularly in mathematical areas where we know it understands reliably. Because of this, I have a hard time imagining a scenario where models regress. It seems more likely that the boundary capabilities of what these models are capable of will quickly expand until we reach a point where we can no longer easily check their work. (I personally do not enjoy reading thousand-page proofs, much less hundred thousand-page proofs.) This is where formalization comes in as a necessary step.</p>",
        "id": 509818929,
        "sender_full_name": "Justin Asher",
        "timestamp": 1743641343
    },
    {
        "content": "<p>My experience with Gemini-2.5 and other models (and a colleagues with DeepSeek-R1) is that if we go a couple of levels beyond their capabilities they still have useful output but with errors creeping in. So one needs to check and have back and forth.</p>\n<p>This is why I feel (auto)formalization will <strong>always</strong> be worthwhile. Whatever the level the best model reaches  at any stage, with feedback from formalization it can go some levels higher.</p>",
        "id": 509822993,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1743643790
    },
    {
        "content": "<p>I also like asking questions about circle dynamics. E.g., \"Is it true that a circle homeomorphism is always conjugate to a rotation?\" or \"Is it true that a circle homeomorphism with an irrational rotation number is always conjugate to a rotation?\"</p>",
        "id": 509824668,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1743644661
    },
    {
        "content": "<p>(in fact, we also need it to be at least <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>C</mi><mrow><mn>1</mn><mo>+</mo><mi>V</mi><mi>B</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">C^{1+VB}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span></span></span></span></span></span></span></span></span></span></span></span>)</p>",
        "id": 509824707,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1743644690
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"266253\">Joseph Myers</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509814348\">said</a>:</p>\n<blockquote>\n<p>Getting the AIs to produce formal proofs avoids that difficulty. But producing 1000 correct formal statements from all areas of mathematics, to be able to better benchmark AI prover performance without hand grading, is also hard (especially if you validate the statements by producing formal proofs of them all)! It would, however, have the value of adding a lot of material to mathlib (if genuinely covering \"all areas\") that's of value independent of any utility of benchmarks.</p>\n</blockquote>\n<p>I don't think it's so hard to produce such problems. You don't even need to know the answer. Just produce some statements which seem difficult after a few hours of thought and ask the AI to either prove or disprove the statement. You don't need to provide correct statements bc finding counter examples is also a good thing to test.</p>",
        "id": 509862503,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743665103
    },
    {
        "content": "<p>In contrast to the frontiermath type questions (which tbf is probably by far the best benchmark for research math for AIs right now) which are very difficult to generate</p>",
        "id": 509862911,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743665238
    },
    {
        "content": "<p>I guess the main difficulty would be adding enough to mathlib to state those problems in lean</p>",
        "id": 509863107,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743665297
    },
    {
        "content": "<p>The frontiermath type questions are not a very good benchmark IMO because they have revealed that coming up with an educated guess for a number is far easier than working out a number rigorously. This is why we need to check proofs. The frontiermath approach overinflates the ability of LLMs and the Proof or Bluff paper gives a far more accurate idea of where LLMs actually are when it comes to mathematics, if by mathematics you mean \"what mathematicians actually do\".</p>",
        "id": 509881073,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1743670650
    },
    {
        "content": "<p>Even the proof or bluff paper isn't perfect because it's testing \"Olympiad math\" rather than real math but I think we really need to move away from \"what is this number\" questions which are even less representative.</p>",
        "id": 509881277,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1743670704
    },
    {
        "content": "<p>Many of us make up questions to ask our students, or in selection interviews for research students. I personally just ask these to the LLMs, or ask similar ones my colleagues suggest.</p>\n<p>I had a collection giving the limits over the years, but Gemini-2.5-pro saturated these. I then moved to questions arising out of research questions, and it hit its limit (as I mentioned, often still saying useful things but often failing).</p>\n<p>It would be good if those on the thread not convinced made up similar questions. Essentially one which if a research students (saying into second year or beyond) failed to answer, we would be disappointed in the student. These can be tried on the LLMs.</p>",
        "id": 509882762,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1743671126
    },
    {
        "content": "<p>By the way I think a better benchmark methodology on USAMO would be to report also what happens if you do naive compute scaling--say let Gemini 2.5 pro do ~500 generations and self-report the confidence score and grade the highest confidence answer. It is also worth measuring in case of failure to produce a proof how much it recognizes it cannot prove it vs believing in an incorrect proof. If the model cannot easily fool itself then probably it will benefit from compute scaling more.</p>",
        "id": 509906263,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743678203
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"197836\">Jireh Loreaux</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509595311\">said</a>:</p>\n<blockquote>\n<p>I always ask LLMs to prove that a sequentially compact metric space is compact. They have always failed me, which I find hilarious because this has to be present in the training data.</p>\n</blockquote>\n<p>I tried it with perplexity (with the \"pro\" mode too) and it gave me a transparently circular proof as well. I would have bet money they would all be capable of regurgitating such a classic exercise, so much for AGI by 2027...</p>",
        "id": 509983452,
        "sender_full_name": "Luigi Massacci",
        "timestamp": 1743696874
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"197836\">Jireh Loreaux</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509774394\">said</a>:</p>\n<blockquote>\n<p>But the point is this: if it <em>can't</em> successfully recite a (valid!) proof that it's seen thousands of times, then it's certainly not reasoning successfully because it would just be able to look at that proof and realize it's correct. So, this isn't a <em>sufficient</em> condition for reasoning, but a <em>necessary</em> one.</p>\n</blockquote>\n<p>I definitely don't think that only quizzing well-known proofs are a good benchmark for research ability (though it probably should form part of such a benchmark). But would like to gently disagree with this statement here. I think LLMs responding to a question like this behaves more similarly to a person trying to \"remember\" the proof than a machine which is able to search within its training data for this proof and verify its correctness on the spot. This failure is probably more of a memory/recall failure than a reasoning one. A reasoning test on its ability to see if a proof is correct would be to find similar level problems and give various arguments, some of which are faulty, and see if it can identify the faulty ones.</p>",
        "id": 509999255,
        "sender_full_name": "Andy Jiang",
        "timestamp": 1743701449
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"662620\">Andy Jiang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509999255\">said</a>:</p>\n<blockquote>\n<p>I think LLMs responding to a question like this behaves more similarly to a person trying to \"remember\" the proof than a machine which is able to search within its training data for this proof and verify its correctness on the spot. </p>\n</blockquote>\n<p>More like remembering the <em>text</em> of a proof, which is not the same thing.</p>",
        "id": 510228120,
        "sender_full_name": "Luigi Massacci",
        "timestamp": 1743779710
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"197836\">Jireh Loreaux</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Proof.20or.20Bluff/near/509595311\">said</a>:</p>\n<blockquote>\n<p>I always ask LLMs to prove that a sequentially compact metric space is compact. They have always failed me, which I find hilarious because this has to be present in the training data.</p>\n</blockquote>\n<p>It seems like the new ChatGPT o4-mini got it right without using any additional complicated theorems.</p>\n<p><a href=\"https://chatgpt.com/share/68015d21-f030-800e-8b9c-c6647e21cad6\">https://chatgpt.com/share/68015d21-f030-800e-8b9c-c6647e21cad6</a></p>",
        "id": 512905425,
        "sender_full_name": "Deming Xu",
        "timestamp": 1744920208
    },
    {
        "content": "<p>Indeed, I just had a friend try that for me last night.</p>",
        "id": 512924826,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1744927806
    },
    {
        "content": "<p>On the other hand:<br>\n<a href=\"/user_uploads/3121/rAPXDEPouogy1Bl4hG1CMWl1/Screenshot-from-2025-04-18-12-29-56.png\">Screenshot from 2025-04-18 12-29-56.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/rAPXDEPouogy1Bl4hG1CMWl1/Screenshot-from-2025-04-18-12-29-56.png\" title=\"Screenshot from 2025-04-18 12-29-56.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"852x601\" src=\"/user_uploads/thumbnail/3121/rAPXDEPouogy1Bl4hG1CMWl1/Screenshot-from-2025-04-18-12-29-56.png/840x560.webp\"></a></div>",
        "id": 513009764,
        "sender_full_name": "Luigi Massacci",
        "timestamp": 1744972234
    },
    {
        "content": "<p>Which does not seem very good to me...</p>",
        "id": 513009855,
        "sender_full_name": "Luigi Massacci",
        "timestamp": 1744972290
    }
]