[
    {
        "content": "<p>I've been experimenting a little with extracting tactic invocations in Lean4, and have some moderate success. In particular, I've been able to extract the goal state and rewrite lemma for (almost) every rewrite used in the mathlib4 library. It should be possible to extract other tactics (i.e. besides <code>rw</code>) in a similar fashion.</p>\n<p>Here's a random sample of the data:<br>\n<a href=\"/user_uploads/3121/aJifRV-n8fxz5PJc96iJ59po/Screen-Shot-2023-03-17-at-10.59.06-am-1.png\">Screen-Shot-2023-03-17-at-10.59.06-am-1.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/aJifRV-n8fxz5PJc96iJ59po/Screen-Shot-2023-03-17-at-10.59.06-am-1.png\" title=\"Screen-Shot-2023-03-17-at-10.59.06-am-1.png\"><img src=\"/user_uploads/3121/aJifRV-n8fxz5PJc96iJ59po/Screen-Shot-2023-03-17-at-10.59.06-am-1.png\"></a></div><p>I also have the full goal state (i.e. the hypotheses) available as well. There are approximately 25000 pairs at present (once we restrict to rewriting on the goal, and rewriting by an atomic term rather than some complicated expression), and I'd expect this to roughly double once all of mathlib is ported to mathlib4.</p>\n<p>Would anyone like to help me do something with this data? I'd love to have an effective tool for selecting promising rewrite lemmas, given a goal.</p>\n<p>Left to my own devices, I would first try cosine-similarity nearest neighbours in the tf-idf vectors for the goals (tokenizing on whitespaces and parentheses?). But I don't think this will do a very good job! I'm sure people here have much cleverer things to do with it, and know all the relevant tooling.</p>",
        "id": 342451550,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1679014825
    },
    {
        "content": "<p>You may also want to look at the tooling Daniel Selsam wrote for mining Lean 4 proof data: <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What.20is.20the.20AMI.20project's.20vision.20for.20AI.20in.20Lean.3F/near/290848202\">https://leanprover.zulipchat.com/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/What.20is.20the.20AMI.20project's.20vision.20for.20AI.20in.20Lean.3F/near/290848202</a></p>",
        "id": 342457435,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679018624
    },
    {
        "content": "<p>And of here is a previous conversation about building rewrite search tools using machine learning: <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/rw_hint\">#Machine Learning for Theorem Proving &gt; rw_hint</a></p>",
        "id": 342457561,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679018721
    },
    {
        "content": "<p>Before you go too far, how well does the built-in Lean 3 tactic suggestion tool (based on meta's HTPS paper) work if you force it to suggest <code>rw</code> tactics?  I guess the question is if there is a lot of value in making a specific rewrite suggestion tool over just a general tactic suggestion tool.  I could see it go either way.  Rewriting is a particularly special setting with a lot of advantages (like the fact that equality proofs are reversible and don't branch, so you can rewrite both the LHS or the RHS when doing a proof search).</p>",
        "id": 342458331,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679019288
    },
    {
        "content": "<p>Also, I've always though rewriting was very similar to solving a Rubik's cube (but harder and more general).  I think these two papers are very interesting in that regard:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2108.11204\">Subgoal Search For Complex Reasoning Tasks</a></li>\n<li><a href=\"https://arxiv.org/abs/2206.00702\">Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search\n</a></li>\n</ul>\n<p>The main idea is that instead of predicting the next move, you predict the state you will be at N moves later (for a small value of N like 3 or 4).  Then you search for that.  It does much better.  This is hard to implement for theorem proving in general, but for rewriting it should be reasonably straight forward (maybe).</p>",
        "id": 342458638,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679019522
    },
    {
        "content": "<p>I haven't forgotten the previous conversation about <code>rw_hint</code> and <code>rw_search</code>. :-) I'd like them to one day happen!</p>",
        "id": 342461840,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1679021965
    },
    {
        "content": "<p>I feel like the \"built-in Lean 3 tactic suggestion tool\" is a bit of a dead-end. It's far from actually built-in, requires remote queries (so is slow and limited) and I want tools that can be used by other tactics.</p>",
        "id": 342461955,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1679022034
    },
    {
        "content": "<p>There are a lot of models out there with vastly different speeds to generate features for things like goals and theorem statements.  For example, Tactician uses something like TF-IDF, but based on the syntax tree, so it is really fast and cheap to run.  (IBM's TRAIL is another similar approach.)  Other models use neural networks like TreeLSTMs, graph neural networks, and Transformers (of vastly different sizes).  Some are faster than others.  The larger (and hence slower) models are possibly better for situations where you don't have to call them as frequently, but sometimes a large model which can make really good suggestions is more useful over a fast model which makes poor solutions.  If you want it to be runnable by other tactics, neural models may not be an (easy) way to go (unless you want to interface Lean and a neural network library or an API).</p>",
        "id": 342464649,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679024003
    },
    {
        "content": "<p>The ability to assign every theorem a cached embedding and to similarly assign an embedding to a goal in real time, is immensely useful.  It would facilitate all sorts of other tools like library search, Hammer-like tools (e.g. MagnusHammer), tactic suggestions, proof search, library search, etc that other tactics could use.  I think this would be more valuable than any one application, especially if the interface was flexible enough to support different embedding methods.  Simple ones could be written in pure Lean, and more complex ones could be gotten from external tools or APIs.</p>",
        "id": 342464703,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679024028
    },
    {
        "content": "<p>But I also think the world is moving in the direction of larger models for better or worse.  Such models seem to interface well with widgets and editor plugins, so one might also consider that as well.</p>",
        "id": 342464779,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679024059
    },
    {
        "content": "<p>I thought I heard that Lean 4 has some ability to assign metadata to declarations in the environment, but I might be mistaken.</p>",
        "id": 342465277,
        "sender_full_name": "Jason Rute",
        "timestamp": 1679024410
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> , you may want to check out our recent work we submitted to ITP with <span class=\"user-mention\" data-user-id=\"132858\">@Ramon Fern√°ndez Mir</span> and <span class=\"user-mention\" data-user-id=\"121918\">@Edward Ayers</span> . Machine-Learned Premise Selection for Lean: <a href=\"https://bartoszpiotrowski.pl/p/lean-premise-selection-paper.pdf\">preprint</a>, <a href=\"https://github.com/BartoszPiotrowski/lean-premise-selection\">code</a> with simple <a href=\"https://github.com/BartoszPiotrowski/lean-premise-selection/blob/main/Tests/TacticTest.lean\">demo</a>. (The preprint will appear also on arXiv on Monday). </p>\n<p>The assumption of the project was to build a lightweight, easy-to-use and easy-to-modify premise selection tool implemented directly in Lean 4. So we avoid large neural nets and calls to a server, but rather rely on simple, custom-built ML models trained on combinations of syntactic features. (And pretrained models are available for downloading.)</p>\n<p>The suggestions are provided via <code>suggest_premises</code> tactic; we also provide a widget that tries to combine the suggested lemmas with simple tactics and shows which ones advance the current proof state.</p>\n<p>We focus on premise selection in general, not only <code>rw</code>, and we train on all pairs <code>(theorem statement and its hypotheses, lemmas used in the proof)</code> we could extract from <code>mathlib</code>. But the tool may be adapted specifically to the <code>rw</code> tactic.</p>\n<p>(The tool at the current stage may not be super precise, but it was meant to provide a baseline solution, and it may be improved in many ways.)</p>",
        "id": 342531082,
        "sender_full_name": "Bartosz Piotrowski",
        "timestamp": 1679051686
    },
    {
        "content": "<p>This sounds great, I'll have a look.</p>",
        "id": 342548949,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1679056676
    },
    {
        "content": "<p>What are the prospects for making it usable in practice? (My bar for usable is pretty high: essentially integration into mathlib4.)</p>",
        "id": 342549145,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1679056727
    },
    {
        "content": "<p>(e.g. a tactic built-in to mathlib, which, if it can't detect a pretrained model locally, offers to curl it for you)</p>",
        "id": 342549370,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1679056798
    },
    {
        "content": "<p>I think it may be rather easily made usable in practice in that sense. Extracting predictions form a pretrained model is fast, and the model itself is not super heavy (&lt; 1 GB).</p>",
        "id": 342566719,
        "sender_full_name": "Bartosz Piotrowski",
        "timestamp": 1679060891
    },
    {
        "content": "<p>Go ahead, see if you can run it and play with it. Any suggestions for improvements are welcome!</p>",
        "id": 342566927,
        "sender_full_name": "Bartosz Piotrowski",
        "timestamp": 1679060947
    },
    {
        "content": "<p>Any progress on making this available in mathlib, <span class=\"user-mention\" data-user-id=\"257492\">@Bartosz Piotrowski</span>?</p>",
        "id": 348704254,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681300505
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> how does your rw extraction work? Do you modify term elaboration in Core?</p>",
        "id": 348704841,
        "sender_full_name": "David Renshaw",
        "timestamp": 1681300642
    },
    {
        "content": "<p>I just dump all the infotrees, modified to include slightly more information in their printed format, and parse them afterwards. It was a very low effort experiment. :-)</p>",
        "id": 348705377,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681300767
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/corpus.20of.20all.20rewrites.20in.20mathlib4/near/348704254\">said</a>:</p>\n<blockquote>\n<p>Any progress on making this available in mathlib, <span class=\"user-mention silent\" data-user-id=\"257492\">Bartosz Piotrowski</span>?</p>\n</blockquote>\n<p>Not yet! Lately I was busy with other stuff, but soon I'll focus on this. I'll let you know!</p>",
        "id": 350453804,
        "sender_full_name": "Bartosz Piotrowski",
        "timestamp": 1681723480
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/corpus.20of.20all.20rewrites.20in.20mathlib4/near/348705377\">said</a>:</p>\n<blockquote>\n<p>I just dump all the infotrees, modified to include slightly more information in their printed format, and parse them afterwards. It was a very low effort experiment. :-)</p>\n</blockquote>\n<p>Do you have an example showing how I can get my hands on infotrees for this kind of thing? I'm trying <code>Lean.Elab.getInfoTrees</code>, but it's returning an empty array.</p>\n<p>Oh... maybe you are doing <code>set_option trace.Elab.info true</code> to get a textual dump?</p>",
        "id": 360889205,
        "sender_full_name": "David Renshaw",
        "timestamp": 1684959532
    },
    {
        "content": "<p>I can add this to my lakefile:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">package</span> <span class=\"n\">mypackage</span> <span class=\"n\">where</span>\n  <span class=\"n\">moreLeanArgs</span> <span class=\"o\">:=</span> <span class=\"bp\">#</span><span class=\"o\">[</span><span class=\"s2\">\"-D\"</span><span class=\"o\">,</span> <span class=\"s2\">\"trace.Elab.info=true\"</span><span class=\"o\">]</span>\n</code></pre></div>\n<p>but then I need to parse the textual output. I'm looking for a cleaner way to extract goal information for all locations in a library.</p>",
        "id": 360891704,
        "sender_full_name": "David Renshaw",
        "timestamp": 1684960388
    },
    {
        "content": "<p>I'll send you a link to my very-WIP branch.</p>",
        "id": 360918142,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1684974259
    }
]