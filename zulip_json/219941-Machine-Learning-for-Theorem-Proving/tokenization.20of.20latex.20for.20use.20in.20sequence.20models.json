[
    {
        "content": "<p>I have been experimenting with GPTs trained on latex texts. The tokenization does not respect the semantics of the text. While we know that the paper structure is driven by the logic of the proofs that underlie the theorems. It seems to be reasonable to seek a version of attention mechanism based on the math symbols rather than statistics of bytes ( as it is done now, using tools like SentencePiece). I.e., I am looking for something like combination of attention and Syntax Tree ( in proof theory sense), as applied to math texts as usually found on arxiv. Was there work in this direction, and if so what are the references?</p>",
        "id": 360301921,
        "sender_full_name": "Stan111",
        "timestamp": 1684768338
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"116395\" href=\"/#narrow/stream/116395-maths/topic/tokenization.20of.20latex.20for.20use.20in.20sequence.20models\">#maths &gt; tokenization of latex for use in sequence models</a> by <span class=\"user-mention silent\" data-user-id=\"306601\">Kyle Miller</span>.</p>",
        "id": 360311923,
        "sender_full_name": "Notification Bot",
        "timestamp": 1684770520
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"619258\">@Stan111</span> I moved your message to this stream since it seems like it's a better match (<a class=\"stream\" data-stream-id=\"116395\" href=\"/#narrow/stream/116395-maths\">#maths</a> is generally for discussions about formalizing pure math)</p>",
        "id": 360312636,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1684770699
    },
    {
        "content": "<p>I guess you are thinking of something like this: <a href=\"https://arxiv.org/pdf/1905.10006.pdf\">https://arxiv.org/pdf/1905.10006.pdf</a><br>\nBut as it turns out transformers with dense attention (to all other tokens) generalize this and are very good on syntactical tasks, so there is no need to spend too much effort on this part.<br>\nClearly, tokenizers based on the statistics of mathematical text would be more economic for mathematical texts, but the statistics should be optimized on the pretraining distribution where &gt;99% of the computation is spent.</p>",
        "id": 360319593,
        "sender_full_name": "Fabian Gl√∂ckle",
        "timestamp": 1684772427
    }
]