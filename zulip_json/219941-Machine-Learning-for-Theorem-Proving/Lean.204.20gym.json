[
    {
        "content": "<p>There was <a href=\"https://github.com/openai/lean-gym\">Lean Gym</a> a couple of years ago, but it was Lean 3. Are there any plans for a Lean 4 Gym? Someone at Imperial is interested in this question and asked me, so I thought I'd ask here to see if there are people thinking about gyms for Lean.</p>",
        "id": 420262027,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1707315601
    },
    {
        "content": "<p>I think this is <a href=\"https://github.com/leanprover-community/repl/\">https://github.com/leanprover-community/repl/</a></p>",
        "id": 420262241,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1707315648
    },
    {
        "content": "<p>You can also use LeanDojo to interact with Lean from Python: <a href=\"https://github.com/lean-dojo/LeanDojo\">https://github.com/lean-dojo/LeanDojo</a></p>",
        "id": 420262833,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1707315792
    },
    {
        "content": "<p>I think the short answer is that Lean Dojo or the repl is the way to go if you want if you want to control Lean from the outside, and if you want to control lean from the inside (just interacting with an external tool like Lean copilot does) then Aesop has some good functionality for that as talked about in <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/How.20to.20run.20a.20simple.20ATP.20baseline.20for.20Lean\">#Machine Learning for Theorem Proving &gt; How to run a simple ATP baseline for Lean</a></p>",
        "id": 420332152,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707335987
    },
    {
        "content": "<p>But I think the best would be for your friend to come on here and talk about what they would like to see in a gym.  I feel the field has changed since the Lean 3 gym.  There are a lot more different avenues for interaction with Lean, including seeing the full proof history so far, the other code in the file, previous theorems in the environment, and such.  Also pretty printed text is not the only (or even best) mode for communication about the current state of a proof.  Lean Gym 3 had a very simple interface (that as far as I know was never used by anyone but OpenAI who built it).</p>",
        "id": 420332740,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707336215
    },
    {
        "content": "<p>I'm also curious what interface <a href=\"https://arxiv.org/pdf/2310.04353.pdf\">Copra</a> uses.  Is it custom?  How does it work?  <span class=\"user-mention\" data-user-id=\"657996\">@Amitayush Thakur</span></p>",
        "id": 420333166,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707336365
    },
    {
        "content": "<p>And <a href=\"https://aclanthology.org/2023.acl-long.706.pdf\">DT Solver</a> for that matter?</p>",
        "id": 420333582,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707336514
    },
    {
        "content": "<p>I assume <a href=\"https://arxiv.org/pdf/2310.04353.pdf\">Llemma</a> used Lean Dojo's interface for the Formal-to-Formal experiments, but I don't know for sure.</p>",
        "id": 420333858,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707336616
    },
    {
        "content": "<p>Copra uses custom interface, but the same interface can be used with other languages too. The Lean 3 interface in Copra is implemented using the <code>lean</code> commandline. Unlike LeanDojo it is lightweight in setup and doesn't need docker installation. Although it doesn't support the level of annotations you can generate through LeanDojo. You can checkout <a href=\"https://github.com/trishullab/copra\">https://github.com/trishullab/copra</a>. We are planning to create a nice simple python package of cross language ITP python interface which can be used by anyone.  We recently added support of Isabelle other than Lean 3 and Coq. I would recommend copra if you want to try multiple ITPs and not just Lean 3.</p>",
        "id": 420335629,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707337276
    },
    {
        "content": "<p>Also feel free to open any feature request you would want for Copra</p>",
        "id": 420335849,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707337327
    },
    {
        "content": "<p>So every proof step you run <code>lean</code>?  Or do you use the lean language server?</p>",
        "id": 420338267,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707338213
    },
    {
        "content": "<p>A long time ago (pre pandemic) I made some tools to interact with Lean 3 via the language server, but it was quite slow since the language server had a hard coded delay of 0.1 seconds.  I don't know if it is still that case.  (Of course if you are calling GPT4, then the LSP is no longer the bottle neck.)  Nonetheless, I'm more convinced the language server needs to be part of a working solution.  It is the only way to get tactics to see and interact with the text in the file.</p>",
        "id": 420339172,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707338544
    },
    {
        "content": "<p>Yes. I tried using LSP, ran into some nasty concurrency issues. In terms of parsing get responses from LSP. It also uses line number and column number delimitation which makes it hard to parse subsequent states or detect state changes. It is also not very well documented. The only documentation I found was for Javascript (the VScode plugin for Lean). Also, if I have understood LeanDojo code correctly, then I think it also does the same, and doesn't use LSP.</p>",
        "id": 420339685,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707338758
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"657996\">@Amitayush Thakur</span> But you are using the language server, right?  So you fixed your issues?</p>",
        "id": 420340128,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707338936
    },
    {
        "content": "<p>I see that part of the code is here: <a href=\"https://github.com/trishullab/copra/blob/ba06409aaf3d261e989a01a5589712a55922df62/src/lean_server/lean_cmd_server.py#L30\">https://github.com/trishullab/copra/blob/ba06409aaf3d261e989a01a5589712a55922df62/src/lean_server/lean_cmd_server.py#L30</a></p>",
        "id": 420340189,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707338969
    },
    {
        "content": "<p>For how to handle the concurrency issues, see <a href=\"https://github.com/leanprover-community/lean-client-python\">https://github.com/leanprover-community/lean-client-python</a>.</p>",
        "id": 420340435,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707339079
    },
    {
        "content": "<p>Wait, maybe you are just running Lean, parsing the output, and then running Lean again?  How is this possibly fast enough to do a tree search?</p>",
        "id": 420340755,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707339215
    },
    {
        "content": "<p>It is fast, and probably faster than using server. This is very similar to how LeanDojo does it (only with a custom Docker image). The server code works in some cases but the back and forth chitchat waiting also wastes time and leads to race conditions in some cases. So to avoid all that we just used lean cmdline</p>",
        "id": 420341196,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707339413
    },
    {
        "content": "<p>Do you know how fast?  Do you have numbers on the latency?  Either <span class=\"user-mention\" data-user-id=\"657996\">@Amitayush Thakur</span> or <span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span>?</p>",
        "id": 420341433,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707339508
    },
    {
        "content": "<p>Some of the numbers you can get from Copra paper. The per query time includes these time to run. I think it was around &lt; 1s</p>",
        "id": 420341942,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707339700
    },
    {
        "content": "<p>Ok, so I guess again this is because you are calling GPT-4, so that is what is taking so much time.  But for other approaches, like my recent Graph2Tac paper, 1s per call would be horrible.  That means you can only make 60 model calls in 10 minutes.  You would need a really good model for that, and it seems no longer a good idea to call the model on every step.</p>",
        "id": 420342220,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707339815
    },
    {
        "content": "<p>If you look at the times in the <a href=\"https://arxiv.org/abs/2401.02949\">Graph2Tac paper</a>, our graph model is running about 30 tactics per second.  The k-NN is running about 300 tactics per second.</p>",
        "id": 420343026,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707340140
    },
    {
        "content": "<p>The ReProver model is faster so as per our experiments it took 0.52s per query (including Lean execution). I still think a big chunk of 0.52s went in querying the LLM. So just running Lean from commandline is also not very bad.</p>",
        "id": 420343085,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707340169
    },
    {
        "content": "<p>The infra for Coq is very good. It is much faster and preserves history and everything. For our Coq code implementation we use the same one as Proverbot</p>",
        "id": 420343282,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1707340235
    },
    {
        "content": "<p>Yes, our transformer baselines with GPU do about 5 tactics per second, and .5 with CPU.  So there the transformer is taking so long that the interface speed doesn't matter.</p>",
        "id": 420343436,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707340318
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690247\">Cris SALVI</span> has marked this topic as resolved.</p>",
        "id": 420433127,
        "sender_full_name": "Notification Bot",
        "timestamp": 1707390136
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690247\">Cris SALVI</span> has marked this topic as unresolved.</p>",
        "id": 420433164,
        "sender_full_name": "Notification Bot",
        "timestamp": 1707390147
    },
    {
        "content": "<p>Is there existing work trying to fine-tune pre-trained LLMs by reinforcement learning (RL) with feedback from Lean 4 instead of RL with human feedback?</p>",
        "id": 420435251,
        "sender_full_name": "Cris Salvi",
        "timestamp": 1707390914
    },
    {
        "content": "<p>Note that there are WIP <a href=\"https://github.com/leanprover-community/repl/pull/5\">python bindings for the lean repl</a>. </p>\n<p>An alternative to a gym that works better in many contexts is to write a Lean tactic that queries your ML model via HTTP or the Lean FFI. Then you can benchmark your tactic using something like the 'tactic_benchmark' utility in <a href=\"https://github.com/semorrison/lean-training-data\">semorrison/lean-training-data</a>.</p>",
        "id": 420496689,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1707410192
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"690247\">@Cris Salvi</span> If you replace Lean 4 with Lean 3, then yes there are at least two works.  A <a href=\"https://arxiv.org/pdf/2202.01344v1.pdf\">curriculum version of Lean GPT-f using expert iteration</a> and <a href=\"https://arxiv.org/abs/2205.11491\">HTPS using MCTS</a>.   But both LLMs are still tactic prediction models, not general purpose LLMs.  For Isabelle, I think <a href=\"https://arxiv.org/abs/2303.04910\">Baldur</a> learns from Isabelle feedback, but I'm not certain on this.</p>",
        "id": 420498172,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707410625
    },
    {
        "content": "<p>And even older is the <a href=\"https://arxiv.org/abs/1904.03241\">HOList family</a> of projects in HOL-Light, some of which use RL (and get really impressive results).  (But this is a graph neural network model, not a Transformer model.)</p>",
        "id": 420502683,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707411825
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690247\">Cris Salvi</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.204.20gym/near/420435251\">said</a>:</p>\n<blockquote>\n<p>Is there existing work trying to fine-tune pre-trained LLMs by reinforcement learning (RL) with feedback from Lean 4 instead of RL with human feedback?</p>\n</blockquote>\n<p>I'd like to collaborate if you're planning to try something there; I'd plan to go off of LeanCopilot's RAG with a PG approach, possibly using some of the spare tokens to train a critic into the tactic generator LLM like FB's RL theorem prover work did <a href=\"https://arxiv.org/abs/2205.11491\">https://arxiv.org/abs/2205.11491</a>, for better guiding aesop than just using cumulative LLM decoder logit's along the search tree of aesop. That one would be a rather simple RL aspect.</p>",
        "id": 420575274,
        "sender_full_name": "namibj",
        "timestamp": 1707444060
    },
    {
        "content": "<p>The paper link to FBâ€™s work is two messages above (HTPS).</p>",
        "id": 420576718,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707445185
    },
    {
        "content": "<p>I might be late to the party, but I am also in if you plan to set up a gymnasium environment for Lean 4. I believe RL can help a lot in ML for Theorem Proving. Please let me know if this goes forward!</p>",
        "id": 423188126,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1708793836
    },
    {
        "content": "<p>I believe it was already pointed above that <a href=\"https://github.com/leanprover-community/repl/\">Lean 4 REPL</a> is a Lean Gym and more.</p>",
        "id": 423230008,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1708835014
    },
    {
        "content": "<p>If anyone is still looking for a gym environment for theorem proving, our environment <code>itp-interface</code> (<a href=\"https://github.com/trishullab/itp-interface\">https://github.com/trishullab/itp-interface</a>) might come in handy. It is implemented in Python and can support both Lean 4 and Coq.</p>\n<p>So what's different?</p>\n<ul>\n<li>Supports both Lean 4 and Coq. Yes, you just have to write code only once (for proof automation) and it will work for both the ITPs!</li>\n<li>Supports parallel tactic execution, which can help a lot while automatically searching for proofs through LLMs. It can maintain a proof environment pool which can help in running multiple tactics seamlessly for the same state.</li>\n<li>Manages memory more efficiently. If you use Lean 4 REPL, then after a couple of backtracks the JRPC bloats and can take a large amount of memory. We avoid such pitfalls.</li>\n<li>Lean 4 REPL proof mode has some issues regarding accepting incorrect proofs, we avoid that. (See <a href=\"https://github.com/leanprover-community/repl/issues/44\">https://github.com/leanprover-community/repl/issues/44</a>)</li>\n<li>One can in parallel collect proof step data across multiple repositories.</li>\n<li>Support for multiple versions of Coq and Lean 4</li>\n<li>The library is designed to scale and run on <code>ray cluster</code> while doing a highly parallel proof search.</li>\n</ul>\n<p>Setup is super easy:</p>\n<ol>\n<li>pip install itp-interface</li>\n<li>install-itp-interface</li>\n<li>We are done setting up for Lean 4. No more installation steps (including the need to install Lean 4) and other hassles. <br>\n(See our python package: <a href=\"https://pypi.org/project/itp-interface/\">https://pypi.org/project/itp-interface/</a> for more details)</li>\n</ol>\n<p>We build our work on top of the libraries Lean 4 REPL(<a href=\"https://github.com/leanprover-community/repl\">https://github.com/leanprover-community/repl</a>) and Coq Serapy (<a href=\"https://github.com/HazardousPeach/coq_serapy\">https://github.com/HazardousPeach/coq_serapy</a>). </p>\n<p>I would like to thank my co-contributer: <span class=\"user-mention\" data-user-id=\"644040\">@George Tsoukalas</span> , and the community at Zulip for giving us valuable feedback while building this tool. </p>\n<p>This tool can be further supplemented by our proof search tool (<a href=\"https://pypi.org/project/proof-wala/\">https://pypi.org/project/proof-wala/</a>) which can run multilingual proof searches for Coq and Lean. The two can be used for end-to-end proof generation given a theorem in Lean 4 or Coq. The <code>proof-wala</code> tool also supports visualization of tree searched and annotation of proof trees which can be further used for some form of RL style training.</p>\n<p>Looking forward to any feedback you might have.</p>",
        "id": 498675615,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1739166741
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"657996\">@Amitayush Thakur</span> , I recommend creating a new thread, and editing your two <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/498675876\">messages</a> to just link to that thread. Otherwise you'll get replies in two places.</p>",
        "id": 498831381,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1739211692
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"310045\">@Eric Wieser</span> Thank you. We have now announced on a topic here: <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/itp-interface.20and.20proof-wala\">#Machine Learning for Theorem Proving &gt; itp-interface and proof-wala</a></p>",
        "id": 498892056,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1739232274
    }
]