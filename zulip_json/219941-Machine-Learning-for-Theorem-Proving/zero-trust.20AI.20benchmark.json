[
    {
        "content": "<p><a href=\"http://\">Uploading ks38P2g6.jpg…</a><br>\n<a href=\"/user_uploads/3121/3zEwLwVKGGn6CmVglJU-W_R_/Screenshot-2026-02-18-at-9.53.36AM.png\">Screenshot 2026-02-18 at 9.53.36 AM.png</a><br>\nA few months ago at a YC hackathon I built a zero-trust benchmark to evaluate AI on the 297 unsolved formalized conjectures from DeepMind's formal-conjectures repo. Each proof gets compiled via <code>lake lean</code> with <code>warningAsError=true</code>, 19 banned tokens (sorry, admit, native_decide, etc.), and #print axioms checking. Only verified proof so far is Green's Problem 24 by Gemini 3 Flash in 178 seconds <a href=\"https://asiprize.com/verified/greens-problem-24/\">https://asiprize.com/verified/greens-problem-24/</a>. Unfortunately I only had the money to evaluate Flash on all 297 problems but I think a few more could be solved.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/3zEwLwVKGGn6CmVglJU-W_R_/Screenshot-2026-02-18-at-9.53.36AM.png\" title=\"Screenshot 2026-02-18 at 9.53.36 AM.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"1772x1072\" src=\"/user_uploads/thumbnail/3121/3zEwLwVKGGn6CmVglJU-W_R_/Screenshot-2026-02-18-at-9.53.36AM.png/840x560.webp\"></a></div>",
        "id": 574575809,
        "sender_full_name": "Austin Hatfield",
        "timestamp": 1771438771
    },
    {
        "content": "<p><a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/zero-trust.20AI.20benchmark/near/574575809\">A message</a> was moved here from <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/Lean.20in.20the.20wild/with/573216124\">#general &gt; Lean in the wild</a> by <span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span>.</p>",
        "id": 574580420,
        "sender_full_name": "Notification Bot",
        "timestamp": 1771440309
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"983524\">@Austin Hatfield</span> I moved your message here, because a new thread is a better place, so that discussion can happen.</p>",
        "id": 574580506,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1771440341
    },
    {
        "content": "<p>Do you know about Comparator? It is the recommended way to verify AI-generated proofs.</p>",
        "id": 574580638,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1771440389
    },
    {
        "content": "<p>Hey <span class=\"user-mention\" data-user-id=\"983524\">@Austin Hatfield</span>! Cool website! One small thing worth noting: the problem solved here isn't Green's  Problem, but rather a trivial upper bound that already existed in the literature (the docstring above mentions a reference). Unfortunately it seems the <code>@[category research open]</code> tag was wrong...</p>",
        "id": 574593794,
        "sender_full_name": "Paul Lezeau",
        "timestamp": 1771445027
    },
    {
        "content": "<p>Indeed, \"zero trust\" sounds a lot more waterproof than \"disallowing some tokens like <code>sorry</code> and flagging warnings as errors\". As Johan Commelin (implicitly) pointed out, there are ways to write Lean4 code that appears to prove a theorem but actually doesn't, which your checking process would not catch.</p>\n<p>I'm not aware of any LLMs making use of such hacks thus far in order to cheat on benchmarks, but in due time they might (I would imagine especially if RL is used with direct feedback from the proof assistant). </p>\n<p>I didn't know about <a href=\"https://github.com/leanprover/comparator\">Comparator</a>. I guess it's supposed to be stricter than <a href=\"https://github.com/leanprover/lean4checker\">lean4checker</a>? A comparison would be interesting... Generally, my impression is that robustness against adversarial inputs isn't a priority in Lean or Mathlib development. One would need to invest quite a bit of effort before one could, e.g., award bounties for proven theorems just based on running submitted code without manual human review...</p>",
        "id": 574696203,
        "sender_full_name": "Adomas Baliuka",
        "timestamp": 1771497750
    },
    {
        "content": "<p>If you search zulip, you will find previous discussions comparing comparator and lean4checker --- i.e., answering some of the questions you're asking. (In particular, I would disagree with the following, as there is actual work happening in this direction. But feel free to check yourself!)</p>\n<blockquote>\n<p>my impression is that robustness against adversarial inputs isn't a priority in Lean or Mathlib development.</p>\n</blockquote>",
        "id": 574697033,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1771497982
    },
    {
        "content": "<p>Hello <span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span>! Thank you so much I've never seen Comparator before. my 6 deep researches couldn't find it so this is the best help ever! Will integrate that for v2 and see what improvements I can make. My verification pipeline was based on the advice in the <a href=\"#narrow/channel/113489-new-members/topic/Vibe.20coding.20safety\">\"Vibe coding safety\"</a> thread and heavily inspired by AlphaProof. As I am learning though that's appropriate for non-adversarial AI output but doesn't cover redefinition attacks or statement integrity, which Comparator handles via declaration comparison and kernel replay? I'm gonna deep dive into that repo but are there any other resources you might refer me to? Sorry to slam you I'm just new to lean so want to make sure everything is rigorous.</p>",
        "id": 574771453,
        "sender_full_name": "Austin Hatfield",
        "timestamp": 1771518643
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"376152\">@Paul Lezeau</span>  Lol I was wondering why Gemini 3 Flash cracked a Green's problem. It's really good at lean for it's size but not THAT good. Mystery solved I'll update the site and pull that claim and remove problem from benchmark for v2.</p>",
        "id": 574772570,
        "sender_full_name": "Austin Hatfield",
        "timestamp": 1771518954
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"638715\">@Adomas Baliuka</span>  I agree v1 doesn't earn the zero-trust label (didn't know about Comparator and some of the other things you guys flagged). That's the goal though for v2, which I'm trying to get done this week. I will integrate Comparator (sandboxed builds, declaration comparison, kernel replay) so the verification actually backs up the claim. I also realized I was only tracking end-of-compile output rather than mid-compile goal states, so the iteration feedback loop should be a lot stronger next time around too (just finished this). Any other resources or attack vectors you know of I should be aware of?</p>",
        "id": 574775324,
        "sender_full_name": "Austin Hatfield",
        "timestamp": 1771519705
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"638715\">Adomas Baliuka</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/zero-trust.20AI.20benchmark/near/574696203\">said</a>:</p>\n<blockquote>\n<p>I didn't know about <a href=\"https://github.com/leanprover/comparator\">Comparator</a>. I guess it's supposed to be stricter than <a href=\"https://github.com/leanprover/lean4checker\">lean4checker</a>? A comparison would be interesting... Generally, my impression is that robustness against adversarial inputs isn't a priority in Lean or Mathlib development.</p>\n</blockquote>\n<p>Comparator is a lot more airtight than lean4checker, and it actually is designed with adversarial input in mind (which is why it runs Lean files in a sandbox).</p>",
        "id": 574788220,
        "sender_full_name": "James E Hanson",
        "timestamp": 1771523033
    },
    {
        "content": "<p>Lean4checker is really only good for catching basic environment hacking.</p>",
        "id": 574789248,
        "sender_full_name": "James E Hanson",
        "timestamp": 1771523367
    }
]