[
    {
        "content": "<p><a href=\"https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf\">https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf</a></p>\n<p><a href=\"/user_uploads/3121/HCxQ_OA-yjBLXnp8nifH4Q1n/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/HCxQ_OA-yjBLXnp8nifH4Q1n/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"852x373\" src=\"/user_uploads/thumbnail/3121/HCxQ_OA-yjBLXnp8nifH4Q1n/image.png/840x560.webp\"></a></div>",
        "id": 560642903,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764263267
    },
    {
        "content": "<p><a href=\"https://github.com/deepseek-ai/DeepSeek-Math-V2/tree/main\">https://github.com/deepseek-ai/DeepSeek-Math-V2/tree/main</a></p>\n<p><a href=\"/user_uploads/3121/cdCn7Dntq3d6dPdvn6aljH6H/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/cdCn7Dntq3d6dPdvn6aljH6H/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"856x425\" src=\"/user_uploads/thumbnail/3121/cdCn7Dntq3d6dPdvn6aljH6H/image.png/840x560.webp\"></a></div>",
        "id": 560643403,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764263467
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 560645761,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764264470
    },
    {
        "content": "<p><a href=\"https://arxiv.org/abs/2402.03300\">https://arxiv.org/abs/2402.03300</a> is the v1. It introduced the GRPO algorithm r1 was based on</p>",
        "id": 560646216,
        "sender_full_name": "Ivan Eric",
        "timestamp": 1764264655
    },
    {
        "content": "<p>Yeah, I just saw that now.  I see Zhihong Shao is the only one from the previous papers?  Not a lot in common <a href=\"https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/blob/main/config.json\">https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/blob/main/config.json</a> versus <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-Math-V2/blob/main/config.json\">https://huggingface.co/deepseek-ai/DeepSeek-Math-V2/blob/main/config.json</a></p>",
        "id": 560647077,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764264964
    },
    {
        "content": "<p>TBH, I think this is more of a continuation of DeepSeek-Prover-V2 (which they actually cite in the paper).   v1 was based on answer, while prover was more about RL on proofs.https://arxiv.org/abs/2504.21801</p>\n<p>Interesting</p>\n<blockquote>\n<p>We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hardto-verify proofs, creating training data to further improve the verifier.</p>\n</blockquote>\n<p>So much output, hard to keep track sometimes.</p>",
        "id": 560649521,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764265913
    },
    {
        "content": "<p>Lean is turning into a bit of a bottleneck.   More SAT stuff might be a good idea (maybe) <a href=\"https://www.quantamagazine.org/to-have-machines-make-math-proofs-turn-them-into-a-puzzle-20251110/\">https://www.quantamagazine.org/to-have-machines-make-math-proofs-turn-them-into-a-puzzle-20251110/</a></p>",
        "id": 560651048,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764266563
    },
    {
        "content": "<blockquote>\n<p>Lean is turning into a bit of a bottleneck.</p>\n</blockquote>\n<p>I heard an interesting point about this from an author of AlphaGeometry. They said that writing Lean proofs require one to consider many things that are important for logical completeness but irrelevant for the actual proof insights/progress, which is not how humans approach proofs.</p>\n<p>So if you're trying to train an LLM to think more like mathematicans, theoretically you're likely to have much better results if you simulate the human thought process by only considering the important parts of the solution. Basically outputting Lean proofs distracts the LLM from actually finding the solution.</p>\n<p>To remedy this, you either:</p>\n<ol>\n<li>Don't use Lean at all and instead use a slightly less robust logical framework that discards the trivial/distracting parts of the proof, which is what AlphaGeometry does. Formally encoding orientations is very cumbersome so AlphaGeometry relies on diagram checks, which is <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/AlphaGeometry.20doesn't.20have.20a.20secure.20foundation/with/539916171\">not completely sound</a> but gets the job done.</li>\n<li>Maybe split up the tasks to have a dedicated model writing the Lean proofs so the model that's doing the hard work of finding the solution is not aware of the existence of Lean and is only focused on solving the problem.</li>\n</ol>",
        "id": 560658917,
        "sender_full_name": "Gavin Zhao",
        "timestamp": 1764270449
    },
    {
        "content": "<p>This seems to be the key idea:</p>\n<blockquote>\n<p>While using informal reasoning to guide formal proof generation has been explored extensively (Jiang et al., 2023), recent reasoning models have dramatically improved informal reasoning quality, making this guidance far more effective</p>\n</blockquote>\n<p>I think what we're seeing right now the gains are coming from scaling and training on the work of mathematicians, though the pre-training set did have some synthetic data (though that model that generated them had been mostly training on the works of mathematicians).</p>\n<p>LLM is probably not the best pure RLVR approach for math, but clearly good for extracting knowledge from humans.</p>",
        "id": 560660156,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764271053
    },
    {
        "content": "<p>I suspect Ilya might be on to something and the scaling of LLMs is coming to an end.  A new continual learning approach (or at least a significant change in architecture) is going to be required to get to the next set of leaps.</p>",
        "id": 560660679,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764271367
    },
    {
        "content": "<p>Isn't it a bit weird no one is hosting the model online yet (including DeepSeek themselves)? The architecture isn't different from their flagship models, right? (I asked <a href=\"https://www.perplexity.ai/search/which-websites-host-the-deepse-d3IXrDkzTyS4E5rnxAaJTg#0\">Perplexity</a> to do the research for me.)</p>\n<p><a href=\"https://x.com/askOkara/status/1994057253843030347\">As of yesterday</a>, \"the model isn't deployed by any inference providers. we'll integrate it as soon as it becomes available.\"</p>",
        "id": 560831186,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1764350474
    },
    {
        "content": "<p>This model is unlikely to get good support as it is expensive to host and very low traffic.  </p>\n<p>But if you do want it, here you go:</p>\n<p><a href=\"https://endpoints.huggingface.co/new?repository=deepseek-ai/DeepSeek-Math-V2\">https://endpoints.huggingface.co/new?repository=deepseek-ai/DeepSeek-Math-V2</a></p>\n<p>I've been able to get this to work for other models.  No promises for this one.</p>",
        "id": 560850923,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1764361402
    },
    {
        "content": "<p>Maybe people will make a distilled model for AIMO ...</p>",
        "id": 560859155,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1764367641
    },
    {
        "content": "<p><a href=\"https://x.com/nrehiew_/status/1994038917277077853\">https://x.com/nrehiew_/status/1994038917277077853</a></p>",
        "id": 560875125,
        "sender_full_name": "Anh Nguyễn",
        "timestamp": 1764383542
    },
    {
        "content": "<p>A concise description</p>",
        "id": 560875143,
        "sender_full_name": "Anh Nguyễn",
        "timestamp": 1764383552
    }
]