[
    {
        "content": "<p><a href=\"https://epochai.org/frontiermath\">https://epochai.org/frontiermath</a></p>\n<p>\"We collaborated with 60+ leading mathematicians to create hundreds of original, exceptionally challenging math problems, of which current AI systems solve less than 2%.\"</p>",
        "id": 481491156,
        "sender_full_name": "Ivan Eric",
        "timestamp": 1731183332
    },
    {
        "content": "<p>Looking at the example problems, I can't do any of them. Even 2% is very impressive here, if those problems are representative. It seems less a benchmark and more a challenge: an AI that can get a very high score on this is like an AI that can beat top humans at Go.</p>",
        "id": 481491269,
        "sender_full_name": "Ivan Eric",
        "timestamp": 1731183404
    },
    {
        "content": "<p>Awesome!  Just to clarify, these are natural language problems right (as opposed to say formal Lean problems)?  Also, how confident are you that there are no mistakes in the answers?  (I still need to read the paper of course.)</p>",
        "id": 481492127,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731184131
    },
    {
        "content": "<p>Direct link to ArXiv: <a href=\"https://arxiv.org/abs/2411.04872\">FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI</a><br>\nThe authors have had some interesting reviewers/contributors: \"We thank Terence Tao, Timothy Gowers, and Richard Borcherds for their insightful interviews and thank Terence Tao for contributing several problems to this benchmark.\"  (Does <span class=\"user-mention\" data-user-id=\"657719\">@Terence Tao</span>  ever sleep?  :-) )</p>\n<p>The sample problems in the paper (appendix, 5 given) are quite challenging! Formalizing the problems and solutions  might be an interesting challenge in itself!</p>",
        "id": 481492740,
        "sender_full_name": "Oisin McGuinness",
        "timestamp": 1731184659
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"488831\">@Ivan Eric</span> Silly questions that I can't seem to figure out from a quick reading of the report:</p>\n<ul>\n<li>How many problems are there?</li>\n<li>Are they public or private?  If public, what do you plan to do to avoid them becoming part of LLM training data?  If private, how can others evaluate on this benchmark?</li>\n</ul>",
        "id": 481494231,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731185994
    },
    {
        "content": "<p>Also, I'd love for you to do an evaluation of the top AIMO Progress Prize 1 winning solutions (and the solutions for progress prize 2, unless the problems are publically released and there is a serious worry that the Kaggle contestants trained on these problems).  I think the problem format should be roughly similar enough to directly compare the models.</p>",
        "id": 481494234,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731186000
    },
    {
        "content": "<p>Finally, what is EpochAI and what is their motivation for creating this benchmark?  Also, what steps have they put in place to maintain this benchmark in the future?  (I still think about MiniF2F and how the creators left OpenAI and the benchmark became splintered and poorly maintained since then.)</p>",
        "id": 481494279,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731186004
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> </p>\n<p>Responses from some of the authors to these questions: <br>\n\"No, we have not formalized the problems. We have thought about it. Some problems are difficult to formalize because e.g. Lean does not support the relevant math.\" <br>\n<a href=\"https://x.com/tamaybes/status/1855102789858517163\">https://x.com/tamaybes/status/1855102789858517163</a></p>\n<p>\"Note that the dataset is not publicly shared except for some representative examples! We take data contamination very seriously.\"</p>\n<p><a href=\"https://x.com/Jsevillamol/status/1855010482882482655\">https://x.com/Jsevillamol/status/1855010482882482655</a></p>",
        "id": 481494531,
        "sender_full_name": "Ivan Eric",
        "timestamp": 1731186240
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/481492127\">said</a>:</p>\n<blockquote>\n<p>Also, how confident are you that there are no mistakes in the answers?</p>\n</blockquote>\n<p>Section 2.3 (page 5-6) of the paper discusses their verification and review process for the problems.</p>",
        "id": 481494626,
        "sender_full_name": "Oisin McGuinness",
        "timestamp": 1731186333
    },
    {
        "content": "<p>Oh, sorry <span class=\"user-mention\" data-user-id=\"488831\">@Ivan Eric</span>, I thought you were one of the authors.  I guess I'll have to reach out on X. <span aria-label=\"scared\" class=\"emoji emoji-1f628\" role=\"img\" title=\"scared\">:scared:</span>  Thanks for the pointers <span class=\"user-mention\" data-user-id=\"488831\">@Ivan Eric</span> and <span class=\"user-mention\" data-user-id=\"522558\">@Oisin McGuinness</span>!</p>",
        "id": 481494683,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731186369
    },
    {
        "content": "<p>I skimmed the article earlier today.  From our community's point of view, the key quote about format is probably:</p>\n<blockquote>\n<p>we often structured problems to have integer solutions .... We also included solutions that could be represented as arbitrary SymPy objects — including symbolic expressions, matrices, and more.</p>\n</blockquote>\n<p>So (as Jason already noted) it's a similar format to the current AIMO challenges on Kaggle: cleverly-designed problems which are hard but still admit integer answers (or answers from some other countable type).</p>\n<p>The benchmark's authors correctly point out in their article that this format restriction prevents them from covering certain areas of mathematics (analysis, geometry, etc):</p>\n<blockquote>\n<p>Number theory and combinatorics are most prominently represented, collectively accounting for approximately 34% of all MSC2020 tags. This prominence reflects ... these domains’ natural amenability to problems with numerical solutions ....</p>\n</blockquote>",
        "id": 481495316,
        "sender_full_name": "Heather Macbeth",
        "timestamp": 1731186966
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"488831\">Ivan Eric</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/481494531\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> </p>\n<p>Responses from some of the authors to these questions: <br>\n\"No, we have not formalized the problems. We have thought about it. Some problems are difficult to formalize because e.g. Lean does not support the relevant math.\" <br>\n<a href=\"https://x.com/tamaybes/status/1855102789858517163\">https://x.com/tamaybes/status/1855102789858517163</a></p>\n</blockquote>\n<p>I've previously argued that the idea of the IMO Grand Challenge should be considered to include a mathlib side of things (teach mathlib the entire (unwritten) IMO syllabus so that all IMO problems can be stated and proved in a reasonably idiomatic way in Lean) as well as an AI side of things (develop an AI that can generate those solutions). Empirically, however, the work on those two things has been largely disjoint (and the AI side has developed faster than mathlib coverage; more people are interested in building AIs that work on such problems than in increasing mathlib coverage of relevant mathematics).</p>\n<p>You could consider the same for such a benchmark as this: a challenge to expand mathlib to cover the mathematics needed to (a) state and (b) solve the problems. But there are also obvious major difficulties that don't apply so much to expanding coverage for IMO problems: to add the mathematics for a problem in this benchmark you'd need someone who has access to the problem (in confidence), <em>and</em> has (or can gain) specialized expertise in the relevant mathematics, <em>and</em> has Lean expertise, so they can work out the best way to formalize the relevant definitions and theory (and this is all assuming that the choice of what they contribute to mathlib doesn't leak too much about the contents of the problems).</p>",
        "id": 481499861,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1731191003
    },
    {
        "content": "<p>If this is what I think it is, the definitions should be relatively common, or they are given in the problem. If so, then mathlib’s formalization could still proceed as normal, formalizing standard math concepts.</p>",
        "id": 481527172,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731214895
    },
    {
        "content": "<p>Also, there is an easy solution to the problem of AI for Lean verse building Mathlib: AI for building Mathlib, i.e. auto-formalization at scale! <span aria-label=\"joking\" class=\"emoji emoji-1f61c\" role=\"img\" title=\"joking\">:joking:</span> (I jest a bit, but I’m also somewhat serious.  I don’t exactly know what is holding us back from much stronger auto-formalization on a large and systematic scale.)</p>",
        "id": 481527434,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731215086
    },
    {
        "content": "<p>Very interesting work! In my opinion the reported results represent a relatively \"cheap\" inference setup: 0-shot, with only 1 attempt per problem. I think this might not accurately represent what is achievable with current SOTA methods. They even state in the paper that \"When re-evaluating problems that were solved at least once by any model, o1-preview demonstrated the strongest performance across repeated trials (see Section B.2).\", so self-consistency would likely improve the results.</p>",
        "id": 481575660,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1731255748
    },
    {
        "content": "<p>However, my concern is more about the token limit. The token limit is set to 10'000 tokens. But o1 models are known to generate CoT longer than that, especially on hard problems. So I guess part of the bad performance of o1 models is caused by this limit. <a href=\"https://platform.openai.com/docs/guides/reasoning/controlling-costs#allocating-space-for-reasoning\">Official OpenAI recommendation</a>: \"OpenAI recommends reserving at least 25,000 tokens for reasoning and outputs when you start experimenting with these models.\"<br>\nAdditionally, page 9 of the <a href=\"https://arxiv.org/abs/2411.04872\">FrontierMath</a> paper, it is stated that Claude, Grok and GPT-4o reaches 10'000 tokens in more than 45% of their answers. Interestingly, they don't report this statistic for o1 models. Since even \"non-thinking\" models already use more than 10'000 tokens, increasing this token limit would likely improve performance for all models, and I expect even more for o1 models.</p>",
        "id": 481575668,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1731255750
    },
    {
        "content": "<p>Good catch.  (Note I don’t think the authors are aware of this discussion, so it might be interesting to point this out on Twitter.)</p>",
        "id": 481578120,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731257359
    },
    {
        "content": "<p>I think if they are expecting LLMs with strict token limits to solve research level math problems, they are a bit crazy.  Yes, this field will improve, but token limits is an arbitrary and harmful limit.  And if they are setting token limits, are they also ignoring tool use (Python, Lean, etc.)?  That is what helped in the AIMO competition.</p>",
        "id": 481578124,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731257366
    },
    {
        "content": "<p>Overall, I think evaluation on stuff like this is really hard.  The benchmark authors don’t have the money to test SoTA models, but they also don’t want to release the problems.  The same goes for AIMO problems.  And with AlphaProof, DeepMind used a lot of compute to solve the IMO problems.  Can that be improved over time?  Sure, but it might be that test time compute is especially important for solving problems like this and it is harmful to set such tight bounds.</p>",
        "id": 481578128,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731257369
    },
    {
        "content": "<p>I agree this is arbitrary and harmful, although I understand it might be expensive to let models generate more tokens. In FrontierMath, they let models interact with a Python interpreter for their experiments <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 481581640,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1731260130
    },
    {
        "content": "<p>I think it would be nice to write Lean formalisations of the FrontierMath sample problems (just the statements for now).</p>",
        "id": 481607584,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1731281822
    },
    {
        "content": "<p>Here's an easy one:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span><span class=\"bp\">.</span><span class=\"n\">FieldTheory</span><span class=\"bp\">.</span><span class=\"n\">Finite</span><span class=\"bp\">.</span><span class=\"n\">GaloisField</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span><span class=\"bp\">.</span><span class=\"n\">LinearAlgebra</span><span class=\"bp\">.</span><span class=\"n\">Projectivization</span><span class=\"bp\">.</span><span class=\"n\">Basic</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span><span class=\"bp\">.</span><span class=\"n\">Tactic</span><span class=\"bp\">.</span><span class=\"n\">NormNum</span><span class=\"bp\">.</span><span class=\"n\">Prime</span>\n\n<span class=\"kn\">open</span><span class=\"w\"> </span><span class=\"n\">FiniteField</span><span class=\"w\"> </span><span class=\"n\">Polynomial</span><span class=\"w\"> </span><span class=\"n\">LinearAlgebra</span><span class=\"bp\">.</span><span class=\"n\">Projectivization</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">fact_five_prime</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Fact</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Nat</span><span class=\"bp\">.</span><span class=\"n\">Prime</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">constructor</span><span class=\"bp\">;</span><span class=\"w\"> </span><span class=\"n\">norm_num</span>\n\n<span class=\"kn\">attribute</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"kn\">local</span><span class=\"w\"> </span><span class=\"kn\">instance</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"n\">fact_five_prime</span>\n\n<span class=\"kn\">abbrev</span><span class=\"w\"> </span><span class=\"n\">F5_18</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Type</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">GaloisField</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"w\"> </span><span class=\"mi\">18</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">curve_equation</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">F5_18</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">  </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">^</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">^</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"bp\">^</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">projective_points_on_curve</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Set</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">ℙ</span><span class=\"w\"> </span><span class=\"n\">F5_18</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Fin</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">F5_18</span><span class=\"o\">))</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">  </span><span class=\"o\">{</span><span class=\"w\"> </span><span class=\"n\">P</span><span class=\"w\"> </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"bp\">∃</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">F5_18</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">!</span><span class=\"o\">[</span><span class=\"n\">x</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"bp\">≠</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">),</span><span class=\"w\"> </span><span class=\"n\">P</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"bp\">.</span><span class=\"n\">mk</span><span class=\"w\"> </span><span class=\"bp\">_</span><span class=\"w\"> </span><span class=\"bp\">!</span><span class=\"o\">[</span><span class=\"n\">x</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"n\">curve_equation</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"o\">}</span>\n\n<span class=\"kn\">noncomputable</span><span class=\"w\"> </span><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">count_nonzero_points_on_curve</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"w\"> </span><span class=\"o\">:=</span>\n<span class=\"w\">  </span><span class=\"n\">Nat</span><span class=\"bp\">.</span><span class=\"n\">card</span><span class=\"w\"> </span><span class=\"n\">projective_points_on_curve</span>\n</code></pre></div>\n<p>Although this makes me sad in several ways:</p>\n<ul>\n<li><code>attribute [local instance] fact_five_prime</code> is clunky!</li>\n<li>where is my term elaborator <code>MVPolynomial.of% fun x y z =&gt; x ^ 3 * y + y ^ 3 * z + z ^ 3 * x</code>?</li>\n<li>How do I take the zeros of a homogeneous MVPolynomial as subset of projective space?</li>\n</ul>",
        "id": 481607732,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1731281970
    },
    {
        "content": "<p>We could</p>\n<ul>\n<li>collect these somewhere</li>\n<li>if someone want to try asking for access to write Lean formalisations of the whole set, having the sample problems done is a good start</li>\n<li>formalising the sample problems will already reveal holes in Mathlib</li>\n</ul>",
        "id": 481607906,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1731282095
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/481607584\">said</a>:</p>\n<blockquote>\n<p>I think it would be nice to write Lean formalisations of the FrontierMath sample problems (just the statements for now).</p>\n</blockquote>\n<p>I think this is a worthwhile task, but another place to direct the formalisation community's energy is in collecting challenge problems of a similar level which <em>don't</em> admit integer/polynomial/integer-matrix/... answers.  That is, problems where \"the proof is the point\", and there is no \"shortcut\" (like an otherwise-unguessable integer) for quickly convincing an evaluator that the problem has been solved.</p>",
        "id": 481608919,
        "sender_full_name": "Heather Macbeth",
        "timestamp": 1731283067
    },
    {
        "content": "<p>Epoch has shipped some great helper tools for AI forecasting, e.g. <a href=\"https://epochai.org/data/machine-learning-hardware\">https://epochai.org/data/machine-learning-hardware</a> -- I don't actually know why this benchmark is within their mandate, haha. I run into the first author here in berkeley from time to time so i'll ask him</p>",
        "id": 481865504,
        "sender_full_name": "Quinn",
        "timestamp": 1731387827
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/481527434\">said</a>:</p>\n<blockquote>\n<p>Also, there is an easy solution to the problem of AI for Lean verse building Mathlib: AI for building Mathlib, i.e. auto-formalization at scale! <span aria-label=\"joking\" class=\"emoji emoji-1f61c\" role=\"img\" title=\"joking\">:joking:</span> (I jest a bit, but I’m also somewhat serious.  I don’t exactly know what is holding us back from much stronger auto-formalization on a large and systematic scale.)</p>\n</blockquote>\n<p>As someone working on this, I feel there are (at least) three auxiliary tools we need:</p>\n<ol>\n<li><strong>Proof Automation</strong>: to complete the \"obvious\" steps in a proof, which will be omitted in a human proof.</li>\n<li><strong>Search</strong>: when an informal proof invokes a theorem, the corresponding theorem in Mathlib should be found and passed on to the proof automation.</li>\n<li><strong>Disproving</strong>: Some errors are inevitable in translation. So we need to filter, feedback and try again.</li>\n</ol>\n<p>We do have <code>aesop</code>, <code>duper</code> etc for <em>proof automation</em> and both <em>leansearch</em> and <em>moogle</em> (with APIs <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span> ) for search. Hopefully such tools will get better over time.</p>\n<p>For <em>disproving</em> we have <code>plausible</code> (formerly <code>slim_check</code>) but I feel this is the weakest link at present. </p>\n<p>There is another potential weak link: more advanced mathematics skips more details. But in my experience LLMs can fill in details pretty well.</p>",
        "id": 482084302,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1731470608
    },
    {
        "content": "<p>I think both DeepSeek Prover and AlphaProof had disproving capabilities they used for RL.  It is a pity this sort of thing isn’t more upstreamed.</p>",
        "id": 482084613,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731470812
    },
    {
        "content": "<p>Actually all those capabilities you mentioned are much stonger and more fleshed out in papers.  Its now time to build them in practice.</p>",
        "id": 482084807,
        "sender_full_name": "Jason Rute",
        "timestamp": 1731470936
    },
    {
        "content": "<p>It looks like o3 moved the ball forward here. <a href=\"https://x.com/OpenAI/status/1870186518230511844\">https://x.com/OpenAI/status/1870186518230511844</a></p>",
        "id": 490223819,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1734725356
    },
    {
        "content": "<p><a href=\"/user_uploads/3121/n4lP2voRpWxhsDSJs4k-q8mj/Screenshot-2024-12-20-at-3.56.46PM.png\">Graphic on the results</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/n4lP2voRpWxhsDSJs4k-q8mj/Screenshot-2024-12-20-at-3.56.46PM.png\" title=\"Graphic on the results\"><img data-original-dimensions=\"352x390\" src=\"/user_uploads/thumbnail/3121/n4lP2voRpWxhsDSJs4k-q8mj/Screenshot-2024-12-20-at-3.56.46PM.png/840x560.webp\"></a></div>",
        "id": 490229511,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1734728241
    },
    {
        "content": "<p><a href=\"https://openai.com/12-days/\">https://openai.com/12-days/</a> up now</p>",
        "id": 490234888,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1734731211
    },
    {
        "content": "<p>Yeah, I'm impressed!  Scaling laws are real.  If you are willing to spend o3/alphaproof level of inference compute, you can do impressive things (with good algorithms and sufficient training of course).</p>",
        "id": 490237046,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734732347
    },
    {
        "content": "<p>Just to be clear, by \"impressive things\" I don't necessarily mean solving the RH.  But solving easier unsolved math problems?  The kind a math graduate student solves for their first paper?  Maybe soon? Any thoughts from those more familiar with FrontierMath?</p>",
        "id": 490237413,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734732565
    },
    {
        "content": "<p>And also to be clear, I think o3 (run in this configuration) is currently costing thousands of dollars per benchmark problem.  (Or at least that was the case for the best o3 scores on ARC-AGI.  Does anyone have a reference either way?)</p>",
        "id": 490238020,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734732903
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/490238020\">said</a>:</p>\n<blockquote>\n<p>And also to be clear, I think o3 (run in this configuration) is currently costing thousands of dollars per benchmark problem.  (Or at least that was the case for the best o3 scores on ARC-AGI.  Does anyone have a reference either way?)</p>\n</blockquote>\n<p>Scaling laws are real <span aria-label=\"face with diagonal mouth\" class=\"emoji emoji-1fae4\" role=\"img\" title=\"face with diagonal mouth\">:face_with_diagonal_mouth:</span></p>",
        "id": 490294366,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1734783956
    },
    {
        "content": "<p>It’s just become more clear that the FrontierMath math problems are not quite as difficult as the quotes from Tao and Gowers made it seem.  There are three levels of difficulty and the lowest level is about IMO level (but still not requiring proofs like the IMO, just numerical solutions).  This is about 25% of problems in FrontierMath.  The Tao/Gowers quotes are for the hardest tier of problems in FrontierMath.  <a href=\"https://x.com/elliotglazer/status/1870235655714025817?s=12\">https://x.com/elliotglazer/status/1870235655714025817?s=12</a></p>",
        "id": 490294548,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734784104
    },
    {
        "content": "<p>It'll get cheaper. I'm very hopeful an AIME or IMO question can be solved for less than $1 in one year.</p>",
        "id": 490296348,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1734785527
    },
    {
        "content": "<p>Somewhat tangential but not wholly irrelevant; is there an industry standard procedure for controlling who gets to see what when running a nonpublic model on a nonpublic benchmark like frontier math, or is it negotiated on a case by case basis?</p>",
        "id": 490297246,
        "sender_full_name": "Chris Bailey",
        "timestamp": 1734786288
    },
    {
        "content": "<p>Going from 2% to 25% on FrontierMath sounds impressive. However, if you have a look at how this 2% has been obtained (10,000 tokens limit per problem, 0 examples in the prompt, 1 attempt per problem if I remember well), it is not fair to compare o3 25% with that. This is not the same compute budget class. To me, the 2% result was never representative of sota performance in LLM reasoning given this cheap inference setting. My controversial opinion is that this sounds like a marketing setup by OpenAI to create a \"wow\" effect on their o3 model. Let's not forget the amount of money that is at stake here <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span><br>\nHowever don't get me wrong, I think o3 is really impressive and that this is real progress. I'm just a bit critical about the marketing around it which makes it look like bigger than it really is</p>",
        "id": 490298723,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1734787547
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"258175\">Albert Jiang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/490296348\">said</a>:</p>\n<blockquote>\n<p>It'll get cheaper. I'm very hopeful an AIME or IMO question can be solved for less than $1 in one year.</p>\n</blockquote>\n<p>Especially now that I realize they are using majority voting with a lot of samples (one of the most inefficient search algorithms) I have no doubt the price will go way down.  The better these models get, the less they need to search (either in their “thinking time” or in majority voting), and that is not even factoring the ability to distill to smaller models and efficiencies in hardware.</p>",
        "id": 490298831,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734787641
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"321854\">Auguste Poiroux</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/490298723\">said</a>:</p>\n<blockquote>\n<p>Going from 2% to 25% on FrontierMath sounds impressive. However, if you have a look at how this 2% has been obtained (10,000 tokens limit per problem, 0 examples in the prompt, 1 attempt per problem if I remember well), it is not fair to compare o3 25% with that. This is not the same compute budget class. To me, the 2% result was never representative of sota performance in LLM reasoning given this cheap inference setting. My controversial opinion is that this sounds like a marketing setup by OpenAI to create a \"wow\" effect on their o3 model. Let's not forget the amount of money that is at stake here <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span><br>\nHowever don't get me wrong, I think o3 is really impressive and that this is real progress. I'm just a bit critical about the marketing around it which makes it look like bigger than it really is</p>\n</blockquote>\n<p>We really need to stop talking about  benchmark problem being solved/not solved.  It really depends on the amount of compute going in.  We see that in formal math too with MiniF2F and IMO.  Log-linear scaling is interesting and cool, but we should look at the plot, not a single number.</p>",
        "id": 490299085,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734787857
    },
    {
        "content": "<p>It will be interesting when we have an algorithm which we believe scales up to solving RH.  How much would we pay?  $1 million, $1 billion, $1 trillion?  (I don’t think AlphaProof or o3 would scale to RH.  I think for problems of that type the algorithms would plateaue.  But this is the difficulty with scaling.  It’s hard to measure the upper bound.)</p>",
        "id": 490299543,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734788314
    },
    {
        "content": "<p>heh, 1 million is apropos for that problem</p>",
        "id": 490299577,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1734788345
    },
    {
        "content": "<p>it is somehow ironic that one could end up spending the entire prize money on the compute budget</p>",
        "id": 490299699,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1734788433
    },
    {
        "content": "<p>I was already thinking the budget was a bit iffy at 1 cent per problem, back when this was something that is supposed to be competing with <code>sledgehammer</code></p>",
        "id": 490299814,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1734788533
    },
    {
        "content": "<p>this is now way outside anything I consider a reasonable budget for a formalization assistant</p>",
        "id": 490299833,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1734788564
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"321854\">Auguste Poiroux</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/FrontierMath.2C.20a.20benchmark.20for.20evaluating.20advanced.20mathema.2E.2E.2E/near/490298723\">said</a>:</p>\n<blockquote>\n<p>Going from 2% to 25% on FrontierMath sounds impressive. However, if you have a look at how this 2% has been obtained (10,000 tokens limit per problem, 0 examples in the prompt, 1 attempt per problem if I remember well), it is not fair to compare o3 25% with that. This is not the same compute budget class. To me, the 2% result was never representative of sota performance in LLM reasoning given this cheap inference setting. My controversial opinion is that this sounds like a marketing setup by OpenAI to create a \"wow\" effect on their o3 model. Let's not forget the amount of money that is at stake here <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span><br>\nHowever don't get me wrong, I think o3 is really impressive and that this is real progress. I'm just a bit critical about the marketing around it which makes it look like bigger than it really is</p>\n</blockquote>\n<p>I agree with you. Sometimes I think OpenAI is a necessary evil to get more money on AI research.</p>",
        "id": 490341200,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1734825392
    },
    {
        "content": "<p>I think $1 million for the RH would be a better-than-just reasonable amount to pay for a computer. In terms of the mathematical progress that would surely be spurred by studying the proof - and the ideas and fields it uncovers - I think it would be totally on par with what, say, $1 million buys the NSF in terms of mathematical output.</p>\n<p>That is to say: governments (and to a degree, private institutions) already pay in some units of ($ / mathematical ingenuity), and I think $1 million for a proof of the RH beats market rate. :)</p>",
        "id": 490569152,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1734981080
    },
    {
        "content": "<p>Of course we can't know exactly how much math will follow, and that is partially because we don't know what the proof will look like. But <em>all</em> grant money / exploratory research comes with that caveat, that you don't know exactly how hard it will be or how big the benefits will be in the end.</p>",
        "id": 490569424,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1734981152
    },
    {
        "content": "<p>On the other hand ... $1 trillion seems too high. To put it in perspective, that's (very roughly) equal to the combined salaries of all professors and graduate students in the US, across all disciplines, for four years. So spending $1 trillion could be financially equivalent to halting all research at US universities for four years.</p>\n<p>(Of course this is not quite true, because you can't just spend 2x the salaries to summon 2x the talent; and there's cost of support staff, and equipment, etc... But, on the margin, and to within an order of magnitude, something like this is true.)</p>",
        "id": 490570164,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1734981484
    },
    {
        "content": "<p>And I think we really need to consider AI models that don't only solve problems but make partial progress.  If we could have AI which generates solutions and partial progress on a range of problems in the same area, then we could have a much better sense of the value of the output per dollar spent on a model.  Then we could make reasonable decisions about if it is worth continuing to spend money on this.  (Also, see my tweet here: <a href=\"https://x.com/JasonRute/status/1870831707844124766\">https://x.com/JasonRute/status/1870831707844124766</a>.)</p>",
        "id": 490570488,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734981633
    },
    {
        "content": "<p>And I hope none of us are serious about rushing to RH, without first trying it on much simpler unsolved problems like those left open in recent papers or Mathoverflow posts.</p>",
        "id": 490570728,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734981737
    },
    {
        "content": "<p>For sure. On both points. If I spent $1M on an AI trying to prove RH, and I didn't get a proof out but it made me a cool new theory relating arithmetic functions and their essential singularities over C and their homology classes, I would be pretty satisfied with my purchase :P</p>",
        "id": 490571120,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1734981931
    }
]