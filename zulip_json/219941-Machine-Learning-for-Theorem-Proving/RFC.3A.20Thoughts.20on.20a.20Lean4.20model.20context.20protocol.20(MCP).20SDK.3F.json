[
    {
        "content": "<p>For those who don't know, a MCP is a way for LLMs to talk to external systems, so they can get more context for code generation. The external system can be documentation for a language or library like lean4/mathlib, or an API that does something else entirely. According to <a href=\"https://www.youtube.com/watch?v=PLKrSVuT-Dg\">this video</a>, MCPs are especially useful for writing code in new languages/frameworks that the AI training data didn't have enough examples of, such as svelte. <a href=\"https://github.com/modelcontextprotocol/servers\">This repository</a> has several examples of MCP SDKs for various languages. <a href=\"https://github.com/oOo0oOo/lean-lsp-mcp\">Someone already made a MCP for lean</a>, but there are just a few people working on it and it doesn't look like it's supported by the official org or community.</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"PLKrSVuT-Dg\" href=\"https://www.youtube.com/watch?v=PLKrSVuT-Dg\"><img src=\"https://uploads.zulipusercontent.net/ed0539c8f0fa02cfb7bb36278dfaaba3105a7fd5/68747470733a2f2f692e7974696d672e636f6d2f76692f504c4b72535675542d44672f6d7164656661756c742e6a7067\"></a></div><ul>\n<li>\n<p><strong>User Experience</strong>: How does this feature improve the user experience?<br>\nThis feature will make lean4 code generation more reliable with tools like Claude Code; this will lead to progress in the field of automatic theorem proving. </p>\n</li>\n<li>\n<p><strong>Beneficiaries</strong>: Which Lean users and projects benefit most from this feature/change?<br>\nMathematicians who want to experiment with prompt engineering, and anyone who is interested in Lean's applications to AI or likes vibe coding in general. </p>\n</li>\n<li>\n<p><strong>Maintainability</strong>: Will this change streamline code maintenance or simplify its structure?<br>\nI'm not sure exactly what impact it will have on the official lean4 project's code maintenance- if the maintainers want to use AI coding agents, it will be a big plus as it will help automate at least some of their work, if not, then I'm sure it will still help the community maintained repos, ie. more people will want to add theorems to mathlib if they get to play around with AI in doing so.</p>\n</li>\n</ul>\n<p>I'm curious to hear your thoughts on this, if there seems to be enthusiasm I'll open an RFC in the github issues.</p>",
        "id": 547358214,
        "sender_full_name": "Daniel Lebedinsky",
        "timestamp": 1761593915
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F\">#general &gt; RFC: Thoughts on a Lean4 model context protocol (MCP) SDK?</a> by <span class=\"user-mention silent\" data-user-id=\"110031\">Patrick Massot</span>.</p>",
        "id": 547361445,
        "sender_full_name": "Notification Bot",
        "timestamp": 1761595176
    },
    {
        "content": "<p>lean-lsp-mcp works perfectly for me, so I'm not sure if there's the need for another Lean MCP server. Are you suggesting that Lean should have an official MCP server?</p>",
        "id": 547363880,
        "sender_full_name": "Gavin Zhao",
        "timestamp": 1761596112
    },
    {
        "content": "<p>I could be wrong but I think lean-lsp-mcp is the go-to for people who want to use a Lean MCP server, so it's not like it's niche or not used actively by the community.</p>",
        "id": 547364141,
        "sender_full_name": "Gavin Zhao",
        "timestamp": 1761596205
    },
    {
        "content": "<p>Yes, lean-lsp-mcp is already ultra powerful, i don't think we need another one it has :</p>\n<ul>\n<li>infoview feedback</li>\n<li>lean compiler feedback</li>\n<li>local search</li>\n<li>external search lemmas (pattern search, semantic search, ...)</li>\n</ul>",
        "id": 547365034,
        "sender_full_name": "Alfredo Moreira-Rosa",
        "timestamp": 1761596568
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"766285\">Gavin Zhao</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547363880\">said</a>:</p>\n<blockquote>\n<p>lean-lsp-mcp works perfectly for me, so I'm not sure if there's the need for another Lean MCP server. Are you suggesting that Lean should have an official MCP server?</p>\n</blockquote>\n<p>I wasn't necessarily saying that there should be another one made from scratch. If that MCP is good then I guess my only suggestion would be to add it to the documentation.</p>",
        "id": 547375721,
        "sender_full_name": "Daniel Lebedinsky",
        "timestamp": 1761601068
    },
    {
        "content": "<p>There's probably a decent amount of room for improvement. </p>\n<p>For instance, LeanSearch does not live index Mathlib, meaning AI assistants do not have access to the most recent data, making collaboration difficult. They could technically read files and grep for things that they are looking for, but I do not think that this is a good way to do things (this is what ripgrep is doing in <code>lean-lsp-mcp</code> for local files). I am in the process of updating LeanExplore to fix this issue. </p>\n<p>I also think that using the LSP might be the wrong approach to doing things. A lot of projects simply compile files repeatedly and use the compiler feedback to fix proofs instead of needing to submit individual MCP tool requests to examine each error that pops up.</p>\n<p>In either <span class=\"user-mention\" data-user-id=\"802311\">@Oliver Dressler</span> has done a wonderful job with this package, and I am super happy to see progress being made in this area.</p>",
        "id": 547495674,
        "sender_full_name": "Justin Asher",
        "timestamp": 1761658614
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547495674\">said</a>:</p>\n<blockquote>\n<p>I also think that using the LSP might be the wrong approach to doing things.</p>\n</blockquote>\n<p>Interesting, what possible alternative approach(es) could a MCP take?</p>",
        "id": 547519536,
        "sender_full_name": "Daniel Lebedinsky",
        "timestamp": 1761664203
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"928151\">Daniel</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547519536\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547495674\">said</a>:</p>\n<blockquote>\n<p>I also think that using the LSP might be the wrong approach to doing things.</p>\n</blockquote>\n<p>Interesting, what possible alternative approach(es) could a MCP take?</p>\n</blockquote>\n<p>I think Justin answers this in his next sentence: run <code>lake build</code>.</p>\n<p>For small projects I've found this more reliable than having Claude read diagnostics.</p>",
        "id": 547613006,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1761700831
    },
    {
        "content": "<blockquote>\n<p>I think Justin answers this in his next sentence: run <code>lake build</code>.</p>\n</blockquote>\n<p>Yes, thanks, <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span>! <span aria-label=\"smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"smile\">:smile:</span> </p>\n<p>The only stipulation is that you would want to break the proof down into small enough steps that not too many diagnostics pop up in each individual proof. Meaning: use lots of lemmas! This is what handwritten blueprints do already, and why <a href=\"http://Math.inc\">Math.inc</a> has had the most autoformalization success.</p>\n<p>You could also have an MCP tool that targets getting diagnostics from specific theorems, meaning a method <code>get_diagnostics_from_declaration(name: str)</code>, so that we do not need to get diagnostics from the entire file each time for very long files.</p>",
        "id": 547621461,
        "sender_full_name": "Justin Asher",
        "timestamp": 1761707338
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547621461\">said</a>:</p>\n<blockquote>\n<p>You could also have an MCP tool that targets getting diagnostics from specific theorems, meaning a method <code>get_diagnostics_from_declaration(name: str)</code>, so that we do not need to get diagnostics from the entire file each time for very long files.</p>\n</blockquote>\n<p>Agreed, I have been rewriting the LSP interaction to be more async. One of the goals is to have optional line range parameters on get_diagnostics. This is not straightforward, as unlike most other LSP requests the diagnostic messages are not actively requested but just come in, whenever a line is done processing.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547495674\">said</a>:</p>\n<blockquote>\n<p>For instance, LeanSearch does not live index Mathlib, meaning AI assistants do not have access to the most recent data, making collaboration difficult. They could technically read files and grep for things that they are looking for, but I do not think that this is a good way to do things (this is what ripgrep is doing in <code>lean-lsp-mcp</code> for local files). I am in the process of updating LeanExplore to fix this issue. </p>\n</blockquote>\n<p>Excited to see what solution you are cooking up! Maybe some kind of local encoding?</p>\n<p>I think it should be mentioned here that the reason <code>lean-lsp-mcp</code> does not include a tool to interact with leanexplore is that they provide their own <a href=\"https://www.leanexplore.com/docs/mcp\">MCP</a> which works well.</p>",
        "id": 547661641,
        "sender_full_name": "Oliver Dressler",
        "timestamp": 1761729001
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547621461\">said</a>:</p>\n<blockquote>\n<p>The only stipulation is that you would want to break the proof down into small enough steps that not too many diagnostics pop up in each individual proof.</p>\n</blockquote>\n<p>I think, to the contrary, one wants to instruct the agent to only ever write one step at a time, so that there is only ever one diagnostic (on the most recently added step of the proof).</p>\n<p>I have never seen an agent successfully write a long one-shot proof that had an error early on, and then sensibly recover.</p>",
        "id": 547683083,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1761734799
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547683083\">said</a>:</p>\n<blockquote>\n<p>I have never seen an agent successfully write a long one-shot proof that had an error early on, and then sensibly recover.</p>\n</blockquote>\n<p>For examples see the <a href=\"https://arxiv.org/abs/2508.03613\">Goedel-Prover-V2</a> article, for example Table 4 and Figure 7 show how Goedel-Prover-V2 performance improves with self-correction. However, this might not  fit your understanding of \"long\" or \"early on\".</p>\n<p>To ground \"long\" and \"early on\" with an example, just this morning I had Goedel-Prover-V2 on its first round of self-correction fix a 263 line proof of Putnam 1962 A6.</p>",
        "id": 547711727,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1761743396
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547683083\">said</a>:</p>\n<blockquote>\n<p>I have never seen an agent successfully write a long one-shot proof that had an error early on, and then sensibly recover.</p>\n</blockquote>\n<p>I've also seen sonnet 4.5 (via Claude code with my <a href=\"https://github.com/cameronfreer/lean4-skills/blob/main/plugins/lean4-theorem-proving/README.md\">skill</a>) recover many times on 100+ line proofs in this situation -- it often works best for it to fix errors at the top first (leading to further cascading errors lower down in the proof, but it eventually eliminates them all in the course of working downwards).</p>",
        "id": 547756803,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1761753885
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"802311\">Oliver Dressler</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547661641\">said</a>:</p>\n<blockquote>\n<p>Excited to see what solution you are cooking up! Maybe some kind of local encoding?</p>\n</blockquote>\n<p>I am planning to start to wrap doc-gen4, so I can build on top of the wonderful work that the Lean FRO is doing already, and just handle the semantic search side of things myself. I want to do nightly updates.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547683083\">said</a>:</p>\n<blockquote>\n<p>I think, to the contrary, one wants to instruct the agent to only ever write one step at a time, so that there is only ever one diagnostic (on the most recently added step of the proof).</p>\n</blockquote>\n<p>This does <em>seem</em> ideal to me, too. However, I think the current SOTA is doing whole-proof rollout with proof-repair (<a href=\"https://arxiv.org/abs/2507.23726\">Seed-Prover</a>, <a href=\"https://arxiv.org/abs/2507.15225\">Delta Prover</a>, <a href=\"https://arxiv.org/abs/2509.22819\">Hilbert</a>). I think there is a good middle ground here by keeping the proofs on the shorter side of things (instead of encouraging the agent to produce 100+ line proofs). This allows the models to focus on a few errors at once, then use the completed lemmas in subsequent proofs without needing to worry about them.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"116846\">Cameron Freer</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547756803\">said</a>:</p>\n<blockquote>\n<p>leading to further cascading errors lower down in the proof, but it eventually eliminates them all in the course of working downwards</p>\n</blockquote>\n<p>Yes, we should test if it is indeed faster to have one long proof or breaking things into lemmas. My gut tells me that encouraging the LLM to structure its output in a way that mimics the way Mathlib is written would be ideal, but I could be wrong.</p>",
        "id": 547786461,
        "sender_full_name": "Justin Asher",
        "timestamp": 1761762494
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547613006\">said</a>:</p>\n<blockquote>\n<p>I think Justin answers this in his next sentence: run <code>lake build</code>.</p>\n<p>For small projects I've found this more reliable than having Claude read diagnostics.</p>\n</blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"802311\">Oliver Dressler</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547661641\">said</a>:</p>\n<blockquote>\n<p>I think it should be mentioned here that the reason <code>lean-lsp-mcp</code> does not include a tool to interact with leanexplore is that they provide their own <a href=\"https://www.leanexplore.com/docs/mcp\">MCP</a> which works well.</p>\n</blockquote>\n<p>The keyword is \"small projects\". I've only started testing out lean-lsp-mcp and it seems very good so far, I haven't tried the leanexplore mcp yet, from my job experience I've found that in general genAI is a great force multiplier for large software projects, and MCPs make it more accurate. </p>\n<p>What I meant by my last question though, is LSP the standard for designing MCPs?</p>",
        "id": 547804649,
        "sender_full_name": "Daniel Lebedinsky",
        "timestamp": 1761769168
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"928151\">Daniel</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547804649\">said</a>:</p>\n<blockquote>\n<p>What I meant by my last question though, is LSP the standard for designing MCPs?</p>\n</blockquote>\n<p>That's an excellent question, and slightly outside my area of expertise. It seems like there are some open source projects in this direction (e.g., <a href=\"https://github.com/isaacphi/mcp-language-server\"><code>mcp-language-server</code></a>), but I am not aware how much of an advantage this provides. I do not believe Claude Code or Codex use this, and instead just run the files each time to see any errors. Any thoughts?</p>",
        "id": 547805170,
        "sender_full_name": "Justin Asher",
        "timestamp": 1761769413
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116846\">Cameron Freer</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547756803\">said</a>:</p>\n<blockquote>\n<p>@Kim Morrison <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/RFC.3A.20Thoughts.20on.20a.20Lean4.20model.20context.20protocol.20.28MCP.29.20SDK.3F/near/547683083\">said</a>:</p>\n<blockquote>\n<p>I have never seen an agent successfully write a long one-shot proof that had an error early on, and then sensibly recover.</p>\n</blockquote>\n<p>I've also seen sonnet 4.5 (via Claude code with my <a href=\"https://github.com/cameronfreer/lean4-skills/blob/main/plugins/lean4-theorem-proving/README.md\">skill</a>) recover many times on 100+ line proofs in this situation -- it often works best for it to fix errors at the top first (leading to further cascading errors lower down in the proof, but it eventually eliminates them all in the course of working downwards).</p>\n</blockquote>\n<p>I also rarely see agents successfully write non-trivial Python programs with a one shot prompt, in my work we use techniques like <a href=\"https://www.promptingguide.ai/techniques/tot\">Tree of Thought prompting</a> and prompt graphs to generate large bodies of text with greater accuracy. I'd be very interested to see research into these kinds of techniques being used to generate proofs.</p>",
        "id": 547805481,
        "sender_full_name": "Daniel Lebedinsky",
        "timestamp": 1761769540
    },
    {
        "content": "<p>One downside of a <code>lake build</code> agentic workflow is that the agent can't benefit from Lean's incremental compilation.</p>",
        "id": 548033316,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1761858437
    }
]