[
    {
        "content": "<p>There are so many formal theorem proving submissions to this year's ICLR.  (<a href=\"https://openreview.net/group?id=ICLR.cc/2025/Conference&amp;referrer=%5BHomepage%5D(%2F)#tab-active-submissions\">Submissions to ICLR are public.</a>)  If you search for keywords like \"theorem proving\", \"autoformalization\", \"lean\", \"isabelle\" (and maybe others) you will see a lot.  I think I count 12 papers for just the keyword \"Lean\" (that seem to be about the Lean theorem prover) and 1 for \"Isabelle\".  (None for \"Coq\". <span aria-label=\"sad\" class=\"emoji emoji-2639\" role=\"img\" title=\"sad\">:sad:</span> ) Most are new to me.  I'll try to make a list of all the theorem proving papers soon.</p>",
        "id": 474935355,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728095104
    },
    {
        "content": "<p>Wondering where can we find the PDFs? It seems that there is no buttons for that... Googling the titles for an arxiv also fails (for many papers)</p>",
        "id": 474935753,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1728095517
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"563306\">fzyzcjy</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ICLR.202025.20Submissions/near/474935753\">said</a>:</p>\n<blockquote>\n<p>Wondering where can we find the PDFs? It seems that there is no buttons for that... Googling the titles for an arxiv also fails (for many papers)</p>\n</blockquote>\n<p>Oh, I might have jumped the gun, but I would guess that the PDFs will be available on OpenReview in about a week or so.</p>",
        "id": 474968259,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728124541
    },
    {
        "content": "<p>Thanks!</p>",
        "id": 474968324,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1728124599
    },
    {
        "content": "<p>A subset of the relevant papers that I compiled is below - </p>\n<p>Synthetic Theorem Generation in Lean - <a href=\"https://openreview.net/forum?id=EeDSMy5Ruj\">https://openreview.net/forum?id=EeDSMy5Ruj</a> - tl;dr - forward reasoning to generate theorems</p>\n<p>Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically - <a href=\"https://openreview.net/forum?id=D23JcXiUwf\">https://openreview.net/forum?id=D23JcXiUwf</a> - tl;dr - RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas</p>\n<p>ImProver: Agent-Based Automated Proof Optimization - <a href=\"https://openreview.net/forum?id=dWsdJAXjQD\">https://openreview.net/forum?id=dWsdJAXjQD</a> - tld;r - proof rewriting to make proofs shorter, more modular, more readable</p>\n<p>Alchemy: Amplifying Theorem-Proving Capability Through Symbolic Mutation - <a href=\"https://openreview.net/forum?id=7NL74jUiMg\">https://openreview.net/forum?id=7NL74jUiMg</a> - tl;dr - sounds like forward reasoning on existing theorems to generate new ones? Not clear to me tho</p>\n<p>CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search - <a href=\"https://openreview.net/forum?id=VQwI055flA\">https://openreview.net/forum?id=VQwI055flA</a> </p>\n<p>FormalAlign: Automated Alignment Evaluation for Autoformalization - <a href=\"https://openreview.net/forum?id=B5RrIFMqbe\">https://openreview.net/forum?id=B5RrIFMqbe</a></p>\n<p>3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes - <a href=\"https://openreview.net/forum?id=7gGVDrqVaz\">https://openreview.net/forum?id=7gGVDrqVaz</a> - tl;dr - DPP to subselect for a diverse set of tactics to reduce branching factor </p>\n<p>Collaborative Theorem Proving with Large Language Models: Enhancing Formal Proofs with ProofRefiner - <a href=\"https://openreview.net/forum?id=y9xNQZjUJM\">https://openreview.net/forum?id=y9xNQZjUJM</a> </p>\n<p>LeanAgent: Lifelong Learning for Formal Theorem Proving - <a href=\"https://openreview.net/forum?id=Uo4EHT4ZZ8\">https://openreview.net/forum?id=Uo4EHT4ZZ8</a> - </p>\n<p>SubgoalXL: SUBGOAL-BASED EXPERT LEARNING FOR THEOREM PROVING - <a href=\"https://arxiv.org/pdf/2408.11172\">https://arxiv.org/pdf/2408.11172</a></p>\n<p>Herald: A Natural Language Annotated Lean 4 Dataset - <a href=\"https://openreview.net/forum?id=Se6MgCtRhz\">https://openreview.net/forum?id=Se6MgCtRhz</a></p>\n<p>Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search - <a href=\"https://openreview.net/forum?id=I4YAIwrsXa\">https://openreview.net/forum?id=I4YAIwrsXa</a> - This is the DeepSeek Prover 1.5 paper I believe</p>\n<p>Process-Driven Autoformalization in Lean 4 - <a href=\"https://openreview.net/forum?id=k8KsI84Ds7\">https://openreview.net/forum?id=k8KsI84Ds7</a> - tl;dr Lean compilation feedback to enhance autoformalization </p>\n<p>Lean-STaR: Learning to Interleave Thinking and Proving - <a href=\"https://arxiv.org/abs/2407.10040\">https://arxiv.org/abs/2407.10040</a> - tl;dr interleave natural language and Lean </p>\n<p>Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization - <a href=\"https://openreview.net/forum?id=Qdp7hlenr6\">https://openreview.net/forum?id=Qdp7hlenr6</a></p>",
        "id": 475186957,
        "sender_full_name": "Sid",
        "timestamp": 1728273007
    },
    {
        "content": "<p>Hopefully without abusing of your kindness since you already went through all this trouble, do you know if any of the above are actually usable (without too much trouble) for someone actually working with lean?</p>",
        "id": 475282407,
        "sender_full_name": "Luigi Massacci",
        "timestamp": 1728303009
    },
    {
        "content": "<p>I don't really know how openreview works. Is it possible to see the papers at this point?</p>",
        "id": 475284081,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1728303584
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"243562\">@Adam Topaz</span> see below.  My guess is they will be available in a few days.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ICLR.202025.20Submissions/near/474968259\">said</a>:</p>\n<blockquote>\n<p>Oh, I might have jumped the gun, but I would guess that the PDFs will be available on OpenReview in about a week or so.</p>\n<p><div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code>\n</code></pre></div><br>\n</p>\n</blockquote>",
        "id": 475284702,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728303791
    },
    {
        "content": "<p>Thanks!</p>",
        "id": 475284757,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1728303809
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466290\">Luigi Massacci</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ICLR.202025.20Submissions/near/475282407\">said</a>:</p>\n<blockquote>\n<p>Hopefully without abusing of your kindness since you already went through all this trouble, do you know if any of the above are actually usable (without too much trouble) for someone actually working with lean?</p>\n</blockquote>\n<p>It is difficult to tell since they are just abstracts at the moment (and when the PDFs are visible in about a week, they still will be anonymous without, say, GitHub links).  Moreover, usability hasn't been a concern of most researchers.  There are no incentives for it.  Frankly, even the current \"usable\" models aren't very good, and few (if any) people use them.  But, having said that, I have three caveats:</p>\n<ol>\n<li>Some trained LLM models like DeepSeek-Prover v1.5 are publically available on huggingface, so it is possible to easily code something up trying to use it locally from within Lean.  (But it may not be useful since it was optimized for competition math problems.)</li>\n<li>Some papers use existing LLM APIs. Even if not public, they could be reimplemented with minimal work.  (The <a href=\"https://openreview.net/forum?id=dWsdJAXjQD\">ImProver paper</a> may be an interesting one, but again I can't tell from the abstract.)</li>\n<li>The many autoformalization papers this year may lead to a lot of possibilities for useful applications.  It may be that someone on one of those papers made something useful, or if not, that it would be worth it to reimplement their ideas.</li>\n</ol>",
        "id": 475290585,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728305619
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466290\">Luigi Massacci</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ICLR.202025.20Submissions/near/475282407\">said</a>:</p>\n<blockquote>\n<p>Hopefully without abusing of your kindness since you already went through all this trouble, do you know if any of the above are actually usable (without too much trouble) for someone actually working with lean?</p>\n</blockquote>\n<p>To add to Jason Rute's excellent answer, InternLM2-Step-Prover is another model I've found fairly usable. But ya, until the papers' text is released it's hard to judge. I think some of them are already on arxiv and for those, the Lean-Star paper is something that I think the DeepSeek team took some ideas from so that might be a good one to check out. The DeepSeek 1 and 1.5 Prover papers are very good reads as well. Lots of good ideas.</p>",
        "id": 475371957,
        "sender_full_name": "Sid",
        "timestamp": 1728327889
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ICLR.202025.20Submissions/near/475290585\">said</a>:</p>\n<blockquote>\n<p>It is difficult to tell since they are just abstracts at the moment (and when the PDFs are visible in about a week, they still will be anonymous without, say, GitHub links). </p>\n</blockquote>\n<p>My apologies, I typed my message from the phone so I didn’t look carefully at the website, and assumed it was some kind of preprint setup from the name.</p>\n<blockquote>\n<p>Moreover, usability hasn't been a concern of most researchers.  There are no incentives for it. </p>\n</blockquote>\n<p>I’m sadly aware of that (you explained really well why that is the case in the past somewhere around here) but hope is the last to die… </p>\n<p>As for the rest, anything beginning by “reimplement” doesn’t qualify as “usable” : ( </p>\n<p>Incidentally, <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span>, I seem to remember the FRO was working on ML integration, or did I hallucinate that? Arguably providing some kind of more plug and play pipeline is the kind of important but not incentivised by the system™ development where the FRO excels</p>",
        "id": 475380541,
        "sender_full_name": "Luigi Massacci",
        "timestamp": 1728330537
    },
    {
        "content": "<p>Paper pdfs are visible now</p>",
        "id": 476871461,
        "sender_full_name": "Zheng Yuan",
        "timestamp": 1728953750
    },
    {
        "content": "<p>After searching for terms like \"Lean,Isabelle,miniF2F,ProofNet\" and \"autoformal\", I found 20 papers related to formal proofs. All of these papers use Lean, except for <a href=\"https://openreview.net/pdf?id=EXaKfdsw04\">4-StepProof</a>, <a href=\"https://openreview.net/pdf?id=mb2rHLcKN5\">6-SubgoalXL</a>, and <a href=\"https://openreview.net/pdf?id=D23JcXiUwf\">9-ProD-RL</a>, which use Isabelle. (None for Coq)</p>\n<p>Here's the list:</p>\n<table>\n<thead>\n<tr>\n<th>Index</th>\n<th>Abbreviation</th>\n<th>Source</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>LeanAgent</td>\n<td><a href=\"https://openreview.net/pdf?id=Uo4EHT4ZZ8\">LeanAgent: Lifelong Learning for Formal Theorem Proving</a></td>\n</tr>\n<tr>\n<td>2</td>\n<td>ImProver</td>\n<td><a href=\"https://openreview.net/pdf?id=dWsdJAXjQD\">ImProver: Agent-Based Automated Proof Optimization</a></td>\n</tr>\n<tr>\n<td>3</td>\n<td>Herald</td>\n<td><a href=\"https://openreview.net/pdf?id=Se6MgCtRhz\">Herald: A Natural Language Annotated Lean 4 Dataset</a></td>\n</tr>\n<tr>\n<td>4</td>\n<td>StepProof</td>\n<td><a href=\"https://openreview.net/pdf?id=EXaKfdsw04\">StepProof: Step-by-step verification of natural language mathematical proofs</a></td>\n</tr>\n<tr>\n<td>5</td>\n<td>Lean-STaR</td>\n<td><a href=\"https://openreview.net/pdf?id=SOWZ59UyNc\">Lean-STaR: Learning to Interleave Thinking and Proving</a></td>\n</tr>\n<tr>\n<td>6</td>\n<td>SubgoalXL</td>\n<td><a href=\"https://openreview.net/pdf?id=mb2rHLcKN5\">SubgoalXL: Subgoal-based Expert Learning for Theorem Proving</a></td>\n</tr>\n<tr>\n<td>7</td>\n<td>FormL4</td>\n<td><a href=\"https://openreview.net/pdf?id=k8KsI84Ds7\">Process-Driven Autoformalization in Lean 4</a></td>\n</tr>\n<tr>\n<td>8</td>\n<td>miniCTX</td>\n<td><a href=\"https://openreview.net/pdf?id=KIgaAqEFHW\">miniCTX: Neural Theorem Proving with (Long-)Contexts</a></td>\n</tr>\n<tr>\n<td>9</td>\n<td>ProD-RL</td>\n<td><a href=\"https://openreview.net/pdf?id=D23JcXiUwf\">Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically</a></td>\n</tr>\n<tr>\n<td>10</td>\n<td>ProofRefiner</td>\n<td><a href=\"https://openreview.net/pdf?id=y9xNQZjUJM\">Collaborative Theorem Proving with Large Language Models: Enhancing Formal Proofs with ProofRefiner</a></td>\n</tr>\n<tr>\n<td>11</td>\n<td>Lean-ing</td>\n<td><a href=\"https://openreview.net/pdf?id=Qdp7hlenr6\">Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization</a></td>\n</tr>\n<tr>\n<td>12</td>\n<td>Con-NF</td>\n<td><a href=\"https://openreview.net/pdf?id=hUb2At2DsQ\">Rethinking and improving autoformalization: towards a faithful metric and a Dependency Retrieval-based approach</a></td>\n</tr>\n<tr>\n<td>13</td>\n<td>FormalAlign</td>\n<td><a href=\"https://openreview.net/pdf?id=B5RrIFMqbe\">FormalAlign: Automated Alignment Evaluation for Autoformalization</a></td>\n</tr>\n<tr>\n<td>14</td>\n<td>CARTS</td>\n<td><a href=\"https://openreview.net/pdf?id=VQwI055flA\">CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search</a></td>\n</tr>\n<tr>\n<td>15</td>\n<td>3D-Prover</td>\n<td><a href=\"https://openreview.net/pdf?id=7gGVDrqVaz\">3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes</a></td>\n</tr>\n<tr>\n<td>16</td>\n<td>SynLean</td>\n<td><a href=\"https://openreview.net/pdf?id=EeDSMy5Ruj\">Synthetic Theorem Generation in Lean</a></td>\n</tr>\n<tr>\n<td>17</td>\n<td>RMaxTS</td>\n<td><a href=\"https://openreview.net/pdf?id=I4YAIwrsXa\">Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search</a></td>\n</tr>\n<tr>\n<td>18</td>\n<td>Alchemy</td>\n<td><a href=\"https://openreview.net/pdf?id=7NL74jUiMg\">Alchemy: Amplifying Theorem-Proving Capability Through Symbolic Mutation</a></td>\n</tr>\n<tr>\n<td>19</td>\n<td>ZIP-FIT</td>\n<td><a href=\"https://openreview.net/pdf?id=4JBEpP6eRS\">ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment</a></td>\n</tr>\n<tr>\n<td>20</td>\n<td>LIPS</td>\n<td><a href=\"https://openreview.net/pdf?id=FiyS0ecSm0\">Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning</a></td>\n</tr>\n</tbody>\n</table>\n<p>And Some other works:</p>\n<ol>\n<li>\n<p>FOL Reasoning:</p>\n<ul>\n<li><a href=\"https://openreview.net/pdf?id=C25SgeXWjE\">Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation</a></li>\n<li><a href=\"https://openreview.net/pdf?id=JNZ3Om6NPS\">On Inherent Limitations of GPT/LLM Architecture</a></li>\n</ul>\n</li>\n<li>\n<p>SAT:</p>\n<ul>\n<li><a href=\"https://openreview.net/pdf?id=VVO3ApdMUE\">Transformer Encoder Satisfiability: Complexity and Impact on Formal Reasoning</a></li>\n</ul>\n</li>\n<li>\n<p>Automatic Geometry Problem Solving (GPS):</p>\n<ul>\n<li><a href=\"https://openreview.net/pdf?id=6RiBl5sCDF\">GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training</a></li>\n</ul>\n</li>\n</ol>",
        "id": 477553694,
        "sender_full_name": "RexWang",
        "timestamp": 1729217891
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"572535\">@RexWang</span>  That's really useful. Thanks! FYI, <a href=\"https://openreview.net/forum?id=y9xNQZjUJM\">ProofRefiner</a> is plagiarizing LeanCopilot and has been desk rejected.</p>",
        "id": 477826117,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1729360288
    },
    {
        "content": "<p>Indeed the abstracts of those two papers are surprisingly similar.</p>",
        "id": 477828332,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1729362497
    },
    {
        "content": "<p>A longer list for papers related to AI for mathematics in ICLR 2025 submissions: <a href=\"https://github.com/fzyzcjy/ai_math_paper_list\">https://github.com/fzyzcjy/ai_math_paper_list</a>. In total there are ~150 papers, since many of them are not formal methods. I mainly do it because of my personal need, and put it here since it might be useful for someone.</p>",
        "id": 477966797,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1729490609
    },
    {
        "content": "<p>i link to this thread in this month's Guaranteed Safe AI Newsletter <a href=\"https://gsai.substack.com/p/october-2024-progress-in-guaranteed\">https://gsai.substack.com/p/october-2024-progress-in-guaranteed</a></p>",
        "id": 479376283,
        "sender_full_name": "Quinn",
        "timestamp": 1730158049
    }
]