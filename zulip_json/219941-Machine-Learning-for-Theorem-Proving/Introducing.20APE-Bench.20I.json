[
    {
        "content": "<p>We are excited to share our latest work, \"APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries.\" In this paper, we introduce a novel paradigm of Automated Proof Engineering (APE), which goes beyond the traditional focus on isolated theorem proving by applying large language models to real-world development and maintenance tasks within Mathlib4. Under this new paradigm, models are expected to generate syntactically correct and semantically aligned code patches based on natural language instructions and pre-edit Lean files.</p>\n<p>Inspired by the design of SWE-Bench, we present APE-Bench I, the first benchmark targeting file-level structural edits in formal mathematics. We also develop a scalable verification infrastructure tailored for Lean, and combine compilation with LLM-based semantic evaluation. Finally, our work includes a systematic evaluation of current state-of-the-art models on this challenging new task. The paper has been uploaded (<a href=\"https://xinhuajian.wordpress.com/wp-content/uploads/2025/04/ape_bench_i-2.pdf\">paper link</a>), and all datasets and code will be open-sourced in the near future.</p>\n<p>As the starting point of the APE-Bench series, this work lays the groundwork for automated proof engineering. Looking ahead, we plan to extend the benchmark towards multi-file coordination scenarios (APE-Bench II) and autonomous agent systems with task planning and feedback-driven repair capabilities (APE-Bench III), paving the way toward practical, scalable, and engineering-ready formal mathematics powered by LLMs.</p>\n<p><a href=\"/user_uploads/3121/afLn8S2QlLKcxvM0BStBSQ-H/image.png\">image.png</a><br>\n<a href=\"/user_uploads/3121/BlP1E7BPRVBgStuuFMFYtPNB/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/afLn8S2QlLKcxvM0BStBSQ-H/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"1078x1394\" src=\"/user_uploads/thumbnail/3121/afLn8S2QlLKcxvM0BStBSQ-H/image.png/840x560.webp\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/3121/BlP1E7BPRVBgStuuFMFYtPNB/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"1939x2048\" src=\"/user_uploads/thumbnail/3121/BlP1E7BPRVBgStuuFMFYtPNB/image.png/840x560.webp\"></a></div>",
        "id": 514661773,
        "sender_full_name": "Huajian Xin",
        "timestamp": 1745771450
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"481527\">@Huajian Xin</span> Thanks for sharing this. It appears o3-mini does very well. Have you started testing newer models like Gemini 2.5 Pro or o4-mini? I would be very interested in seeing how these models perform.</p>",
        "id": 514662513,
        "sender_full_name": "Justin Asher",
        "timestamp": 1745771972
    },
    {
        "content": "<p>Great benchmark!<br>\nI have a few comments and improvement suggestions. These are not specific to your work only. They represent my opinion, do what you want with them :)</p>\n<ul>\n<li><strong>LLM-As-A-Judge</strong>: Semantic evaluation is super hard, I agree. However, when using LLM-As-A-Judge, it requires trusting the underlying LLM here (1) to be accurate and (2) to not be biased/gamed towards some predictions.<ul>\n<li>I see that you do a majority voting over 4 samples per evaluation, which can mitigate (2) to some extent. But, it is known that LLMs have a tendency to overestimate the correctness of their own output. Claude Sonnet 3.7 is the Judge in your experiments. And Claude Sonnet 3.7 is also the only model evaluated with a 0.0% \"Semantic Drop\". Are the outputs overestimated? Or is Claude excellent at producing semantically valid code?</li>\n<li>Regarding (1), did you manually measure how accurate your LLM-As-A-Judge metric is? I think this would really strengthen your results and claims in the paper if you add more information about that.</li>\n<li>LLM-As-A-Judge is likely to return some false positives, even with majority voting. Doing pass@k on a metric with false positives can be quickly deceptive and overestimate the real accuracy as k increases.</li>\n</ul>\n</li>\n<li><strong>Data contamination</strong>: Since you are using Mathlib4 as data for APE-Bench I, it is very likely that LLMs are trained on it. How do you address this issue?</li>\n</ul>",
        "id": 514673304,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1745779984
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Introducing.20APE-Bench.20I/near/514662513\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"481527\">Huajian Xin</span> Thanks for sharing this. It appears o3-mini does very well. Have you started testing newer models like Gemini 2.5 Pro or o4-mini? I would be very interested in seeing how these models perform.</p>\n</blockquote>\n<p>Thanks for your comments! We will test them in the near future :)</p>",
        "id": 514716295,
        "sender_full_name": "Huajian Xin",
        "timestamp": 1745812890
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"321854\">Auguste Poiroux</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Introducing.20APE-Bench.20I/near/514673304\">said</a>:</p>\n<blockquote>\n<p>Great benchmark!<br>\nI have a few comments and improvement suggestions. These are not specific to your work only. They represent my opinion, do what you want with them :)</p>\n<ul>\n<li><strong>LLM-As-A-Judge</strong>: Semantic evaluation is super hard, I agree. However, when using LLM-As-A-Judge, it requires trusting the underlying LLM here (1) to be accurate and (2) to not be biased/gamed towards some predictions.<ul>\n<li>I see that you do a majority voting over 4 samples per evaluation, which can mitigate (2) to some extent. But, it is known that LLMs have a tendency to overestimate the correctness of their own output. Claude Sonnet 3.7 is the Judge in your experiments. And Claude Sonnet 3.7 is also the only model evaluated with a 0.0% \"Semantic Drop\". Are the outputs overestimated? Or is Claude excellent at producing semantically valid code?</li>\n<li>Regarding (1), did you manually measure how accurate your LLM-As-A-Judge metric is? I think this would really strengthen your results and claims in the paper if you add more information about that.</li>\n<li>LLM-As-A-Judge is likely to return some false positives, even with majority voting. Doing pass@k on a metric with false positives can be quickly deceptive and overestimate the real accuracy as k increases.</li>\n</ul>\n</li>\n<li><strong>Data contamination</strong>: Since you are using Mathlib4 as data for APE-Bench I, it is very likely that LLMs are trained on it. How do you address this issue?</li>\n</ul>\n</blockquote>\n<p>Thanks for your comments! As for LLM-as-a-Judge, we mentioned in the paper that we also used o3-mini as the judge model and it also shows nearly zero semantic drop for Claude Sonnet 3.7 and even higher semantic drop for o3-mini itself. But Sonnet's judgement responses are more stable than o3-mini's and to control the complexity of overall evaluation framework for future comparison, we choose the Sonnet alone as the judgement. As for data contamination, all test cases are extracted from commits after Feb this year and it is believed that generally all these models are pretrained before that, and in response samples we also do not observed any response that is exactly same with the ground truth.</p>",
        "id": 514716821,
        "sender_full_name": "Huajian Xin",
        "timestamp": 1745813242
    }
]