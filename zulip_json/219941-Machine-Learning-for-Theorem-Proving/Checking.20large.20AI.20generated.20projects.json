[
    {
        "content": "<p>The recent proof by Gauss of the Strong PNT brings up an important question, one that I briefly discussed with <span class=\"user-mention\" data-user-id=\"635243\">@Talia Ringer</span> on X (<a href=\"https://x.com/TaliaRinger/status/1967148801862533359\">https://x.com/TaliaRinger/status/1967148801862533359</a>).</p>",
        "id": 539531672,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936066
    },
    {
        "content": "<p><strong>How do we verify large AI proofs?</strong></p>",
        "id": 539531692,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936074
    },
    {
        "content": "<p>A common answer is just:</p>\n<ul>\n<li>Does it check in Lean?</li>\n<li>Does it only use the three standard axioms (according to  <code>#print axioms</code>)?</li>\n<li>Does it check in <code>lean4checker</code>?</li>\n</ul>",
        "id": 539531716,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936080
    },
    {
        "content": "<p>But this avoids more subtle questions:</p>\n<ul>\n<li>Is the statement correct, and how does one know?</li>\n<li>Did the AI exploit bugs in Lean (in any form, not necessarily in the kernel)?  There are many kinds of exploits that lean4checker can’t catch.</li>\n<li>Is the statement what it really seems to mean, either because of a bug in an AI-generated definition, because of a bug in a mathlib definition, because of misleading pretty printing, or because of the code changing the meaning of notation or coercion?</li>\n</ul>",
        "id": 539531760,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936092
    },
    {
        "content": "<p>As for checking things automatically, I’ve been very impressed with <a href=\"https://github.com/GasStationManager/SafeVerify\">SafeVerify</a> by <span class=\"user-mention\" data-user-id=\"776090\">@GasStationManager</span>.  I think it is really ahead of any official Lean tool in this regard.  It lets you put the theorem statement (and any needed definitions to state the statement) in one file (with a sorry proof) and the full development in another file (including all lemmas, definitions, and proofs).  Then it runs a lean4checker-like verifier on the new proof terms, checks axioms, and also checks that the code didn’t change the meaning of the statement.  I don’t think, however, that it works on multiple file developments, so that is an issue.</p>",
        "id": 539531779,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936098
    },
    {
        "content": "<p>As for checking the statement of the theorem, I’m less sure about what should be standard here.  Obviously, one needs to check the theorem statement and the new definitions used in it.  (The Strong PNT statement only uses one non-mathlib definition, and that depends only on mathlib definitions.) But can one blindly trust definitions in mathlib?  One approach is to go far back in the definition hierarchy until one gets to common definitions that are well used (with lots of theorems about them).  (It would be nice to have a tool that automatically lists the hierarchy of definitions for a statement.)</p>",
        "id": 539531809,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936105
    },
    {
        "content": "<p>Another approach to definitions is to follow the Liquid Tensor Experiment and give unit-test-like theorems that uniquely characterize each definition.  One can even have an AI fill in the proofs of these theorems.  One could conceivably build a library of such tests, and a list of what standard mathlib definitions have been verified this way.</p>",
        "id": 539531931,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936140
    },
    {
        "content": "<p>Ideally, (for checking correctness at least), one should not have to go through every lemma and intermediate definition used in the development (and certainly not every proof).  That is the point of automatic theorem provers.  But for that to really work, we have to trust the automated tools.</p>",
        "id": 539531954,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936147
    },
    {
        "content": "<p>Of course, the best test is human understanding.  In this particular case, the strong PNT is a well-known fact that is 100 years old.  So even if this proof was wrong, we wouldn't doubt the original theorem.  Also, the original Lean PNT project plans to clean up and incorporate this strong PNT proof into their project.  That will provide a lot of human scrutiny.  But as this sort of thing becomes more common, human review will be a bottleneck.</p>",
        "id": 539531966,
        "sender_full_name": "Jason Rute",
        "timestamp": 1757936150
    },
    {
        "content": "<p>I’ve wondered a lot about this, since our group is gearing up to do a large-scale autoformalization in the science domain. Here, potential semantic issues are way more prominent. Formalizing science will inevitably involve many custom definitions outside of Mathlib, as well as involve probing a domain where the literature is much less precise than mathematics.</p>",
        "id": 539576002,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1757946817
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"424214\">Tyler Josephson ⚛️</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Checking.20large.20AI.20generated.20projects/near/539576002\">said</a>:</p>\n<blockquote>\n<p>I’ve wondered a lot about this, since our group is gearing up to do a large-scale autoformalization in the science domain. Here, potential semantic issues are way more prominent. Formalizing science will inevitably involve many custom definitions outside of Mathlib, as well as involve probing a domain where the literature is much less precise than mathematics.</p>\n</blockquote>\n<p>For this I'd suggest starting with much simpler, more well-defined foundations (e.g. axiomized Everettian QM, Blanchet/Turaev TQFT axioms, Wightman QFT axioms, Haag Kastler QFT axioms...), then working \"upward\" from one or more of these.</p>",
        "id": 539584384,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1757948788
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> for starting the discussion. It looks like multi-file autoformalizations will become more common, as larger scale autoformalizatoins projects are being attempted. Currently SafeVerify is limited to single files; I think it is very doable to extend it to multiple files, but care need to be taken to the design and the technical implementation. </p>\n<p>I'm not too worried about this particular proof; Morph lab's previous project (abc almost always true) passed SafeVerify, and this one is most likely correct too. But it is worthwhile to have a SafeVerify that can check multiple files. If anyone wants to collaborate on a PR, (or help reviewing PRs), please let me know!</p>\n<p>In particular, here's a question about how lean4checker deals with multiple files that perhaps <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> and <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> know the answer: right now lean4checker could process multiple modules in parallel,  or (with the <code>--fresh</code> option) replay multiple modules into a single environment. My guess is that the latter is most likely needed, if the goal is to check the validity of a single theorem (that depends on the other files)? But I don't have a deep enough understanding of the details of Environment.replay to be sure.</p>",
        "id": 539613803,
        "sender_full_name": "GasStationManager",
        "timestamp": 1757956999
    },
    {
        "content": "<p>Sorry, I didn't understand what the question is here.</p>",
        "id": 539913055,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1758070782
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> I don’t know the specifics, but the larger context is that there is a Lean program, SafeVerify (<a href=\"https://github.com/GasStationManager/SafeVerify\">https://github.com/GasStationManager/SafeVerify</a>), which can be used to check Lean code.  It uses the environment replay from lean4checker I think. The goal of the project is to check that the kernel proof of a theorem is correct, that the axioms are just the usual three, and that the term of the theorem statement is the same as the reference version (which just has a sorry proof an no lemmas, etc).  It’s the best thing we currently have for checking AI proofs.  One thing <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> pointed out is that while he thought SafeVerify’s code looked good and secure, it will just trust any imports.  This means in multi file projects, that it won’t replay the proof of the imports (even if they are new imports not found in the reference file).  <span class=\"user-mention\" data-user-id=\"776090\">@GasStationManager</span>’s technical question is about  how to extend his checker to multiple projects imports where one also needs to replay some of the imports.</p>",
        "id": 539947915,
        "sender_full_name": "Jason Rute",
        "timestamp": 1758094849
    },
    {
        "content": "<p>Maybe it is just enough to do both lean4checker and SafeVerify.  The former will check all the term proofs, not just those in the final file.  Then SafeVerify will check the other things that lean4checker is missing (like axioms, and that the final theorem statement matches the reference version).</p>",
        "id": 539949046,
        "sender_full_name": "Jason Rute",
        "timestamp": 1758095257
    },
    {
        "content": "<p>Is there a correctness gap here I’m missing?  (This will require any new definitions needed for the final statement to go in the reference file.)</p>",
        "id": 539949207,
        "sender_full_name": "Jason Rute",
        "timestamp": 1758095317
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Checking.20large.20AI.20generated.20projects/near/539613803\">said</a>:</p>\n<blockquote>\n<p>If anyone wants to collaborate on a PR, (or help reviewing PRs), please let me know!</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"776090\">@GasStationManager</span> I'd be interested in helping out with this!</p>",
        "id": 540002315,
        "sender_full_name": "Paul Lezeau",
        "timestamp": 1758112186
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Checking.20large.20AI.20generated.20projects/near/539613803\">said</a>:</p>\n<blockquote>\n<p>In particular, here's a question about how lean4checker deals with multiple files that perhaps <span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> and <span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> know the answer: right now lean4checker could process multiple modules in parallel, or (with the <code>--fresh</code> option) replay multiple modules into a single environment. My guess is that the latter is most likely needed, if the goal is to check the validity of a single theorem (that depends on the other files)? But I don't have a deep enough understanding of the details of Environment.replay to be sure.</p>\n</blockquote>\n<p>The specification of the single-module version of lean4lean / lean4checker is that assuming the imports are correct, the output is also correct. That means you can run it in parallel in order to prove that all modules are correct, and the proofs compose. If you want to check a multi-file project you just need to make sure each module individually is validated, down to imports that you have previously checked.</p>",
        "id": 540122820,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1758155076
    },
    {
        "content": "<p>Ok, this sounds like what we as a community need to do is as follows:</p>\n<ul>\n<li>trace the main theorem and its definitions</li>\n<li>generate a template project with only those theorems/definitions (with any proofs sorried)</li>\n<li>manually inspect the template project</li>\n<li>run lean4checker on the full final project</li>\n<li>run SafeVerify or similar to compare each template file with the final version.</li>\n</ul>",
        "id": 540191159,
        "sender_full_name": "Jason Rute",
        "timestamp": 1758188567
    },
    {
        "content": "<p>Is there any repo for me to contribute?</p>",
        "id": 541313084,
        "sender_full_name": "AYUSH DEBNATH",
        "timestamp": 1758742105
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Checking.20large.20AI.20generated.20projects/near/539531672\">said</a>:</p>\n<blockquote>\n<p>The recent proof by Gauss of the Strong PNT brings up an important question, one that I briefly discussed with <span class=\"user-mention silent\" data-user-id=\"635243\">Talia Ringer</span> on X (<a href=\"https://x.com/TaliaRinger/status/1967148801862533359\">https://x.com/TaliaRinger/status/1967148801862533359</a>).</p>\n</blockquote>\n<p>Good read:  <a href=\"https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity\">https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity</a></p>\n<p>That said, there is an opportunity with lean versus other domains because it's about verifiable correctness.  The statement formalization problem and lean exploits ofc are at risk of workslop.</p>",
        "id": 541357057,
        "sender_full_name": "Deleted User 968128",
        "timestamp": 1758766467
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"965736\">AYUSH DEBNATH</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Checking.20large.20AI.20generated.20projects/near/541313084\">said</a>:</p>\n<blockquote>\n<p>Is there any repo for me to contribute?</p>\n</blockquote>\n<p>Mathlib. The one library to contribute to.</p>",
        "id": 541362127,
        "sender_full_name": "(deleted)",
        "timestamp": 1758770652
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"965736\">AYUSH DEBNATH</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Checking.20large.20AI.20generated.20projects/near/541313084\">said</a>:</p>\n<blockquote>\n<p>Is there any repo for me to contribute?</p>\n</blockquote>\n<p>Depends on what you want to contribute.</p>",
        "id": 543149236,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1759655516
    }
]