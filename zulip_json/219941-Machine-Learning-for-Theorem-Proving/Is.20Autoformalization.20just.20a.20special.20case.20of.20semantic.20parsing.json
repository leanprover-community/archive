[
    {
        "content": "<p>I've seen papers like <a href=\"https://arxiv.org/pdf/2205.12615.pdf\">https://arxiv.org/pdf/2205.12615.pdf</a> trying to use LLMs for \"automatically translating from natural language mathematics to a formal language\". Although it's exciting to see LLMs can do these things to a certain degree, I'm curious that whether this is actually a special case of semantic parsing. Semantic parsing has been studied for quite a long time in the setting where human interacts with computer through natural languages and I believe LLMs are not absolutely the best for every problems in semantic parsing. It would be great if anyone can share their opinions about this topic.</p>",
        "id": 432631129,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712813670
    },
    {
        "content": "<p>Semantic parsing is considered dead in NLP (at least one of my friends says so). However, I believe ideas in the field will be very helpful in autoformalizing maths.<br>\nWith autoformalization, one can gain access to a much larger amount of data than just hand coding lean. And in the future, people might be able to formalize math without knowing lean at all, with a model for translating informal math statements into formal ones and a model for autocompleting the missing trivial steps. If too many steps are missing, the computer can ask the person to give more details. So the future of formalization could be 99% done in natural languages.</p>",
        "id": 432632983,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712815121
    },
    {
        "content": "<p>When I do lean, the constant pain is to remember how the rules are named in the mathlib. With more math being formalized, it's going to be more painful. So a natural language interface will be more and more desirable.</p>",
        "id": 432633130,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712815210
    },
    {
        "content": "<p>I thought I recall that the best current semantic parsers are now also language models.</p>",
        "id": 432702856,
        "sender_full_name": "Jason Rute",
        "timestamp": 1712841049
    },
    {
        "content": "<p>Nonetheless, I think I certainly see value in a tighter connection between Lean (or whatever the system is) and the machine learning model.  <a href=\"https://github.com/siddhartha-gadgil/LeanAide\">LeanAide</a> is a nice example of this.</p>",
        "id": 432702864,
        "sender_full_name": "Jason Rute",
        "timestamp": 1712841052
    },
    {
        "content": "<p>I would say LLM is a necessary part of semantic parsers, but I doubt that it should be the only part.</p>\n<p>Given enough data and sufficient compute, llms can be scaled to do literally doing anything.</p>\n<p>But for AI math in particular, there could be cheaper plans I guess.</p>\n<p>But anyway, I feel like that just prompting ChatGPT should be easily surpassed by more domain specific methods. Let me try to see if this will work.</p>",
        "id": 432736368,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712850359
    },
    {
        "content": "<p>The current semantic parser tasks are kindof different from autoformalization. \"SQL queries (Zhong et al., 2017), robotic commands (Artzi and Zettlemoyer, 2013), smart phone instructions (Quirk et al., 2015), and even general-purpose programming languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java\" These are not the same as translating the stack project to Lean.</p>\n<p>In mathematics, the subset of natural language we use is much smaller and ChatGPT can help translate user input into more unambiguous and commonly accepted form. But I wouldn't say this for things like robotic commands and SQL queries, where the input could be really wild.</p>",
        "id": 432737219,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712850612
    },
    {
        "content": "<p>Also there is a problem of verifiability. ChatGPT's output is not verifiable. User has to know lean to verify the correctness.</p>",
        "id": 432737516,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712850689
    },
    {
        "content": "<p>If the procedure has to convert \"there exists a real number x such that ...\" to <code>∃ x : ℝ, ...</code>, isn't the step where it selected <code>ℝ</code> as the type of real numbers doing something beyond parsing? It feels more like definition lookup than parsing, since it depends on the ambient mathematical library.</p>",
        "id": 432741917,
        "sender_full_name": "Timo Carlin-Burns",
        "timestamp": 1712851920
    },
    {
        "content": "<p>It would be nice if things can be separated into two steps, parsing and library lookup</p>",
        "id": 432752593,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712855523
    },
    {
        "content": "<p>And another point is, parsing is taking an input and giving out an output. The output has to be in some format, so unavoidable it has to do some kind of lookup. We can use a commonly accepted logical formal language. Maybe Lean4 is changing too fast to be that kind of representation.</p>\n<p>Personally I would suggest using annotated controlled natural language and then do some lookup or even more advanced handling to translate to Lean4</p>",
        "id": 432753151,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712855730
    },
    {
        "content": "<p>Actually in that particular example, <code>ℝ</code> is notation that happens to be defined as <code>Real</code>, so if you include the <code>ℝ</code> syntax in your notion of the target language, you could say that the translation to <code>∃ x : ℝ, ...</code> is independent of mathematical library, since each library could each define the notation however it wanted</p>",
        "id": 432753435,
        "sender_full_name": "Timo Carlin-Burns",
        "timestamp": 1712855852
    },
    {
        "content": "<p>I believe there could be a \"universal semantic parsing\", such that autoformalization can be factored into the composition of this \"universal semantic parsing\" with other simple things like lookup, so that library development can be independently carried out.</p>",
        "id": 432753684,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712855957
    },
    {
        "content": "<p>I don't believe there is any intractable difficulty in autoformalization. And I believe if it's solved, then it will enhance Lean's experience dramatically.</p>",
        "id": 432753979,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712856055
    },
    {
        "content": "<p>The difficulty of mathematics is mostly about coming up with the right proof path. Formalizing it wouldn't be that nontrivial. I expect we could come up with much easier tools that basically doesn't require user to know anything but mathematics itself.</p>",
        "id": 432754210,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712856149
    },
    {
        "content": "<p>The existing tools like ChatGPT+Lean is far from perfect. From my point of view, this is quite a good opportunity actually. The problem is not intractable hard and it's a good chance to show that openai isn't going to win in every battlefield.</p>",
        "id": 432754357,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712856228
    },
    {
        "content": "<p>If your \"universal semantic parsing\" includes <code>ℝ</code> in its target language, won't it need to include every new mathematical structure as well? It seems like then it would cease to be universal shortly after being released</p>",
        "id": 432754485,
        "sender_full_name": "Timo Carlin-Burns",
        "timestamp": 1712856272
    },
    {
        "content": "<p>That's why it's quoted.</p>",
        "id": 432754610,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712856331
    },
    {
        "content": "<p>We could fix a goal, like proving Fermat's Last Theorem, and only care about related notions. Math is too developed to be tacked in one attempt.</p>",
        "id": 432754726,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1712856369
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"466120\">@Xiyu Zhai</span>  I agree with you that autoformalization is essentially a type of semantic parsing. In our recent <a href=\"https://arxiv.org/abs/2403.13312\">paper</a>, we're converting 'natural language logical reasoning data' (a more traditional NLP task) into 'theorems in Lean,' which feels very much like semantic parsing</p>",
        "id": 433365209,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1713207847
    },
    {
        "content": "<p>Great work, congrats! I'm going to develop a systematic approach with little human effort for autoformalizing Michael Atiyah's commutative algebra. Hope you guys don't scoop me.</p>",
        "id": 433416930,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1713229673
    },
    {
        "content": "<p>There have been various earlier approaches combining theorem proving with NLP, e.g.<br>\n<a href=\"https://www.grammaticalframework.org/\">https://www.grammaticalframework.org/</a> (developed from agda)<br>\nNaproche<br>\nMOWGLI project: <a href=\"http://mowgli.cs.unibo.it/library/\">http://mowgli.cs.unibo.it/library/</a><br>\nGanesalingam and Gowers, e.g. <a href=\"https://link.springer.com/book/10.1007/978-3-642-37012-0\">https://link.springer.com/book/10.1007/978-3-642-37012-0</a></p>",
        "id": 453661010,
        "sender_full_name": "Bas Spitters",
        "timestamp": 1721818432
    }
]