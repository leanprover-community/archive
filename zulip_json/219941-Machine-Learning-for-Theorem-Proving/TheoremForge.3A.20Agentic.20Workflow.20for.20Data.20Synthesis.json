[
    {
        "content": "<p>We release <a href=\"https://github.com/timechess/TheoremForge\">TheroemForge</a>, an agentic workflow serving as a formal data synthesis pipeline. It is designed to collect training data for expert models in statement formalization, proof generation, premise selection, proof correction and proof sketching with low budget. See our <a href=\"https://arxiv.org/abs/2601.17332\">tech report</a> for details.</p>\n<p>Specifically, TheoremForge adopts iterative proof refinement and subgoal decomposition strategies inspired by <a href=\"https://arxiv.org/abs/2509.22819\">Hilbert</a>. A key difference is that, for cost control, it refrains from recursively decomposing subgoals.</p>\n<p>Our work represents only a primary exploration of solutions to the scarcity of training data for subtasks in the formalization domain. Experimental results indicate that current general-purpose LLMs, such as Gemini, GPT-5, and Claude, still achieve relatively low performance in formalization. Although increasing the inference budget can mitigate this issue, such an approach is not economically sustainable in the long term.</p>\n<p>We look forward to your feedback and hope that our work will provide some useful insights.</p>",
        "id": 570275621,
        "sender_full_name": "Yicheng Tao",
        "timestamp": 1769507349
    },
    {
        "content": "<p>Hello! I noticed in the technical report the statement: \"We rely on LLM-based extraction rather than the extract_goals tactic, as the latter has proven unreliable in certain contexts\" (Subgoal Decomposition in Appendix A.3) Could you provide some examples or statistical results comparing the accuracy of the LLM-based method and the extract_goals tactic in extracting lemmas? I am also working on generating sketches, so I am quite curious about this.</p>",
        "id": 570326694,
        "sender_full_name": "Yutong Wu",
        "timestamp": 1769522173
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"954234\">Yutong Wu</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/TheoremForge.3A.20Agentic.20Workflow.20for.20Data.20Synthesis/near/570326694\">said</a>:</p>\n<blockquote>\n<p>Hello! I noticed in the technical report the statement: \"We rely on LLM-based extraction rather than the extract_goals tactic, as the latter has proven unreliable in certain contexts\" (Subgoal Decomposition in Appendix A.3) Could you provide some examples or statistical results comparing the accuracy of the LLM-based method and the extract_goals tactic in extracting lemmas? I am also working on generating sketches, so I am quite curious about this.</p>\n</blockquote>\n<p>Thanks for your attention. I should say that I didn't keep that example but sometimes the lemmas <code>extract_goal</code> created will lose all the context, only preserve the name and the goal. I haven't figured out what caused the problem. The <a href=\"https://leanprover-community.github.io/mathlib4_docs/Mathlib/Tactic/ExtractGoal.html\">document</a> has warnings about potential failures.</p>\n<p>BTW, I found <a href=\"https://github.com/KellyJDavis/goedels-poetry\">goedels-poetry</a> takes a syntactical approach by rewriting the Kimina Lean Server. Maybe it will help.</p>",
        "id": 570340108,
        "sender_full_name": "Yicheng Tao",
        "timestamp": 1769525324
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"646276\">Yicheng Tao</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/TheoremForge.3A.20Agentic.20Workflow.20for.20Data.20Synthesis/near/570340108\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"954234\">Yutong Wu</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/TheoremForge.3A.20Agentic.20Workflow.20for.20Data.20Synthesis/near/570326694\">said</a>:</p>\n<blockquote>\n<p>Hello! I noticed in the technical report the statement: \"We rely on LLM-based extraction rather than the extract_goals tactic, as the latter has proven unreliable in certain contexts\" (Subgoal Decomposition in Appendix A.3) Could you provide some examples or statistical results comparing the accuracy of the LLM-based method and the extract_goals tactic in extracting lemmas? I am also working on generating sketches, so I am quite curious about this.</p>\n</blockquote>\n<p>Thanks for your attention. I should say that I didn't keep that example but sometimes the lemmas <code>extract_goal</code> created will lose all the context, only preserve the name and the goal. I haven't figured out what caused the problem. The <a href=\"https://leanprover-community.github.io/mathlib4_docs/Mathlib/Tactic/ExtractGoal.html\">document</a> has warnings about potential failures.</p>\n<p>BTW, I found <a href=\"https://github.com/KellyJDavis/goedels-poetry\">goedels-poetry</a> takes a syntactical approach by rewriting the Kimina Lean Server. Maybe it will help.</p>\n</blockquote>\n<p>Per the linked documentation, it sounds like <code>extract_goal *</code>should solve your question.</p>",
        "id": 570435710,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1769553197
    },
    {
        "content": "<p>Thanks for the LeanExplore citation! </p>\n<p>Just as a heads-up, I am in the process of updating it—the new search engine will be much better—but the API will be different. (I will try not to do breaking changes again going forward.)</p>",
        "id": 570445799,
        "sender_full_name": "Justin Asher",
        "timestamp": 1769558902
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"631691\">Thomas Zhu</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/TheoremForge.3A.20Agentic.20Workflow.20for.20Data.20Synthesis/near/570435710\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"646276\">Yicheng Tao</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/TheoremForge.3A.20Agentic.20Workflow.20for.20Data.20Synthesis/near/570340108\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"954234\">Yutong Wu</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/TheoremForge.3A.20Agentic.20Workflow.20for.20Data.20Synthesis/near/570326694\">said</a>:</p>\n<blockquote>\n<p>Hello! I noticed in the technical report the statement: \"We rely on LLM-based extraction rather than the extract_goals tactic, as the latter has proven unreliable in certain contexts\" (Subgoal Decomposition in Appendix A.3) Could you provide some examples or statistical results comparing the accuracy of the LLM-based method and the extract_goals tactic in extracting lemmas? I am also working on generating sketches, so I am quite curious about this.</p>\n</blockquote>\n<p>Thanks for your attention. I should say that I didn't keep that example but sometimes the lemmas <code>extract_goal</code> created will lose all the context, only preserve the name and the goal. I haven't figured out what caused the problem. The <a href=\"https://leanprover-community.github.io/mathlib4_docs/Mathlib/Tactic/ExtractGoal.html\">document</a> has warnings about potential failures.</p>\n<p>BTW, I found <a href=\"https://github.com/KellyJDavis/goedels-poetry\">goedels-poetry</a> takes a syntactical approach by rewriting the Kimina Lean Server. Maybe it will help.</p>\n</blockquote>\n<p>Per the linked documentation, it sounds like <code>extract_goal *</code>should solve your question.</p>\n</blockquote>\n<p>Since I have lost the failed case, I'm not sure whether it will work. However, the document says only irrelevant variables will be removed. I'm sure in that failed case there are variables that satisfy the relevant conditions. But all the <code>sorry</code>s in the main theorem produce no context. </p>\n<p>Maybe I will find the case in the future. I will add an option to use <code>extract_goal *</code> to extract subgoals later.</p>",
        "id": 570462761,
        "sender_full_name": "Yicheng Tao",
        "timestamp": 1769571316
    },
    {
        "content": "<p>I found that <code>extract_goal</code> can introduce ambiguity when dealing with pretty printed objects (as already noted in the documentation), as shown in the following example:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Aesop</span>\n\n<span class=\"kn\">set_option</span><span class=\"w\"> </span><span class=\"n\">maxHeartbeats</span><span class=\"w\"> </span><span class=\"mi\">0</span>\n\n<span class=\"kn\">open</span><span class=\"w\"> </span><span class=\"n\">BigOperators</span><span class=\"w\"> </span><span class=\"n\">Real</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"n\">Topology</span><span class=\"w\"> </span><span class=\"n\">Rat</span>\n\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">mathd_algebra_153</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">h₀</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span>\n<span class=\"w\">    </span><span class=\"n\">Int</span><span class=\"bp\">.</span><span class=\"n\">floor</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">Int</span><span class=\"bp\">.</span><span class=\"n\">floor</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">100</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">Int</span><span class=\"bp\">.</span><span class=\"n\">floor</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">1000</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">Int</span><span class=\"bp\">.</span><span class=\"n\">floor</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">10000</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">3702</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n\n<span class=\"w\">  </span><span class=\"k\">have</span><span class=\"w\"> </span><span class=\"n\">Step5</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Int</span><span class=\"bp\">.</span><span class=\"n\">floor</span><span class=\"w\"> </span><span class=\"o\">((</span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"bp\">/</span><span class=\"mi\">3</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℤ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n<span class=\"w\">    </span><span class=\"n\">extract_goal</span><span class=\"w\"> </span><span class=\"bp\">*</span>\n<span class=\"w\">    </span><span class=\"gr\">sorry</span>\n</code></pre></div>\n<p>It shows in InfoView:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">mathd_algebra_153</span><span class=\"bp\">.</span><span class=\"n\">extracted_1_1</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">h₀</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">⌊</span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"bp\">/</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"bp\">⌋</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"gr\">sorry</span>\n</code></pre></div>\n<p>But if we just copy the code as a new lemma, it will cause an error. Alternatively, we can use <code>set_option pp.all true in extract_goal</code> to keep <code>Int.floor</code> unchanged, but it will make the lemma extremely verbose and unreadable. I think using LLMs to extract lemmas is indeed necessary, as it is difficult to achieve this with rule-based methods.  Does anyone have any thoughts on this issue?</p>",
        "id": 570728017,
        "sender_full_name": "Yutong Wu",
        "timestamp": 1769674370
    }
]