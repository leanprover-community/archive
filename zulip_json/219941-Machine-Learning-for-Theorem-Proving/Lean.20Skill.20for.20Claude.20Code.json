[
    {
        "content": "<p>In case it's useful to anyone else, I thought I'd share this Claude Skill that I've been finding useful for formalizing math in Lean using Claude Code (with Sonnet 4.5):<br>\n<a href=\"https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-theorem-proving\">https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-theorem-proving</a><br>\n[updated link as the repo/marketplace now has two skills]</p>\n<p>I've found that it generally helps, by reducing Claude's tendency to declare victory too soon (without recompiling to confirm) or to think that it's done even when it has introduced axioms or still has sorries remaining. It also encourages incremental well-documented progress on one thing at a time (rather than piecemeal on many things), reminds it to first look in mathlib before trying to prove something on its own, etc.</p>",
        "id": 545777155,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760815152
    },
    {
        "content": "<p>How powerful is this compared to GPT-5</p>",
        "id": 545788890,
        "sender_full_name": "(deleted)",
        "timestamp": 1760830561
    },
    {
        "content": "<p>And cost efficiency too <span aria-label=\"eyes\" class=\"emoji emoji-1f440\" role=\"img\" title=\"eyes\">:eyes:</span> Even though I have a ChatGPT Pro plan I use up the weekly limit pretty quickly</p>",
        "id": 545789286,
        "sender_full_name": "(deleted)",
        "timestamp": 1760831153
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"511228\">@Huá»³nh Tráº§n Khanh</span> I've found the GPT-5-Codex model (via Codex) somewhat better for planning proof structure and GPT-5 Pro the best for getting unstuck on tricky issues, but this Claude setup (Sonnet 4.5 via Claude Code with this skill) by far the best for filling in proof details, fixing errors, looking up mathlib results, and refactoring existing Lean code.</p>",
        "id": 545790517,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760832820
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 545827429,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760877708
    },
    {
        "content": "<p>Honestly it's fascinating to see how quickly AI models become so good at Lean</p>",
        "id": 545827644,
        "sender_full_name": "(deleted)",
        "timestamp": 1760877918
    },
    {
        "content": "<p>To the point where in practice I don't see any gap between informal and formal math capabilities of LLMs</p>",
        "id": 545827672,
        "sender_full_name": "(deleted)",
        "timestamp": 1760877947
    },
    {
        "content": "<p>It's a solved problem at this point</p>",
        "id": 545827856,
        "sender_full_name": "(deleted)",
        "timestamp": 1760878163
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 545828433,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760878716
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> I created it by asking Claude Code to write it based on patterns that seemed to have worked well in ~1000 commits of my repo containing mostly Claude Code development in Lean. It made use of the various Anthropic documentation for writing skills.</p>\n<p>I've also been using <code>superpowers</code>, which has this skill for writing skills: <a href=\"https://github.com/obra/superpowers/blob/7fc125e5e9d864e082de89e3f72ffa4b95d5cc92/skills/writing-skills/SKILL.md\">https://github.com/obra/superpowers/blob/7fc125e5e9d864e082de89e3f72ffa4b95d5cc92/skills/writing-skills/SKILL.md</a></p>\n<p>I'm in the process of improving it using the <code>skill-creator</code> skill of <a href=\"https://github.com/anthropics/skills\">https://github.com/anthropics/skills</a></p>",
        "id": 545831983,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760882644
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116846\">@Cameron Freer</span> </p>\n<p>For those that work with prompts that are an amalgamation of other prompts, as in this case the with Claude Skills (<a href=\"https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\">Progressive disclosure</a>), do you see the effectiveness of a conversation with skills diminishing as the conversation grows? </p>\n<p>The reasoning behind the question is that many AI Chat bots will summarize the info from the starting  prompts, user prompt and create the first reply. Then summarize the initial prompt and reply along with the next user prompt as the actual prompt sent for the LLM completion. As such as the conversation continues, the generated portion of the prompt looses the initial instructions and thus one needs to start an entire new conversation. Wondering if using skills has either reduced or remedied the problem. Did not see such noted in the documentation but still reading.</p>",
        "id": 545834467,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760885081
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> <br>\nMy understanding is that once a skill is invoked, the metadata (title/description) of the skill remain in context, so that it remembers to reread sections of the skill as needed.</p>\n<p>I think the progressive disclosure of the skill itself means that individual instructions only load incrementally:<br>\n<a href=\"https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview#how-skills-work\">https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview#how-skills-work</a></p>\n<p>It's also efficient about loading from hierarchies as needed, and about running scripts without loading them into context:<br>\n<a href=\"https://docs.claude.com/en/docs/agents-and-tools/agent-skills/best-practices#progressive-disclosure-patterns\">https://docs.claude.com/en/docs/agents-and-tools/agent-skills/best-practices#progressive-disclosure-patterns</a></p>\n<p>I've restructured the Lean Skill to make better use of these patterns:<br>\n<a href=\"https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-theorem-proving/skills/lean4-theorem-proving/references\">https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-theorem-proving/skills/lean4-theorem-proving/references</a><br>\n<a href=\"https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-theorem-proving/skills/lean4-theorem-proving/scripts\">https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-theorem-proving/skills/lean4-theorem-proving/scripts</a><br>\n[updated links as the repo/marketplace now has two skills, each in a separate plugin]</p>",
        "id": 545835279,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760885753
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"116846\">Cameron Freer</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.20Skill.20for.20Claude.20Code/near/545835279\">said</a>:</p>\n<blockquote>\n<p>I think the progressive disclosure of the skill itself means that individual instructions only load incrementally</p>\n</blockquote>\n<p>Yes, correct.</p>",
        "id": 545836110,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760886388
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"657719\">@Terence Tao</span> </p>\n<p>Terry created a Google document that you or others might find of value in incorporating into a Claude Lean skill.</p>\n<p>Not sure if the list is still maintained.</p>\n<hr>\n<p><a href=\"https://mathstodon.xyz/@tao/111360298114925842#:~:text=Google%20DocsLean%20phrasebookPhrasebook,X%20is%20equal%20to%20\">https://mathstodon.xyz/@tao/111360298114925842#:~:text=Google%20DocsLean%20phrasebookPhrasebook,X%20is%20equal%20to%20</a>...</p>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1Gsn5al4hlpNc_xKoXdU6XGmMyLiX4q-LFesFVsMlANo/edit?pli=1&amp;gid=0#gid=0\">https://docs.google.com/spreadsheets/d/1Gsn5al4hlpNc_xKoXdU6XGmMyLiX4q-LFesFVsMlANo/edit?pli=1&amp;gid=0#gid=0</a></p>",
        "id": 545836432,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760886630
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116846\">@Cameron Freer</span> </p>\n<p>Hope you don't mind all of these replies, what you are doing is a good thing! </p>\n<p>In creating <a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\">MCPs</a>, learned of adding memory, e.g.</p>\n<p><a href=\"https://github.com/modelcontextprotocol/servers/tree/main/src/memory\">https://github.com/modelcontextprotocol/servers/tree/main/src/memory</a></p>\n<p>Memory might be of value with skills. I know that with memory, with my other LLM chat bots had to turn off the feature because the memory was not segregated for when it was needed, with skills this might work as needed.</p>",
        "id": 545838432,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760888493
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> no worries at all -- thanks for all the feedback and interesting ideas!</p>\n<p>I've made an experimental skill that uses memory over MCP, indexed by project/file, as you suggested. It seems to be working, though I'm not yet sure how much (or whether) it is helping:<br>\n<a href=\"https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-memories\">https://github.com/cameronfreer/lean4-skills/tree/main/plugins/lean4-memories</a></p>\n<p>(I've renamed the repo/marketplace as it now contains two plugins with separate skills -- the main Lean one is independent of the memory one)</p>\n<p>[edited link to reflect new directory structure]</p>",
        "id": 545869801,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760923113
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> also I've added a reference that the skill can load as needed, based largely on <span class=\"user-mention\" data-user-id=\"657719\">@Terence Tao</span>'s phrasebook -- thanks for the suggestion!</p>\n<p><a href=\"https://github.com/cameronfreer/lean4-skills/blob/main/plugins/lean4-theorem-proving/skills/lean4-theorem-proving/references/lean-phrasebook.md\">https://github.com/cameronfreer/lean4-skills/blob/main/plugins/lean4-theorem-proving/skills/lean4-theorem-proving/references/lean-phrasebook.md</a></p>\n<p>[edited link to reflect new directory structure]</p>",
        "id": 545870717,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760924105
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 546073489,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760987883
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> thanks for these details. I'll update the README installation instructions and see if I can make it any easier for Windows. (I already made the various scripts compatible with Bash 3.2 since many Macs will have this older version.)</p>",
        "id": 546075354,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1760988512
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 546093650,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1760993526
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116846\">@Cameron Freer</span> Thanks very much for creating this!  I found using claude code + skills a very nice way to use AI for doing things besides closing sorries.  I post this elsewhere: <a class=\"message-link\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Claude.20Code/near/546873213\">#Machine Learning for Theorem Proving &gt; Claude Code @ ðŸ’¬</a></p>",
        "id": 546874901,
        "sender_full_name": "Nehal Patel",
        "timestamp": 1761305887
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"758965\">Nehal Patel</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.20Skill.20for.20Claude.20Code/near/546874901\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"116846\">Cameron Freer</span> Thanks very much for creating this!  I found using claude code + skills a very nice way to use AI for doing things besides closing sorries.  I post this elsewhere: <a class=\"message-link\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Claude.20Code/near/546873213\">#Machine Learning for Theorem Proving &gt; Claude Code @ ðŸ’¬</a></p>\n</blockquote>\n<p>Do you find \"codex\" as an VS extension version worser than this set-up?</p>",
        "id": 546912303,
        "sender_full_name": "Olonbayar Temuulen",
        "timestamp": 1761316297
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> <span class=\"user-mention\" data-user-id=\"758965\">@Nehal Patel</span> <span class=\"user-mention\" data-user-id=\"116846\">@Cameron Freer</span></p>",
        "id": 546912424,
        "sender_full_name": "Olonbayar Temuulen",
        "timestamp": 1761316330
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"494654\">@Olonbayar Temuulen</span> I've personally found the Codex extension helpful for structural planning of a big proof or library of results, and also GPT-5 with lots of thinking is good for getting unstuck on tricky mathematical or type issues. But I've found Claude Code with this skill much better and faster at writing proof details, fixing errors, filling sorries, and refactoring large proofs.</p>",
        "id": 546916823,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1761317499
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 546929763,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1761321244
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 546947325,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1761326663
    },
    {
        "content": "<p>I've also learned a bunch by asking Claude to elaborate on some of its thoughts -- here are two examples:</p>\n<p><a href=\"/user_uploads/3121/icEvJHkN1zda1kvalUoKo7Xq/image-18.png\">image-18.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/icEvJHkN1zda1kvalUoKo7Xq/image-18.png\" title=\"image-18.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"3132x2240\" src=\"/user_uploads/thumbnail/3121/icEvJHkN1zda1kvalUoKo7Xq/image-18.png/840x560.webp\"></a></div><p><a href=\"/user_uploads/3121/aEPxDUXf1BW251PUpp82Vd2N/image-16.png\">image-16.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/aEPxDUXf1BW251PUpp82Vd2N/image-16.png\" title=\"image-16.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"3114x1962\" src=\"/user_uploads/thumbnail/3121/aEPxDUXf1BW251PUpp82Vd2N/image-16.png/840x560.webp\"></a></div>",
        "id": 547004396,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1761342897
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"366057\">@Eric Taucher</span> fyi, codex does also save a conversation history<br>\nAnd i would echo <span class=\"user-mention\" data-user-id=\"116846\">@Cameron Freer</span> comments  -- codex seems to be a very good mathematician, but the claude-code cli seems effective in moving stuff along.  And the web chat interfaces for gpt-5-pro with deep research, and the equivalents from Anthropic and Google all seem quite good at providing natural language scaffolding (so if possible tart with the powerful tools to create a scaffold for claude code to follow)</p>",
        "id": 547011308,
        "sender_full_name": "Nehal Patel",
        "timestamp": 1761348627
    },
    {
        "content": "<p>Originally I had started developing this Claude skill in the course of a project to formalize three proofs of de Finetti's theorem. I've recently finished this (announced today in <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/formalization.20of.20three.20proofs.20of.20de.20Finetti.27s.20theorem/with/569060248\">#general &gt; formalization of three proofs of de Finetti's theorem</a>).</p>\n<p>I'd be happy to discuss this process more here if people are interested, and I'd also be interested to hear of other things that people have been doing using the skill.</p>",
        "id": 569064849,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1768924343
    },
    {
        "content": "<p>A \"skill\" in this case - it is essentially a markdown file that writes in structured, natural language the criteria for success and failure?</p>",
        "id": 570783607,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1769690528
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"330967\">Wrenna Robson</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.20Skill.20for.20Claude.20Code/near/570783607\">said</a>:</p>\n<blockquote>\n<p>A \"skill\" in this case - it is essentially a markdown file that writes in structured, natural language the criteria for success and failure?</p>\n</blockquote>\n<p>Something like that, but also including patterns to follow and antipatterns to avoid, and with task-specific details (e.g., for golfing proofs that are already sorry-free, or guides to particular areas of mathlib), all organized hierarchically so that it can be read via \"progressive disclosure\".</p>",
        "id": 570802980,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1769695451
    },
    {
        "content": "<p>Thanks, that is really interesting - and interesting that it is so effective.</p>",
        "id": 570990412,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1769768690
    },
    {
        "content": "<p>What \"plan\" do you use Claude on for Lean work? I find the pricing structure somewhat confusing.</p>",
        "id": 570990489,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1769768710
    },
    {
        "content": "<p>I've been using Max 20x for a while, but Max 5x or Pro or tokens via an API key should also work fine for lower-volume work.</p>",
        "id": 571061094,
        "sender_full_name": "Cameron Freer",
        "timestamp": 1769787456
    }
]