[
    {
        "content": "<p>New preprint + repo: LeanCat (Part I: 1-categories) — 100 statement-level Lean 4 problems for abstraction-heavy, library-grounded category-theory reasoning.<br>\nPaper: <a href=\"https://arxiv.org/pdf/2512.24796\">https://arxiv.org/pdf/2512.24796</a>  Repo: <a href=\"https://github.com/sciencraft/LeanCat\">https://github.com/sciencraft/LeanCat</a><br>\nPinned toolchain: Lean 4.19.0 / mathlib 4.19.0; includes NL descriptions + metadata (Easy/Med/High = 20/42/38).<br>\nBaselines: best 8.25% pass@1 / 12.00% pass@4 (sharp drop on Medium/High).<br>\nFeedback welcome: (i) pointers to closely related work / similar benchmark efforts; (ii) best practices for evaluation + reproducibility (toolchain pinning, scripts, logging, etc.).</p>",
        "id": 566236667,
        "sender_full_name": "Rongge",
        "timestamp": 1767547697
    },
    {
        "content": "<p>Cross-post: LeanCat (Part I: 1-categories) — a Lean 4 benchmark of 100 statement-level category-theory tasks stressing abstraction + library navigation.<br>\nPaper: <a href=\"https://arxiv.org/pdf/2512.24796\">https://arxiv.org/pdf/2512.24796</a>  Repo: <a href=\"https://github.com/sciencraft/LeanCat\">https://github.com/sciencraft/LeanCat</a><br>\nPinned Lean 4.19.0 / mathlib 4.19.0; metadata includes topic clusters + difficulty tiers (20/42/38); best 8.25% pass@1 / 12.00% pass@4.<br>\nPointers welcome on robust eval/reproducibility protocols for tool-augmented proving (retrieval + compiler feedback); main thread: <a href=\"#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory\">LeanCat announcement</a>.</p>",
        "id": 566237769,
        "sender_full_name": "Rongge",
        "timestamp": 1767548877
    },
    {
        "content": "<p>It seems a little strange to release the first version of the benchmark at a toolchain that is 7 months old; do you plan to update it?</p>",
        "id": 566238175,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1767549248
    },
    {
        "content": "<p><a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566238175\">A message</a> was moved here from <a class=\"stream-topic\" data-stream-id=\"287929\" href=\"/#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory/with/566236667\">#mathlib4 &gt; LeanCat: a Lean 4 benchmark suite for category theory</a> by <span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span>.</p>",
        "id": 566238331,
        "sender_full_name": "Notification Bot",
        "timestamp": 1767549406
    },
    {
        "content": "<p>A message was moved from this topic to <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/with/566238331\">#Machine Learning for Theorem Proving &gt; LeanCat benchmark (category theory in Lean 4)</a> by <span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span>.</p>",
        "id": 566238332,
        "sender_full_name": "Notification Bot",
        "timestamp": 1767549406
    },
    {
        "content": "<p>(let's keep discussion in one place)</p>",
        "id": 566238342,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1767549420
    },
    {
        "content": "<p>Some questions:</p>\n<ul>\n<li>Are these new problems, or problems from Mathlib?  The first one is literally in mathlib <code>docs#CategoryTheory.NatTrans.id_comm</code>, so any model trained on mathlib or which can use Mathlib search tools should get it, right?</li>\n<li>The last problem maybe isn't in Mathlib as is, but are all the relevant parts in Mathlib?  Does one just need to do definition chasing and Mathlib search to find the answer?</li>\n<li>If these are not theorems from Mathlib, how do you know they are true and formalized correctly?  Did you formalize them by hand?  Did you have an extensive review process?  (For example, I'm a bit surprised that the third condition of the last problem (CAT_statement_S0100) doesn't depend on <code>F</code> or <code>adj</code>, but I don't know this area of math well, so maybe that is a well known fact.) </li>\n<li>What does this benchmark add that other benchmarks don't?  It seems easier than PutnamBench and MiniF2F, right?  (The fact that you didn't use any Lean-specific baselines make it a bit hard to compare.  And no LLM lean models use with pass@1.)</li>\n<li>Or is the goal of this project to make a benchmark with advanced math or more definition/diagram chasing?</li>\n<li>Do you think a SoTA lean model (anything which does well on PutnamBench) would struggle on this benchmark?  (My largest worry is that a SoTA model gets &gt;90% on this benchmark immediately.)</li>\n</ul>",
        "id": 566239854,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767550603
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566238175\">said</a>:</p>\n<blockquote>\n<p>It seems a little strange to release the first version of the benchmark at a toolchain that is 7 months old; do you plan to update it?</p>\n</blockquote>\n<p>Thanks for flagging this. We pinned Lean/mathlib 4.19 for reproducibility of the v1 results, and because the current eval harness is still tied to a LeanExplore-based retrieval + compiler-feedback loop on that toolchain. We'd like to support a more recent mathlib/Lean release as well, but I can't promise a timeline until we decouple/update the retrieval backend. If there's a community-preferred pattern here (e.g. frozen vs rolling), pointers would be very appreciated.</p>",
        "id": 566241282,
        "sender_full_name": "Rongge",
        "timestamp": 1767551836
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory/near/566239854\">said</a>:</p>\n<blockquote>\n<p>Some questions:</p>\n<ul>\n<li>Are these new problems, or problems from Mathlib?  The first one is literally in mathlib <code>docs#CategoryTheory.NatTrans.id_comm</code>, so any model trained on mathlib or which can use Mathlib search tools should get it, right?</li>\n<li>The last problem maybe isn't in Mathlib as is, but are all the relevant parts in Mathlib?  Does one just need to do definition chasing and Mathlib search to find the answer?</li>\n<li>If these are not theorems from Mathlib, how do you know they are true and formalized correctly?  Did you formalize them by hand?  Did you have an extensive review process?  (For example, I'm a bit surprised that the third condition of the last problem (CAT_statement_S0100) doesn't depend on <code>F</code> or <code>adj</code>, but I don't know this area of math well, so maybe that is a well known fact.) </li>\n<li>What does this benchmark add that other benchmarks don't?  It seems easier than PutnamBench and MiniF2F, right?  (The fact that you didn't use any Lean-specific baselines make it a bit hard to compare.  And no LLM lean models use with pass@1.)</li>\n<li>Or is the goal of this project to make a benchmark with advanced math or more definition/diagram chasing?</li>\n<li>Do you think a SoTA lean model (anything which does well on PutnamBench) would struggle on this benchmark?  (My largest worry is that a SoTA model gets &gt;90% on this benchmark immediately.)</li>\n</ul>\n</blockquote>\n<p>Fair questions — quick answers:</p>\n<p>• It’s a mix by design: a few Easy items are literally in mathlib (calibration for library navigation / interface familiarity); most aren’t “look up this theorem”.<br>\n• Even when ingredients exist in mathlib, the point is composing definitions/lemmas across abstractions (diagram/definition chasing in a big library), not contest-style tricks.<br>\n• For items not already in mathlib, we wrote the statements in Lean and had them sanity-checked by multiple category theorists; for harder ones we also attach sources/citations (see per-problem refs in <code>metadata.json</code>).<br>\n• LeanCat’s goal is different from miniF2F / PutnamBench: those are competition-flavored, while we target abstraction-heavy, library-grounded category theory.<br>\n• On “instant saturation”: current generic LLM baselines are far from it (~8% pass@1 / ~12% pass@4 overall; sharp drop on Medium/High).<br>\n• Happy to hear suggestions on stronger Lean-specific baselines and best practices for eval/reproducibility here.</p>",
        "id": 566242950,
        "sender_full_name": "Rongge",
        "timestamp": 1767553336
    },
    {
        "content": "<p>There is a mistake in problem 76: in <code>Nonempty ((Pro (FintypeCat)) ≃ Profinite)</code>, <code>≃</code> (bijection) must be replaced by <code>≌</code> (equivalence of categories).</p>",
        "id": 566243804,
        "sender_full_name": "Joël Riou",
        "timestamp": 1767554136
    },
    {
        "content": "<p>FYI: please reply in the discussion thread: <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/\">ML thread</a>. I’ll follow up there.</p>",
        "id": 566244040,
        "sender_full_name": "Rongge",
        "timestamp": 1767554338
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"459699\">Joël Riou</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566243804\">said</a>:</p>\n<blockquote>\n<p>There is a mistake in problem 76: in <code>Nonempty ((Pro (FintypeCat)) ≃ Profinite)</code>, <code>≃</code> (bijection) must be replaced by <code>≌</code> (equivalence of categories).</p>\n</blockquote>\n<p>Thanks a lot for catching this — you’re absolutely right. I’ll patch the statement + <code>metadata.json</code>; feel free to open an issue/PR if you prefer.</p>",
        "id": 566244434,
        "sender_full_name": "Rongge",
        "timestamp": 1767554740
    },
    {
        "content": "<p>Announcement/entry point: <a href=\"#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory\">mathlib4 post</a>. Let's keep discussion in this thread.</p>",
        "id": 566244468,
        "sender_full_name": "Rongge",
        "timestamp": 1767554777
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"310045\">@Eric Wieser</span> Sorry.  I posted in the wrong thread.  I thought it was the other one.  Can you move it?  (I can't move between channels.)</p>",
        "id": 566244561,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767554867
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"1005225\">Rongge</span> <a href=\"#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory/near/566242950\">said</a>:</p>\n<blockquote>\n<p>On “instant saturation”: current generic LLM baselines are far from it (~8% pass@1 / ~12% pass@4 overall; sharp drop on Medium/High).</p>\n</blockquote>\n<p>I think your baselines are very weak (even if the models used are strong models overall), including that you are using 1 to 4 model calls per problem which is rare in this field.  It would be nice to use some Lean-specific models.  It might be that the way these problems are stated, with many in-file <code>variable</code>s, <code>abbrev</code>s, and <code>def</code>s will also present novel challenges, especially to models that were trained explicitly on the very simple minif2f format, but that is unclear.</p>",
        "id": 566245380,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767555735
    },
    {
        "content": "<p>The question I'm trying to figure out if there is something inherently challenging mathematically or Lean-wise about this benchmark.  If any mathlib category theory experts are in this thread, do you think you would be pleasantly surprised if a tool filled in these <code>sorry</code>s?</p>",
        "id": 566245547,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767555913
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566245380\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"1005225\">Rongge</span> <a href=\"#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory/near/566242950\">said</a>:</p>\n<blockquote>\n<p>On “instant saturation”: current generic LLM baselines are far from it (~8% pass@1 / ~12% pass@4 overall; sharp drop on Medium/High).</p>\n</blockquote>\n<p>I think your baselines are very weak (even if the models used are strong models overall), including that you are using 1 to 4 model calls per problem which is rare in this field.  It would be nice to use some Lean-specific models.  It might be that the way these problems are stated, with many in-file <code>variable</code>s, <code>abbrev</code>s, and <code>def</code>s will also present novel challenges, especially to models that were trained explicitly on the very simple minif2f format, but that is unclear.</p>\n</blockquote>\n<p>Thanks Jason. For v1 we kept the baselines intentionally simple and reproducible: generate a full proof, then at most a small compiler-error repair loop (1–4 tries), not heavy search. I agree this isn’t the strongest Lean-specific setup, and it makes comparisons harder.</p>\n<p>We’d like to add a Lean-specific prover baseline (Aesop / LeanDojo-style provers like ReProver) once we have the engineering/compute to wire it up cleanly; those are usually reported as “% theorems proved under a fixed time/search budget” rather than a straight pass@1, so we’d also like to align the protocol/metrics.</p>\n<p>On “is it inherently challenging”: I wouldn’t be shocked if the Easy tier gets filled quickly with mathlib search. Where I’d be pleasantly surprised is strong, consistent performance on Medium/High — many items aren’t a single lookup, but require stitching interfaces/diagram chasing (and in a few cases we had to add missing glue lemmas). If there’s a standard baseline/config people expect here, pointers (or PRs) would be much appreciated.</p>",
        "id": 566246009,
        "sender_full_name": "Rongge",
        "timestamp": 1767556402
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"1005225\">Rongge</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566246009\">said</a>:</p>\n<blockquote>\n<p>We’d like to add a Lean-specific prover baseline (Aesop / LeanDojo-style provers like ReProver) once we have the engineering/compute to wire it up cleanly; those are usually reported as “% theorems proved under a fixed time/search budget” rather than a straight pass@1, so we’d also like to align the protocol/metrics.</p>\n</blockquote>\n<p>Aesop is just a tactic and LeanDojo's Reprover is really outdated (and never that high performing).  I would much rather see evaluations with Goedel-prover v2, DeepSeek-prover v2, or Kimina prover v2 (using them with their suggested prompts) with pass@32 or higher.  Or with Aristotle or Claude Code (with the Lean MCP) for a more realistic eval (since these are the two tools people actually use in practice, but that might be harder).</p>",
        "id": 566246994,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767557366
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"287929\" href=\"/#narrow/channel/287929-mathlib4/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory\">#mathlib4 &gt; LeanCat: a Lean 4 benchmark suite for category theory</a> by <span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span>.</p>",
        "id": 566253659,
        "sender_full_name": "Notification Bot",
        "timestamp": 1767564508
    },
    {
        "content": "<p>8 messages were moved here from <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.3A.20a.20Lean.204.20benchmark.20suite.20for.20category.20theory\">#Machine Learning for Theorem Proving &gt; LeanCat: a Lean 4 benchmark suite for category theory</a> by <span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span>.</p>",
        "id": 566253684,
        "sender_full_name": "Notification Bot",
        "timestamp": 1767564552
    },
    {
        "content": "<p>(<span class=\"user-mention\" data-user-id=\"1005225\">@Rongge</span> please don't crosspost)</p>",
        "id": 566253697,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1767564571
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566245547\">said</a>:</p>\n<blockquote>\n<p>do you think you would be pleasantly surprised if a tool filled in these <code>sorry</code>s?</p>\n</blockquote>\n<p>Dear Jason,</p>\n<p>Thank you for your question.<br>\nAs a supplementary to Rongge's answer, we’ve already completed most of the full proofs, in the LEAN language. For medium- and high-level problems, it is common that some additional “bridging” lemmas are needed to connect existing results in the library and close the proof. For some of the problems, completing the proof requires more than 1,000 lines of LEAN code. This reflects the benchmark’s difficulty and robustness.</p>\n<p>Hope readers will get something meaningful from our paper! ;-D</p>\n<p>Best,<br>\nJustin K. Wang<br>\nJan 5, 2025</p>",
        "id": 566258855,
        "sender_full_name": "Justin K. Wang",
        "timestamp": 1767571327
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"1010209\">@Justin K. Wang</span> .  That does help—both in understanding the difficulty and the effort you went through to ensure correctness.  (We will have to see how hard it is to fill in those 1000 lines.)</p>",
        "id": 566268311,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767581675
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566246994\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"1005225\">Rongge</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566246009\">said</a>:</p>\n<blockquote>\n<p>We’d like to add a Lean-specific prover baseline (Aesop / LeanDojo-style provers like ReProver) once we have the engineering/compute to wire it up cleanly; those are usually reported as “% theorems proved under a fixed time/search budget” rather than a straight pass@1, so we’d also like to align the protocol/metrics.</p>\n</blockquote>\n<p>Aesop is just a tactic and LeanDojo's Reprover is really outdated (and never that high performing).  I would much rather see evaluations with Goedel-prover v2, DeepSeek-prover v2, or Kimina prover v2 (using them with their suggested prompts) with pass@32 or higher.  Or with Aristotle or Claude Code (with the Lean MCP) for a more realistic eval (since these are the two tools people actually use in practice, but that might be harder).</p>\n</blockquote>\n<p>Quick clarification: I mentioned Aesop only as a cheap Lean-native <em>sanity-check baseline</em>, not “the main prover”.</p>\n<p>Agree the more interesting strong baselines are modern LLM provers (DeepSeek/Goedel/Kimina/…), probably at higher pass@k with their recommended prompts. v1 started from a minimal reproducible setup.</p>\n<p>If you already have DeepSeek/Goedel/Kimina running somewhere, would you be willing to try a quick pass@32 on LeanCat and share config/logs? Partial is totally fine.</p>",
        "id": 566442630,
        "sender_full_name": "Rongge",
        "timestamp": 1767649088
    },
    {
        "content": "<p><a href=\"https://github.com/sciencraft/LeanCat/blob/main/CAT_statement/S_0003.lean\">Problem 3</a> entails 4 theorems. Do you give partial scores here, or do you flag it as unsolved if &lt; 4 theorems are filled in?</p>",
        "id": 566545765,
        "sender_full_name": "Simon Sorg",
        "timestamp": 1767706996
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566238175\">said</a>:</p>\n<blockquote>\n<p>It seems a little strange to release the first version of the benchmark at a toolchain that is 7 months old; do you plan to update it?</p>\n</blockquote>\n<p>It might be the same problem as FATE benchmark. 7 months might be the time it takes to write code for benchmark and submit paper.</p>",
        "id": 567362346,
        "sender_full_name": "Nick Adfor",
        "timestamp": 1768101350
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"766274\">Simon Sorg</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566545765\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://github.com/sciencraft/LeanCat/blob/main/CAT_statement/S_0003.lean\">Problem 3</a> entails 4 theorems. Do you give partial scores here, or do you flag it as unsolved if &lt; 4 theorems are filled in?</p>\n</blockquote>\n<p>We treat each file as one problem: solved = everything in the file compiles (so S_0003 needs 4/4). Keeps pass@k clean + avoids weird weighting from file splitting.<br>\nNo partial credit in the main score for now—maybe we’ll add a “#theorems solved / total” stat later as a nice-to-have analysis.</p>",
        "id": 567439455,
        "sender_full_name": "Rongge",
        "timestamp": 1768192507
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"877182\">Nick Adfor</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/567362346\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566238175\">said</a>:</p>\n<blockquote>\n<p>It seems a little strange to release the first version of the benchmark at a toolchain that is 7 months old; do you plan to update it?</p>\n</blockquote>\n<p>It might be the same problem as FATE benchmark. 7 months might be the time it takes to write code for benchmark and submit paper.</p>\n</blockquote>\n<p>Hi Nick,</p>\n<p>Thank you for pointing this out. As Rongge mentioned, that's because of taking the LeanExplore into consideration;  otherwise we have to build a search tool ourselves! :-P</p>\n<p>Best, Justin</p>\n<p><span class=\"user-mention silent\" data-user-id=\"1005225\">Rongge</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/566241282\">said</a>:</p>\n<blockquote>\n<p>Thanks for flagging this. We pinned Lean/mathlib 4.19 for reproducibility of the v1 results, and because the current eval harness is still tied to a LeanExplore-based retrieval + compiler-feedback loop on that toolchain. We'd like to support a more recent mathlib/Lean release as well, but I can't promise a timeline until we decouple/update the retrieval backend. If there's a community-preferred pattern here (e.g. frozen vs rolling), pointers would be very appreciated.</p>\n</blockquote>",
        "id": 567951286,
        "sender_full_name": "Justin K. Wang",
        "timestamp": 1768387279
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"1010209\">Justin K. Wang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/567951286\">said</a>:</p>\n<blockquote>\n<p>Hi Nick,</p>\n<p>Thank you for pointing this out. As Rongge mentioned, that's because of taking the LeanExplore into consideration;  otherwise we have to build a search tool ourselves! :-P</p>\n<p>Best, Justin<br>\n</p>\n</blockquote>\n<p>Hi, Justin,</p>\n<p>Thank you for replying this immediately. Actually what I want to know most is about the reason why a search tool is needed (It might be I do not understand the article well). I also know that FATE <a href=\"https://arxiv.org/abs/2511.02872\">https://arxiv.org/abs/2511.02872</a> seems to use search tool implictly (Their search tool is <a href=\"http://leansearch.net\">leansearch.net</a>, which is not included in the article. But I know that when proofreading the benchmark dataset they still uses leansearch to help as the working document referred to it.) What is the relation between the search tool and the benchmark dataset? (For personal use, I am more familiar with moogle as it is in vscode. So I cannot tell the difference between the different search tools)</p>\n<p>Best, Nick</p>",
        "id": 568033593,
        "sender_full_name": "Nick Adfor",
        "timestamp": 1768409253
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"877182\">Nick Adfor</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/568033593\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"1010209\">Justin K. Wang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCat.20benchmark.20.28category.20theory.20in.20Lean.204.29/near/567951286\">said</a>:</p>\n<blockquote>\n<p>Hi Nick,</p>\n<p>Thank you for pointing this out. As Rongge mentioned, that's because of taking the LeanExplore into consideration;  otherwise we have to build a search tool ourselves! :-P</p>\n<p>Best, Justin<br>\n</p>\n</blockquote>\n<p>Hi, Justin,</p>\n<p>Thank you for replying this immediately. Actually what I want to know most is about the reason why a search tool is needed (It might be I do not understand the article well). I also know that FATE <a href=\"https://arxiv.org/abs/2511.02872\">https://arxiv.org/abs/2511.02872</a> seems to use search tool implictly (Their search tool is <a href=\"http://leansearch.net\">leansearch.net</a>, which is not included in the article. But I know that when proofreading the benchmark dataset they still uses leansearch to help as the working document referred to it.) What is the relation between the search tool and the benchmark dataset? (For personal use, I am more familiar with moogle as it is in vscode. So I cannot tell the difference between the different search tools)</p>\n<p>Best, Nick</p>\n</blockquote>\n<p>Hi Nick, the benchmark dataset itself is independent of any search tool. We use search only in our workflow/baselines: in particular, LeanBridge (our kernel-checked pipeline) uses retrieval (e.g. LeanExplore) to fetch relevant mathlib lemmas/definitions as context, which makes both curation and tool-augmented proving much more efficient. So the search tool affects the solver and development workflow, not the benchmark items.<br>\nRegarding search tools: leansearch / LeanExplore / Moogle are just different retrieval frontends/backends over (roughly) the same corpus; they differ in ranking/UI/API, but they’re interchangeable from the dataset’s perspective.</p>",
        "id": 568398720,
        "sender_full_name": "Rongge",
        "timestamp": 1768558317
    }
]