[
    {
        "content": "<p>I think it's clear we could do with a better approach <em>provided as standard in Lean + Mathlib</em> for marking up problems to be solved and then verifying a provided solution. That is, a standard tool to check that a file with a solution (after exporting the proof from Lean and reading back into a separate checker) does indeed provide a properly typed term, depending only on the standard three axioms (not <code>Lean.ofReduceBool</code>), whose type matches (is defeq to) that in the trusted file with the problem. Along with that, there should be standard ways to mark up \"this is the problem to be solved\" and \"this is where you need to substitute an <code>answer</code> of this type\". And then the tools for checking the solution should also be able to print the provided <code>answer</code> using only trusted delaborators for human verification that it's a legitimate answer to the problem. Optionally, but probably useful for multiple repositories, Compfiles-style markup for indicating which parts of a file with both problem and solution are the problem statement and answer to be given, and which are the solution.</p>\n<p>(This might not actually be used in Lean + Mathlib, but it seems sufficiently generally relevant to be worth having as a standard tool there, or failing that in some leanprover-community repository, and arguably parts of it would be appropriate to use in <code>Archive/Imo</code> to indicate what's the original problem and what's the solution to it / answer the problem asked to be found.)</p>",
        "id": 531569920,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1753784526
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"266253\">Joseph Myers</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/near/531569920\">said</a>:</p>\n<blockquote>\n<p>I think it's clear we could do with a better approach <em>provided as standard in Lean + Mathlib</em> for marking up problems to be solved and then verifying a provided solution. </p>\n</blockquote>\n<p>(If I understand you correctly,) I've been advocating for something like this since the DeepSeek v2 bug was found.  The best thing I've found so far is <a href=\"https://github.com/GasStationManager/SafeVerify\">SafeVerify</a> by <span class=\"user-mention\" data-user-id=\"776090\">@GasStationManager</span>.  I think I've successfully convinced <span class=\"user-mention\" data-user-id=\"644040\">@George Tsoukalas</span> to use it to check the PutnamBench solutions, and <a href=\"#narrow/channel/270676-lean4/topic/A.20better.20Lean.20checker\">I've advocated for it to be made into something more official</a>, similar to lean4checker (although it mostly relies on official components like lean4checker's replay and the code Lean uses to check axioms).  <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> seemed to <a href=\"#narrow/channel/270676-lean4/topic/A.20better.20Lean.20checker/near/517413772\">think it was pretty good</a>.  It also has support for solutions (although they would still need to be checked manually for being in the right form).  <span class=\"user-mention\" data-user-id=\"110596\">@Rob Lewis</span>'s <a href=\"https://github.com/robertylewis/lean4-autograder-main\">autograder</a> is the other alternative.</p>",
        "id": 531572959,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753785453
    },
    {
        "content": "<p>One of the issues with asking an AI to fill in the answer is that sometimes the question might ask you to find a set of solutions to an equation, and this implicitly means find a simple description of this set, but in Lean you could just write down the equations as the definition of the set. In fact on all problems you could use <code>Classical.epsilon (property given in the question)</code> as the answer. So you need to be precise about what find the solution means and this isn't that easy.</p>",
        "id": 531573352,
        "sender_full_name": "Chris Hughes",
        "timestamp": 1753785579
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/near/531572959\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"266253\">Joseph Myers</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/near/531569920\">said</a>:</p>\n<blockquote>\n<p>I think it's clear we could do with a better approach <em>provided as standard in Lean + Mathlib</em> for marking up problems to be solved and then verifying a provided solution. </p>\n</blockquote>\n<p>(If I understand you correctly,) I've been advocating for something like this since the DeepSeek v2 bug was found.  The best thing I've found so far is <a href=\"https://github.com/GasStationManager/SafeVerify\">SafeVerify</a> by <span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span>.  I think I've successfully convinced <span class=\"user-mention silent\" data-user-id=\"644040\">George Tsoukalas</span> to use it to check the PutnamBench solutions, and <a href=\"#narrow/channel/270676-lean4/topic/A.20better.20Lean.20checker\">I've advocated for it to be made into something more official</a>, similar to lean4checker (although it mostly relies on official components like lean4checker's replay and the code Lean uses to check axioms).  <span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> seemed to <a href=\"#narrow/channel/270676-lean4/topic/A.20better.20Lean.20checker/near/517413772\">think it was pretty good</a>.  It also has support for solutions (although they would still need to be checked manually for being in the right form).  <span class=\"user-mention silent\" data-user-id=\"110596\">Rob Lewis</span>'s <a href=\"https://github.com/robertylewis/lean4-autograder-main\">autograder</a> is the other alternative.</p>\n</blockquote>\n<p>Yes, that's the sort of thing I want, but as something standard with Lean. So the reference manual would say something like\"To check a claimed solution to a problem, run (some command) in a sandbox to process and export the untrusted proof, then run (some command) to check the exported file does provide a valid proof of the type requested in your trusted file with the problem.\". (Together with <code>answer</code> markup.)</p>",
        "id": 531573853,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1753785747
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110044\">Chris Hughes</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/near/531573352\">said</a>:</p>\n<blockquote>\n<p>One of the issues with asking an AI to fill in the answer is that sometimes the question might ask you to find a set of solutions to an equation, and this implicitly means find a simple description of this set, but in Lean you could just write down the equations as the definition of the set. In fact on all problems you could use <code>Classical.epsilon (property given in the question)</code> as the answer. So you need to be precise about what find the solution means and this isn't that easy.</p>\n</blockquote>\n<p>This is where human verification of the answer comes in. (I referred above to using only trusted delaborators to display the answer for review since an adversarial file could in principle have an answer term that just restates the problem but disguise it so that in the Lean source it looks like a valid answer, until you notice the meta code elsewhere in that file being used to cheat.)</p>",
        "id": 531574389,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1753785925
    },
    {
        "content": "<p>I don't think you need fancy markup.  But if you do, then yes, it should be standard in Lean or at least mathlib.  I think you just need a template file and a filled-in version.  One reason for needing the template anyway is to be sure that the solution didn't change the meaning of the problem.  I've found lots of ways you can do this in Lean, especially if you are allowed to add code before the theorem statement.  (It doesn't even have to be meta code.  A simple <code>local instance</code> could be enough.)</p>",
        "id": 531574525,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753785985
    },
    {
        "content": "<p>By the way, I was referring above to <code>local instance</code> changing the meaning of a theorem statement, but I see <span class=\"user-mention\" data-user-id=\"266253\">@Joseph Myers</span>'s point about how it can also change the meaning of a solution.  I present to you the following:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span>\n<span class=\"c1\">-- pay no attention to this local instance behind the curtain</span>\n<span class=\"kn\">local</span><span class=\"w\"> </span><span class=\"kn\">instance</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">LT</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">⟨</span><span class=\"k\">fun</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"bp\">∣</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">-&gt;</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"bp\">∣</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"o\">))</span><span class=\"bp\">⟩</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">solution</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Set</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"bp\">×</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">.</span><span class=\"m\">1</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">.</span><span class=\"m\">2</span><span class=\"w\"> </span><span class=\"o\">}</span><span class=\"w\">  </span><span class=\"c1\">-- this looks reasonable, right?</span>\n\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"bp\">×</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"bp\">∀</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"bp\">∣</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">.</span><span class=\"m\">1</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"bp\">-&gt;</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"bp\">∣</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">.</span><span class=\"m\">2</span><span class=\"o\">))</span><span class=\"w\"> </span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"n\">solution</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n<span class=\"n\">trivial</span>\n</code></pre></div>",
        "id": 531583524,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753788577
    },
    {
        "content": "<p>And no, SafeVerify wouldn't be able to spot that easily (nor likely any current tool) .</p>",
        "id": 531584424,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753788853
    },
    {
        "content": "<p>The FRO should create an officially maintained tool for checking Lean proofs against a specification.</p>",
        "id": 531586178,
        "sender_full_name": "(deleted)",
        "timestamp": 1753789347
    },
    {
        "content": "<p>Well, there's a separate issue here that I think Lean should issue a warning when defining an <code>instance</code> where one can already be synthesized, at least if they aren't defeq... :) But obviously there's other ways to make a sneaky definition</p>",
        "id": 531588485,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1753790052
    },
    {
        "content": "<p>It's not just the <code>native_decide</code> in those p3 and p4 proofs, which could be checked with additional manual effort; it is also the knowledge that this prover model will always try to use <code>native_decide</code> (since it's been trained to do so) in its future proofs, and then how much you trust these future proofs will boil down to how much you trust <code>native_decide</code>. (I'd say the same for Kimina Prover and its use of <code>native_decide</code>.)</p>\n<p>Right now, SafeVerify (and I assume lean4checker) would not be able to process a proof containing native_decide. Hypothetically, there could be a utility that replaces each <code>native_decide</code>with <code>sorry</code>, then pass it to be checked by lean4checker or SafeVerify, and then prints out the goal state for each <code>sorry</code> (ideally from inside the replayed environment) to show what were the goals resolved by native_decide. But Harmonic (and/or the Kimina team) should really be the one doing that.</p>",
        "id": 531653206,
        "sender_full_name": "GasStationManager",
        "timestamp": 1753806792
    },
    {
        "content": "<p>Data contributors like me and others usually use native_decide to reduce the time needed to write proofs. Looks like its use is controversial within the larger Lean community.</p>",
        "id": 531656935,
        "sender_full_name": "(deleted)",
        "timestamp": 1753807864
    },
    {
        "content": "<p>That is yet another danger, data contributors like <span class=\"user-mention\" data-user-id=\"511228\">@Huỳnh Trần Khanh</span>, especially if paid by the theorem, could use <code>native_decide</code> in an inconsistent manner to supply data even faster. <span aria-label=\"smiling devil\" class=\"emoji emoji-1f608\" role=\"img\" title=\"smiling devil\">:smiling_devil:</span>.</p>",
        "id": 531672999,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753813109
    },
    {
        "content": "<p>I don't think there's a problem with using <code>native_decide</code> to help you write proofs, in the same way that there's no problem with using <code>sorry</code> to help write proofs; only when it is left in the finished proof is it controversial. I suspect that with the number of times <code>@[extern]</code> and <code>@[implemented_by]</code> are used in Lean core, <code>Lean.ofReduceBool</code> is already inconsistent.</p>",
        "id": 531673010,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753813114
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"521331\">@Niels Voss</span> You don't have to suspect: <a href=\"#narrow/channel/113488-general/topic/Using.20.60native_decide.60.20to.20prove.20False.3F\">#general &gt; Using &#96;native_decide&#96; to prove False?</a></p>",
        "id": 531673314,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753813196
    },
    {
        "content": "<p>Oh I remember this thread now.</p>",
        "id": 531673460,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753813248
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/near/531574525\">said</a>:</p>\n<blockquote>\n<p>I don't think you need fancy markup.  But if you do, then yes, it should be standard in Lean or at least mathlib.  I think you just need a template file and a filled-in version.  One reason for needing the template anyway is to be sure that the solution didn't change the meaning of the problem.  I've found lots of ways you can do this in Lean, especially if you are allowed to add code before the theorem statement.  (It doesn't even have to be meta code.  A simple <code>local instance</code> could be enough.)</p>\n</blockquote>\n<p>Is this vulnerability limited to changing the file before the theorem statement? i.e. is lean safe from prompt injection style manipulation after the theorem statement?</p>",
        "id": 531680779,
        "sender_full_name": "Yiwei",
        "timestamp": 1753816021
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"118967\">@Yiwei</span> If you allow native_decide (more specifically if you allow its axioms), then you can put in an inconsistent proof without modifying anything above the proof.  See the example in the above link.  If you only allow the three standard axioms, it’s more subtle.  I know of no “attack” which survives <code>#print axioms</code> and works without putting code above the theorem.  And if there was such an exploit, I doubt it would survive SafeVerify (unless of course the kernel was unsound or the axiom checker had a bug).</p>",
        "id": 531682367,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753816687
    },
    {
        "content": "<p>One such attack is that you can redefine <code>#print axioms</code> to always show the standard three. That being said, these super adversarial attacks seem very unlikely in the case of Harmonic's solutions, given that they published the outputs and made them easy to check by hand. However, I think they would be a more serious concern if LLMs started generating much longer proofs.</p>",
        "id": 531682816,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753816874
    },
    {
        "content": "<p>If you don’t do <code>#print axioms</code> (which also checks that the proof was added to the environment), DeepSeek-Prover v2 found an exploit (since fixed) which silently prevented the theorem from being added to the environment.  The model used it a few times to give bad proofs of theorems.</p>",
        "id": 531682943,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753816923
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"521331\">@Niels Voss</span> Oh, I see.  If you assume print axioms will be run after your code, then just add meta code redifining it to the end of your proof!  I like/hate it!</p>",
        "id": 531683210,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753817028
    },
    {
        "content": "<p>Personally, I believe that in an adversarial context, nothing short of generating .olean files, moving those .olean files to another computer (in case the original code contains malware), running an external verifier on the .oleans, and having a human manually check the theorem statement and every single definition and typeclass instance in use would suffice. But that's a topic for another thread.</p>",
        "id": 531684510,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753817482
    },
    {
        "content": "<p>I agree about moving this to another thread.  But it’s happened once (albeit on a naive level) and it will probably happen again.  It isn’t just academic.  We already have good partial solutions like SafeVerify.  Such tools not only help AI research but could assist large blueprint projects and industrial consulting projects.  I worry throwing our hands up in the air and saying it it too hard will delay adoption and further development of such tools.</p>",
        "id": 531686837,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753818369
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"521331\">Niels Voss</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/near/531684510\">said</a>:</p>\n<blockquote>\n<p>Personally, I believe that in an adversarial context, nothing short of generating .olean files, moving those .olean files to another computer (in case the original code contains malware), running an external verifier on the .oleans, and having a human manually check the theorem statement and every single definition and typeclass instance in use would suffice. But that's a topic for another thread.</p>\n</blockquote>\n<p>I believe SafeVerify is doing basically this (minus the separate computer part)</p>",
        "id": 531687246,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1753818527
    },
    {
        "content": "<blockquote>\n<p>I worry throwing our hands up in the air and saying it it too hard will delay adoption and further development of such tools.</p>\n</blockquote>\n<p>Oh, I'm not trying to argue that reliable and complete verification is virtually unattainable. On the contrary, I think it's very much within reach and should be something we should aim for. I was just trying to provide a counterexample for your statement that <code>#print axioms</code> suffices to check a proof.</p>",
        "id": 531687460,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753818607
    },
    {
        "content": "<p>26 messages were moved here from <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Harmonic.3A.20IMO.20Livestream/with/531568148\">#Machine Learning for Theorem Proving &gt; Harmonic: IMO Livestream</a> by <span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span>.</p>",
        "id": 531687711,
        "sender_full_name": "Notification Bot",
        "timestamp": 1753818696
    },
    {
        "content": "<p>Sorry it was hard to make a clean break from the previous conversation.</p>",
        "id": 531687801,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753818734
    },
    {
        "content": "<blockquote>\n<p>I believe SafeVerify is doing basically this (minus the separate computer part)</p>\n</blockquote>\n<p>From SafeVerify's README.md, it seems that it actually allows you to do the separate computer part if you want? In which case, it satisfies my requirements</p>",
        "id": 531688188,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753818874
    },
    {
        "content": "<blockquote>\n<p>I was just trying to provide a counterexample for your statement that <code>#print axioms</code> suffices to check a proof.</p>\n</blockquote>\n<p>I would love for more people to at least start with <code>#print axioms</code>.  That would have caught the DeepSeek bug.  And IIRC, we don't currently have a good way to make sure only the three axioms are used in Mathlib, and I don't think there is any sort of check for this in the Mathlib CI.  But certainly, I agree this isn't sufficient and I apologize for not making that point more clearly.</p>",
        "id": 531690071,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753819587
    },
    {
        "content": "<p>(Ok, we do have a good manual way: code review.)</p>",
        "id": 531690429,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753819728
    },
    {
        "content": "<p>I wouldn't say <code>native_decide</code> is controversial; rather, different subcommunities have different conventions. I think it's fairly clear in the mathlib and mathlib-adjacent communities that <code>ofReduceBool</code> is not a legitimate axiom to depend on for formally verified mathematics, exactly the same as that using <code>axiom</code> is not a proper way to set up a mathematical theory even though some beginners get the idea that it is (understandably, given informal descriptions of mathematical structures based on axioms they satisfy); only the three standard axioms are legitimate. Whereas it seems some people working in software verification are more likely to wish to use <code>bv_decide</code> and get a dependency on <code>ofReduceBool</code> that way. There may be certain similar niche cases in mathematics (verifying a terabyte-sized certificate generated by an external SAT solver, for example), but in those cases I'd expect the person claiming the proof with <code>ofReduceBool</code> is legitimate to be writing a paper about the project involving computations that can't reasonably be handled in Lean without <code>ofReduceBool</code>, which can discuss in detail exactly how the dependency on <code>ofReduceBool</code> is kept to a minimum.</p>",
        "id": 531729529,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1753840118
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Guarding.20against.20exploits.20in.20AI.20code/near/531686837\">said</a>:</p>\n<blockquote>\n<p>I agree about moving this to another thread.  But it’s happened once (albeit on a naive level) and it will probably happen again.  It isn’t just academic.  We already have good partial solutions like SafeVerify.  Such tools not only help AI research but could assist large blueprint projects and industrial consulting projects.  I worry throwing our hands up in the air and saying it it too hard will delay adoption and further development of such tools.</p>\n</blockquote>\n<p>Right now, the people we see claiming AI solutions to unsolved problems, and claiming that the AI gave a formal proof in Lean, are producing AI slop that doesn't even compile, or proves a statement that <em>obviously</em> has no relation to the actual unsolved problem they're claiming to have solved.</p>\n<p>I expect more plausible-looking claims to appear comparatively soon, simply because occasionally an unsolved problem does turn out to have a fairly straightforward solution using known ideas, and would like us to have an official verification solution, clearly documented, when such claims start appearing. (That still won't help with the people who claim to have solved a major open problem but whose formal statement is something different and much easier.)</p>",
        "id": 531734922,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1753843513
    },
    {
        "content": "<p>Besides the usual stuff (run it in Lean, make sure it type checks with 0 errors) and the stuff that should be usual (print the axioms, run it in lean4checker), it then gets a bit complicated if someone is formalizing and solving a problem themselves.  Of course, if the person is a crank, it is going to be painful.  But if they are sincere and well-meaning, that can go a long way.  The interaction can be about teaching and not adversarial.  And for this latter camp, I agree the Lean community could develop good protocols that would reduce the burden.</p>",
        "id": 531735533,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753843948
    },
    {
        "content": "<p>A simple one would be to make two files, one with just the theorem statement and a sorried proof and the other with everything.  Then run it through SafeVerify.  That will make it easier to check the theorem statement.</p>",
        "id": 531736008,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753844272
    },
    {
        "content": "<p>Another would be to emphasize that for problems in <a href=\"https://github.com/google-deepmind/formal-conjectures\">formal conjectures</a> (or similar), just because you solved a problem as written, doesn't mean you solved the actual open problem.  We need to check that the problem was formalized correctly.  There could be mistakes.  (Of course, this is unpleasant for everyone if this happens, but it is good to get this out in the open right away.)</p>",
        "id": 531736189,
        "sender_full_name": "Jason Rute",
        "timestamp": 1753844399
    },
    {
        "content": "<blockquote>\n<p>But if they are sincere and well-meaning, that can go a long way.  The interaction can be about teaching and not adversarial.  And for this latter camp, I agree the Lean community could develop good protocols that would reduce the burden.</p>\n</blockquote>\n<p>I definitely agree that most Lean code is not written in an adversarial way. The reason I suggested treating this in an adversarial context is that</p>\n<ol>\n<li>RL is somewhat similar to an adversarial context in the sense that the AI will just try random things and if one of them happens to succeed, it will cause the AI to always exploit it. In this case, it would be in the AI developers own interest to make sure the code is adverserally secure.</li>\n<li>As Lean becomes more widely used, people will likely start demanding increasing levels of security. It's better in the long run if everyone always uses the most secure option. Kind of like how we prefer sha256 over md5 even for non-security sensitive applications.</li>\n</ol>\n<p>I think SafeVerify is a good approach right now. I also agree that developing protocols and social convention to use those protocols is just as important as developing tools.</p>",
        "id": 531757125,
        "sender_full_name": "Niels Voss",
        "timestamp": 1753856002
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Guarding.20against.20exploits.20in.20AI.20code/near/531682367\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"118967\">Yiwei</span> If you allow native_decide (more specifically if you allow its axioms), then you can put in an inconsistent proof without modifying anything above the proof.  See the example in the above link.  If you only allow the three standard axioms, it’s more subtle.  I know of no “attack” which survives <code>#print axioms</code> and works without putting code above the theorem.  And if there was such an exploit, I doubt it would survive SafeVerify (unless of course the kernel was unsound or the axiom checker had a bug).</p>\n</blockquote>\n<p>I'm trying to check my understanding:</p>\n<p>The native_decide/implemented_by gap seems to be a different type of problem than the compiler bugs.  There could be a subtler proofs of False within the compiler + runtime that get greenlit by native_decide (a longer \"false loop\") that are harder to detect.  </p>\n<p>csimp has the additional issue that it doesn't correctly output the axioms it uses, which would make it invisible to some of the proposed fixes.</p>\n<p>Appreciate any corrections to the my interpretation.</p>",
        "id": 532395420,
        "sender_full_name": "Yiwei",
        "timestamp": 1754093253
    }
]