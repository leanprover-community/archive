[
    {
        "content": "<p>Has anyone tried out <a href=\"https://github.com/cmu-l3/llmlean\">LLMLean</a>, either locally or via an API?  I'm curious if it is any good?  In particular, I'm curious about the following:</p>\n<ul>\n<li>Does it fill in the kinds of proofs you would like an AI to fill in for you?  (Especially in comparison to similar tools like Lean Copilot, or the general purpose coding copilots?)</li>\n<li>Is it easy to install?</li>\n<li>Does the answer to the first two questions matter if you are using the local or API version?</li>\n<li>If using the API version, is it quickly going through your API credits?  (I assume <code>llmqed</code> is doing an expensive search, but I'm not sure.  Maybe the parameters suggest <code>llmqed</code> has a fairly low limit on the number of model calls.)</li>\n</ul>",
        "id": 467036150,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725306538
    },
    {
        "content": "<p>I would try updating my lean toolchain for my project and installing it now, but it seems it's on the <code>leanprover/lean4:v4.11.0-rc1</code> toolchain and mathlib is now on <code>leanprover/lean4:v4.11.0</code>, so will it even be possible to use?</p>",
        "id": 467106602,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1725336938
    },
    {
        "content": "<p>Those toolchains are fairly compatible. Most of the changes are bugfixes around the <code>variable</code> command, so if that is not used much you may be lucky.</p>",
        "id": 467109062,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1725337887
    },
    {
        "content": "<p>Ok yes, got it working with no changes, thanks!</p>\n<p>Frankly I'm unimpressed with it though. <code>llmqed</code> is failing to find a proof for a a subgoal with <code>hk_pos : 0 &lt; k ⊢ k ≠ 0</code> in the context</p>",
        "id": 467122457,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1725342365
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span> Is that running it locally via ollama or through an OpenAI API?  Also, is that the full goal, or is the <code>0 &lt; k</code> buried in there among other elements?</p>",
        "id": 467217801,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725362523
    },
    {
        "content": "<p>That is running locally through Ollama, entirely possible that a more powerful LLM would be more consistent.<br>\n<span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span> is there any way using a Local LLM to let the LLM crank through more trials to potentially get more consistent results?</p>",
        "id": 467369100,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1725402302
    },
    {
        "content": "<blockquote>\n<p>is that the full goal,</p>\n</blockquote>\n<p><del>It is also buried among other elements, so understandable that the LLM would get confused, but still not exactly encouraging, since I feel most contexts I am in the final steps of closing do have a bunch of now-irrelevant hypotheses.</del></p>\n<p>I am also noticing that <code>llmqed</code> is not really working even with this as the full goal, ie</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">LLMlean</span>\n\n<span class=\"kn\">example</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℕ</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">h</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">¬</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"bp\">≤</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n<span class=\"w\">  </span><span class=\"n\">llmqed</span><span class=\"w\"> </span><span class=\"c1\">-- sometimes gives no suggestions, sometimes one or two step candidates, but doesn't ever close the goal</span>\n</code></pre></div>",
        "id": 467369705,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1725402606
    },
    {
        "content": "<p>I am also seeing that it never suggests more than one step at a time, I wonder if I am using it wrong?</p>",
        "id": 467371314,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1725403443
    },
    {
        "content": "<p>Thanks for trying it out! If you're using the default ollama model, it's fine-tuned for tactic prediction (the <code>llmstep</code> tactic). For <code>llmqed</code> you may see more success with Open AI. </p>\n<p>Here's what <code>llmqed</code> with the Open AI API gives for your example:</p>\n<p><a href=\"/user_uploads/3121/GoWXkgPq5A5oBslGGgQ1AeK7/llmqed.png\">llmqed.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/GoWXkgPq5A5oBslGGgQ1AeK7/llmqed.png\" title=\"llmqed.png\"><img data-original-dimensions=\"2928x1186\" src=\"/user_uploads/thumbnail/3121/GoWXkgPq5A5oBslGGgQ1AeK7/llmqed.png/840x560.webp\"></a></div>",
        "id": 467686670,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1725496912
    },
    {
        "content": "<p>To control the number of trials, you can use the <code>LLMLEAN_NUMSAMPLES</code> environment variable (<a href=\"https://github.com/cmu-l3/llmlean/blob/main/docs/customization.md#llm-on-your-laptop\">ref</a>).</p>",
        "id": 467686939,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1725497127
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"409334\">Sean Welleck</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LLMLean.2C.20anyone.20using.20it.3F/near/467686670\">said</a>:</p>\n<blockquote>\n<p>If you're using the default ollama model, it's fine-tuned for tactic prediction (the <code>llmstep</code> tactic). For <code>llmqed</code> you may see more success with Open AI.</p>\n</blockquote>\n<p>This may be good to add to the documentation.</p>",
        "id": 467701502,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725503293
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"409334\">Sean Welleck</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LLMLean.2C.20anyone.20using.20it.3F/near/467686939\">said</a>:</p>\n<blockquote>\n<p>To control the number of trials, you can use the <code>LLMLEAN_NUMSAMPLES</code> environment variable (<a href=\"https://github.com/cmu-l3/llmlean/blob/main/docs/customization.md#llm-on-your-laptop\">ref</a>).</p>\n</blockquote>\n<p>Is there a reason this couldn't just be added as an optional parameter to the <code>llmqed</code> tactic?  Of course, you could still change the default with a command line parameter (or an option configurable with <code>set_option</code>).</p>",
        "id": 467701835,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725503433
    },
    {
        "content": "<p>Also, does your code generalize to other public API models like those of Mistral, Anthropic, etc?  (Or is there something special about OpenAI, such as using a fine-tuned OpenAI model or a prompt specific to OpenAI?)  Also, would it be easy to incorporate the new DeepSeek-Prover models?</p>",
        "id": 467702388,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725503620
    },
    {
        "content": "<p>And a very silly question (for anyone here), how do you set env variables when running Lean in VS Code?</p>",
        "id": 467818802,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725534712
    },
    {
        "content": "<p>if you spawn a process then <a href=\"https://leanprover-community.github.io/mathlib4_docs/Init/System/IO.html#IO.Process.SpawnArgs\">IO.Process.SpawnArgs</a> has a field for environment variables.</p>",
        "id": 467819548,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1725534791
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"243562\">@Adam Topaz</span> I think I’m talking about the other direction.  LLMlean is using <code>IO.getEnv</code> to read the environmental variables (not to set them for a subprocess).  But since Lean runs in VSCode it is not exactly clear to me (or other people probably) how to set these so that the LLMLean tactics can read them.  (Also, I don’t necessarily think this is the ideal way to change settings for a tactic.  It might be fine for private data like an API key, that you don’t want to leak into your repo, but not for the number of samples to generate.)</p>",
        "id": 467830892,
        "sender_full_name": "Jason Rute",
        "timestamp": 1725536904
    },
    {
        "content": "<p>The usual way to set env variables is by modifying your profile file (at least on Linux, I have no idea what windows does). Is that what you’re asking?</p>",
        "id": 467908134,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1725552225
    },
    {
        "content": "<p>I don’t know how to do this within vscode or if it’s even possible</p>",
        "id": 467908255,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1725552244
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> the 'easiest' way I think is \"start VSCode from your shell rather than a graphical menu\". (I say this not being a VSCode user but having helped someone do this previously)</p>",
        "id": 467913540,
        "sender_full_name": "Julian Berman",
        "timestamp": 1725553438
    },
    {
        "content": "<p>But undoubtedly VSCode has some way of configuring what environment it spawns processes in -- the thing is yeah, it will require figuring out whether that happens <em>before</em> or after it spawns the language server, and which kind of shell it spawns (if any -- which hopefully is 'none' by default), and ...</p>",
        "id": 467913722,
        "sender_full_name": "Julian Berman",
        "timestamp": 1725553483
    },
    {
        "content": "<p>That being said, something like <a href=\"https://stackoverflow.com/a/76880129\">this stackoverflow thread and answer</a> seem to have some details if you want to get it working in other ways</p>",
        "id": 467913907,
        "sender_full_name": "Julian Berman",
        "timestamp": 1725553545
    },
    {
        "content": "<p>It looks like this can be accomplished by modifying the launch configuration of vscode: <a href=\"https://code.visualstudio.com/docs/editor/debugging#_launch-configurations\">https://code.visualstudio.com/docs/editor/debugging#_launch-configurations</a></p>",
        "id": 467923528,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1725555787
    },
    {
        "content": "<p>but it also seems that this is primarily for debugging purposes.</p>",
        "id": 467923672,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1725555818
    },
    {
        "content": "<p>We used to be able to easily set environment variables in the VS Code settings, but this functionality disappeared from VS Code at some point</p>\n<p>Here's a screenshot from the old LLMLean docs before this feature was removed from VS Code:<br>\n<a href=\"/user_uploads/3121/HgJV4p05o0vc_V0TB76zs8Yk/env_example1.png\">env_example1.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/HgJV4p05o0vc_V0TB76zs8Yk/env_example1.png\" title=\"env_example1.png\"><img data-original-dimensions=\"1360x480\" src=\"/user_uploads/thumbnail/3121/HgJV4p05o0vc_V0TB76zs8Yk/env_example1.png/840x560.webp\"></a></div>",
        "id": 467959749,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1725563962
    },
    {
        "content": "<p>Now I add the environment variables to <code>.bashrc</code> (or <code>.zshrc</code>) and restart VS Code</p>",
        "id": 467959771,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1725563970
    },
    {
        "content": "<p><a href=\"/user_uploads/3121/nZi9mAOYqDGJKnv1683v9tXr/image.png\">image.png</a><br>\nI got a stupid question, after i import\"LLMlean\" and write the tactic\"llmstep\", it stay still for such a long time , and i can't  get any output. Can someone tell me why?</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/nZi9mAOYqDGJKnv1683v9tXr/image.png\" title=\"image.png\"><img data-original-dimensions=\"1907x329\" src=\"/user_uploads/thumbnail/3121/nZi9mAOYqDGJKnv1683v9tXr/image.png/840x560.webp\"></a></div>",
        "id": 471205072,
        "sender_full_name": "Phoebe Bai",
        "timestamp": 1726649600
    }
]