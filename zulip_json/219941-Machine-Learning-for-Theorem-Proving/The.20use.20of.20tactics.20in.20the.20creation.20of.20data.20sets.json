[
    {
        "content": "<p>Does anyone know if using tactics can hinder the quality of a data set used to train a transformer to auto-formalize english proofs? </p>\n<p>For example, the problem (r \\in Q) -&gt; (Irrational x) -&gt; (Irrational (r+x)) gets one-shotted by simp because of Rat.cast_add.</p>\n<p>But not using tactics would make the creation of a large enough data set quite exhausting. Any thoughts?</p>",
        "id": 495788955,
        "sender_full_name": "Srivatsa Srinivas",
        "timestamp": 1737751041
    },
    {
        "content": "<p>I don't know what this means.  Are you asking if it is a challenge that natural language proofs don't have a literal equivalent in formal proofs?  Some proofs are long in natural language and short in Lean, while others are long in Lean, but short in natural language.  If so, I do imagine it is quite a challenge.  I don't know if there is really all that much work on automatically translating proofs.  There are some papers like Draft-Sketch-Prove and successors.  It seems from those papers that hierarchical approaches help, since you can then match up the higher abstract levels of the proof with blocks of code which are then filled in by proof search.  And that one shouldn't expect a perfect one-shot translation, but have mechanisms where one can fill in or fix low-level details.  I could imagine this approach getting more refined as the field improves (which again has very little research that I'm aware of).</p>",
        "id": 495814431,
        "sender_full_name": "Jason Rute",
        "timestamp": 1737764624
    },
    {
        "content": "<p>Also, is there any dataset currently for auto-formalization of proofs?  There are datasets for formalization of statements, but any for proofs?</p>",
        "id": 495814632,
        "sender_full_name": "Jason Rute",
        "timestamp": 1737764755
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  </p>\n<p>Thanks for the response to my vague question.  Here is what I meant; natural language proofs tend to keep mathematical information such as the product of two rationals is rational while skipping the type theoretic information such as congrArg Rat.cast. We can skip the type theoretic details using tactics like rw, exact and simp but sometimes these can solve a goal without the user inputting any mathematical content because the @simp tag covers so many theorems. (In the case posted originally, it's Rat.cast_add)</p>\n<p>Would training data that skips a lot of mathematical content due to tactics be useful to train on? Was the question I'm interested in</p>\n<p>There are datasets for proofs, but the problems are not beyond high school level I think. I think they are by InternLM</p>",
        "id": 495815765,
        "sender_full_name": "Srivatsa Srinivas",
        "timestamp": 1737765572
    },
    {
        "content": "<p>If you know a reference, I would be curious.  (I haven't done the best job at keeping track of autoformalization.)</p>",
        "id": 495822683,
        "sender_full_name": "Jason Rute",
        "timestamp": 1737770162
    },
    {
        "content": "<p>But as for your question \"Would training data that skips a lot of mathematical content due to tactics be useful to train on?\", I assume the answer is yes.  Generally more diverse data is better, no?  Of course, if one had better-aligned data that explained step-by-step how to go from natural language to formal with reasons on the particulars of Lean, that would even be better probably, but that is more work.  Of course, I'm not a practitioner in autoformalization, so I could be mistaken.</p>",
        "id": 495822953,
        "sender_full_name": "Jason Rute",
        "timestamp": 1737770335
    },
    {
        "content": "<p>Here is the data set: <a href=\"https://huggingface.co/datasets/internlm/Lean-Workbook\">https://huggingface.co/datasets/internlm/Lean-Workbook</a></p>",
        "id": 495829279,
        "sender_full_name": "Srivatsa Srinivas",
        "timestamp": 1737775413
    },
    {
        "content": "<p>Thanks for the input! I needed to discuss this to just flesh out some thoughts</p>",
        "id": 495829342,
        "sender_full_name": "Srivatsa Srinivas",
        "timestamp": 1737775452
    },
    {
        "content": "<p>Okay, these guys use tactics</p>",
        "id": 495829482,
        "sender_full_name": "Srivatsa Srinivas",
        "timestamp": 1737775572
    },
    {
        "content": "<p>I donâ€™t see any natural language proofs in that dataset.  Am I missing something?</p>",
        "id": 495829811,
        "sender_full_name": "Jason Rute",
        "timestamp": 1737775870
    },
    {
        "content": "<p>You are right. They only have a few proofs (a statement they make) and one that I have not verified. I do not know if those proofs have natural language alongside them</p>",
        "id": 495993661,
        "sender_full_name": "Srivatsa Srinivas",
        "timestamp": 1737919124
    }
]