[
    {
        "content": "<p>We introduce <a href=\"https://github.com/lean-dojo/LeanCopilot\">Lean Copilot</a> for language models to suggest tactics, search for proofs, and select premises in Lean.  Here is a short demo:</p>\n<p><a href=\"/user_uploads/3121/vEiWfN_3QfOGt6RcYIgPSr1m/Lean-Copilot-v1.mp4\">Lean-Copilot-v1.mp4</a></p>\n<div class=\"message_inline_image message_inline_video\"><a href=\"/user_uploads/3121/vEiWfN_3QfOGt6RcYIgPSr1m/Lean-Copilot-v1.mp4\" title=\"Lean-Copilot-v1.mp4\"><video preload=\"metadata\" src=\"/user_uploads/3121/vEiWfN_3QfOGt6RcYIgPSr1m/Lean-Copilot-v1.mp4\"></video></a></div><p>Its initial version was known as LeanInfer, but now we have significantly expanded its scope:</p>\n<ul>\n<li>Faster tactic generation on both CPUs and GPUs (powered by <a href=\"https://github.com/OpenNMT/CTranslate2\">CTranslate2</a>)</li>\n<li>Higher tactic generation quality, using beam search instead of multinomial sampling.</li>\n<li>Integrating LLM-generated tactics with <a href=\"https://github.com/leanprover-community/aesop\">aesop</a> for proof search.</li>\n<li>Retrieval for premise selection</li>\n<li>Supporting user-provided models.</li>\n</ul>",
        "id": 407175937,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702278471
    },
    {
        "content": "<p>This is probably a stupid question, but how far are we from being able to use something like this to \"clean up\" some code and it get PR ready or close to PR ready? I have a bunch of stuff in a repo I need to PR and while I don't expect something like this to be able to prove the results it contains, it would be nice if it could learn from the proofs that are in the repo and then just tidy it up (i.e.  I say this is the main result I want you to prove, then go, find the proof in the repo, break up into smaller results, remove unused results, fix spacing/layout etc).  Perhaps I'm just way behind and this is already possible, or maybe  this is sci fi and its actually hard to do and we are not there yet.</p>",
        "id": 407216529,
        "sender_full_name": "Chris Birkbeck",
        "timestamp": 1702292560
    },
    {
        "content": "<p>(also sorry if this is off topic for this thread, it was just that the premise selection and proof search bits got me thinking about this)</p>",
        "id": 407216899,
        "sender_full_name": "Chris Birkbeck",
        "timestamp": 1702292673
    },
    {
        "content": "<p>One danger of putting the discussion here is that if this is really meant as a user facing tool with good usability (easy to install, no GPU needed, etc), then you may be missing much of your audience.  It so, it might be worthy of a post in <a class=\"stream\" data-stream-id=\"113486\" href=\"/#narrow/stream/113486-announce\">#announce</a> with a link back here.</p>",
        "id": 407226967,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702296556
    },
    {
        "content": "<p>I see you have a short paper on this at Math-AI 2023 (<a href=\"https://mathai2023.github.io/papers/4.pdf\">https://mathai2023.github.io/papers/4.pdf</a>).  I’m curious if this system could be used for AI research, especially with the build your own model capabilities.  In particular one would need more baselines.  You only test on 50 theorems and they are from Mathematics in Lean.  Why?  This is just another incomparable baseline to all the others out there in the literature.  I’m also curious about your test.  Was it a full proof search?  Was it a deterministic timeout?  Either way, how long did it typically take to run per theorem.  Did you use a GPU?  And if so, how long would it take per theorem on a CPU?</p>",
        "id": 407228827,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702297209
    },
    {
        "content": "<p>I’m also curious how this compares to LLMStep by <span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span> both in terms of capabilities and in terms of ease of use.  I guess for both, it would be best if users play with it, but I’m not sure if either is designed for end users yet.</p>",
        "id": 407229311,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702297386
    },
    {
        "content": "<p>And how it compares to published works like your own ReProver.  Is it significantly nerfed?</p>",
        "id": 407229567,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702297455
    },
    {
        "content": "<p>Lean 3 used to have the ability (maybe still does) to just do the tactic suggestions automatically in the infoview (using HTPS model via an API).  That would be a much better user interface than typing a tactic every time.  That is if it doesn’t churn through your CPU too much.  <span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span> added it to Lean 3.</p>",
        "id": 407232940,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702298706
    },
    {
        "content": "<p>Although the Lean 3 infoview suggestions don’t run or rank the tactics which I think this does.  That is even more important than putting them in the infoview.  (I gave a demo of the Lean 3 tactic suggestions at the IPAM workshop once and the audience was really skeptical since all the first suggested tactics failed.)</p>",
        "id": 407236648,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702299962
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"389019\">Chris Birkbeck</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407216529\">said</a>:</p>\n<blockquote>\n<p>This is probably a stupid question, but how far are we from being able to use something like this to \"clean up\" some code and it get PR ready or close to PR ready? I have a bunch of stuff in a repo I need to PR and while I don't expect something like this to be able to prove the results it contains, it would be nice if it could learn from the proofs that are in the repo and then just tidy it up (i.e.  I say this is the main result I want you to prove, then go, find the proof in the repo, break up into smaller results, remove unused results, fix spacing/layout etc).  Perhaps I'm just way behind and this is already possible, or maybe  this is sci fi and its actually hard to do and we are not there yet.</p>\n</blockquote>\n<p>We think it would be interesting to have something that can learn from your repo (as long as it's public on GitHub and can be built via <code>lake build</code>), automatically fill in <code>sorry</code> and submit pull requests to your repo. I don't see a fundamental difficulty here. It's just that we have limited personnel and have to priortize.</p>\n<p>BTW, I was wondering if \"PR\" here means pull requests or public relations <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>.</p>",
        "id": 407793919,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702494798
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407226967\">said</a>:</p>\n<blockquote>\n<p>worthy of a post in #announce with a link back here.</p>\n</blockquote>\n<p>Thanks for the suggestion! I'll do that once I get a chance.</p>\n<blockquote>\n<p>short paper on this at Math-AI 2023 </p>\n</blockquote>\n<p>That paper describes an earlier version of Lean Copilot (around September) and hasn't been updated. We plan to release a full paper in early 2024. We haven't decided whether to submit it to a ML conference or conferences like ITP. Personally I lean towards ITP since we want the main contribution to be a user-facing tool.</p>",
        "id": 407794036,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702494847
    },
    {
        "content": "<blockquote>\n<p>You only test on 50 theorems and they are from Mathematics in Lean. Why? This is just another incomparable baseline to all the others out there in the literature. I’m also curious about your test. Was it a full proof search? Was it a deterministic timeout? Either way, how long did it typically take to run per theorem. Did you use a GPU? And if so, how long would it take per theorem on a CPU?</p>\n</blockquote>\n<blockquote>\n<p>And how it compares to published works like your own ReProver. Is it significantly nerfed?</p>\n</blockquote>\n<p>Just to clarify, Lean Copilot does not introduce new models or algorithms for machine learning to prove theorems. You can think of it as a \"frontend\" that interfaces with existing methods. Under the hood, it uses the same ReProver (w/o retrieval) model as in LeanDojo (and you can bring your own models). Therefore, I don't think it's necessary to evaluate Lean Copilot empirically on benchmarks. The results should just be similar to what we described in the LeanDojo paper, despite some minor differences, e.g., different implementations of beam search by CTranslate2 and Hugging Face.</p>",
        "id": 407794835,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702495263
    },
    {
        "content": "<p>From the user side, I’m wondering how much worse performance gets for a typical user computer versus your LeanDojo evaluation machine with eight GPUs.  But it is also true the real test is what <span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> or other Lean users think.</p>",
        "id": 407797388,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702496401
    },
    {
        "content": "<p>As for the ML side, we need better tools for AI researchers (like me) to experiment with new ideas without having to rebuild everything from scratch.  Since you can plug in your own models here, this might be one such tool.  (And in another thread you suggested that.)  If so, it would be good to have baselines and ways to run those baselines.  [Edit: One advantage of using this system as a research tool is that research experiments can be automatically distributed as user-facing tools right away.]</p>",
        "id": 407797476,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702496423
    },
    {
        "content": "<p>But I also think I’ve been too negative.  Great job, and I hope the Lean community finds this to be a valuable tool which is easy to use and powerful enough to be useful.  I don’t know if there are any such tools yet in ITP.  (Maybe SledgeHammer, CoqHammer, and Tactician come closest and I’d love an informal comparison with those from users familiar with the other tools.)</p>",
        "id": 407797908,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702496598
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407797388\">said</a>:</p>\n<blockquote>\n<p>But it is also true the real test is what <span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> or other Lean users think.</p>\n</blockquote>\n<p>I playfully (!) pick on Kevin here since I think he is the most vocal critic of the current research, but I know <span class=\"user-mention\" data-user-id=\"260507\">@Heather Macbeth</span>, <span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span>, <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span>, <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span> and others have also expressed that they don't have a chance to play with the published AI work, so I hope this is not only a chance for them to play with it, but to tell us AI researchers what these systems still need to go the extra mile (or 10 or 100) both in terms of usability and capabilities.</p>",
        "id": 407799830,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702497371
    },
    {
        "content": "<p>When calling <code>lake update LeanCopilot</code> I get the error</p>\n<blockquote>\n<p>error: no error (error code : 0)</p>\n</blockquote>\n<p>that has the effect that the next command <code>lake exe LeanCopilot/download</code> fails.</p>",
        "id": 407800251,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1702497542
    },
    {
        "content": "<p>Oh, I see; I must be in Win WSL and not in Win itself.</p>",
        "id": 407800474,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1702497624
    },
    {
        "content": "<p>2 messages were moved here from <a class=\"stream-topic\" data-stream-id=\"113486\" href=\"/#narrow/stream/113486-announce/topic/LeanCopilot\">#announce &gt; LeanCopilot</a> by <span class=\"user-mention silent\" data-user-id=\"110596\">Rob Lewis</span>.</p>",
        "id": 407800618,
        "sender_full_name": "Notification Bot",
        "timestamp": 1702497671
    },
    {
        "content": "<p>That message does get printed in Windows. It seems to happen when Lake and/or Lean is already running (as a language server in VS Code, for example).</p>",
        "id": 407801213,
        "sender_full_name": "Richard Copley",
        "timestamp": 1702497856
    },
    {
        "content": "<p>I reported that <a href=\"#narrow/stream/270676-lean4/topic/Lake.20on.20Windows/near/405193754\">here</a>. The conversation tailed off.</p>",
        "id": 407801639,
        "sender_full_name": "Richard Copley",
        "timestamp": 1702498054
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"300245\">Filippo A. E. Nuccio</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407800474\">said</a>:</p>\n<blockquote>\n<p>Oh, I see; I must be in Win WSL and not in Win itself.</p>\n</blockquote>\n<p>Yep, Windows does not work yet. See <a href=\"https://github.com/lean-dojo/LeanCopilot/issues/31\">https://github.com/lean-dojo/LeanCopilot/issues/31</a></p>",
        "id": 407801754,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702498098
    },
    {
        "content": "<p>I see, no problem. I have WSL installed (with an oldish version of lake and little time to update now), but I will be happy to test on Win whenever it will become available.</p>",
        "id": 407802484,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1702498406
    },
    {
        "content": "<p>I'll rise to the bait here. Come April (when teaching is out of the way) I hope to be spending a lot of time proving Fermat's Last Theorem. What can this system offer me? I tend to just ignore all the messages in this stream because my instinct is that right now they can offer me nothing. But I'd be very happy to be proved wrong. In practice I'm going to be translating technical mathematics from research papers into lean.</p>",
        "id": 407808470,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702501016
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> see the 2 min video at the top of this thread.  Previous works like lean-gptf had tactic suggestions, but not full proof search or premise selection for end users.  (I’m not involved in the work, but) I’m curious if you think the tactic predictions and/or proofs it finds are any good.  I’m also curious of if you think tools like this would be useful enough if say they had better user interfaces (automatic tactic/premise suggestions, the ability to search for proofs in the background, etc.)  In short, I’m curious where the state of this field is at from the point of view of end users.</p>",
        "id": 407812285,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702502452
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> Does premises selection include new premises, or just those seen during training?</p>",
        "id": 407812393,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702502503
    },
    {
        "content": "<p>I guess I watch these videos and feel like they are so far away from my actual use case, but really what I should do is just try to use the software I guess.</p>",
        "id": 407813332,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702502928
    },
    {
        "content": "<p>And say what your use case is…, maybe in its own thread or a blog post even after trying these tools.</p>",
        "id": 407813707,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503091
    },
    {
        "content": "<p>I am mildly disappointed that it doesn't seem to work like GitHub Copilot. Is there a way just let my cursor sit where I need to complete my proof and have it automagically finish it for me if it can?</p>",
        "id": 407813883,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1702503161
    },
    {
        "content": "<p>Another minor complaint: The installation seems to indicate that I need to import the project as a Lean dependency. This makes it hard to contribute to Mathlib with this, since I generally just work in the Mathlib repository itself, and I would need to modify Mathlib's lakefile to import this, but then not commit those modifications.</p>",
        "id": 407814534,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1702503448
    },
    {
        "content": "<p>(I guess I'll just make some feature suggestions)</p>",
        "id": 407814758,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1702503569
    },
    {
        "content": "<p>OpenAI has spoiled everyone.  Haha.  But I imagine part of this is just UI and the Lean community could help build better UI for this sort of thing around the existing tech.  Avi Shinnar had <a href=\"https://vimeo.com/831643416#t=38m50s\">some good takes on what a good UI would look like at the recent National Academies meeting</a> (at 38:50 mins in).</p>",
        "id": 407815038,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503699
    },
    {
        "content": "<p>The Mathlib comment was the number one reason no one ever used Lean GPT-f, so it is probably a big deal.</p>",
        "id": 407815160,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503742
    },
    {
        "content": "<p>Actually, maybe that wasn't quite the same, since lean-gptf required mathlib so it couldn't be used for mathlib development, where I guess you are just saying it is annoying to use for mathlib development.</p>",
        "id": 407815705,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503972
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> I remarked on this before, but I think it's a bit more important now that it's a tactic and not just a python API for lean: your tactic <code>select_premises</code> is misnamed, these are not 'premises', they are theorems and lemmas in lean terminology. (I'm aware that this terminology is used esp. in the ATP field, but it makes a bit more sense there given that all premises are taken as axioms or hypotheses when proving the conjecture in question. In the context of lean/mathlib, these are all proved theorems.)</p>",
        "id": 407817086,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702504576
    },
    {
        "content": "<p>“Lemma selection” seems to be the second most common term for this in the literature, so maybe <code>select_lemmas</code>?  But then technically you could be selecting a non-Prop definition as well…</p>",
        "id": 407821098,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702506252
    },
    {
        "content": "<blockquote>\n<p>The installation seems to indicate that I need to import the project as a Lean dependency. This makes it hard to contribute to Mathlib with this, since I generally just work in the Mathlib repository itself, and I would need to modify Mathlib's lakefile to import this, but then not commit those modifications.</p>\n</blockquote>\n<p>I guess we wouldn't need to worry about that if LeanCopilot could become an official dependency of mathlib master ... I hope it doesn't mean every mathlib user has to download GBs of model weights, otherwise it's probably not too heavy a dependency.</p>\n<p>In the meantime, maybe we could consider making LeanCopilot run in another copy of the mathlib repo (or of whatever project you're working on)? It's probably not very helpful to retrieve the new lemmas that you added in your branch that you want to PR to mathlib, as you know them well and know when to apply them; it's much more useful to retrieve unfamiliar lemmas in the wider existing mathlib; periodically merging master in the copy should be good enough. I don't think much of LeanCopilot's functionalities depend on running within the same project, but correct me if I'm wrong.</p>\n<p>I think most existing LLM code assistants work by looking at the plain text of the current file and other relevant files in your repo, and maybe also communicating with the language server at times. Most are not written in the target language; in fact many support multiple languages. The Lean-native approach of LeanCopilot could have speed advantages, which may not be important to a human user, but may prove crucial if we go ahead to train automated agents, where faster interaction with the environment could lead to faster feedback loop, iteration and improvement, and big savings in resources.</p>",
        "id": 407843995,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702518164
    },
    {
        "content": "<blockquote>\n<p>making LeanCopilot run in another copy of the mathlib repo</p>\n</blockquote>\n<p>To elaborate, I think a realistic proposal is to split LeanCopilot into a lightweight user-facing client side and a server side. The client just need to implement the <code>suggest_tactics</code>, <code>search_proof</code>, and <code>select_premises</code> (maybe in some generalized extensible form that could support other assistants in the future), and know the server's address to send requests to and receive suggestions from it. It should be relatively easy to get this merged into mathlib master. The server side will host the inference framework, retriever models, indices (are they there yet?), etc. It also needs a copy of mathlib to retrieve from, but that doesn't have to be the client's copy.</p>",
        "id": 407847677,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702519765
    },
    {
        "content": "<p>But doesn't that ruin the generalizability of the <a href=\"https://github.com/lean-dojo/LeanCopilot#bring-your-own-model\">bring-your-own-model</a> part of the project?  Or is it still possible?  Do you now have to split the custom model into a client and server portion, where the client portion reads the current state and environment, and sends stuff back and forth to the server?</p>",
        "id": 407848884,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702520171
    },
    {
        "content": "<p>And I don't understand the \"also needs a copy of mathlib to retrieve from\".  I'm still not sure if retrieval is intended to be project independent.  Can Kevin retrieve a lemma from his FLT project, or does it only work for lemmas which existed in mathlib when the model was trained (in which case maybe it is possible to have a separate server with its own copy of mathlib since that is all that matters).</p>",
        "id": 407849412,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702520336
    },
    {
        "content": "<p>(Actually, if premise selection is only based on stuff seen during training, then the server doesn't need any access to Mathlib since it was already seen during training.)</p>",
        "id": 407851645,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702521034
    },
    {
        "content": "<p>My experience with retrieval as a user (mostly when I tried <a href=\"https://cursor.sh/\">Cursor</a>) is that you typically need to click a button to refresh the vector index for whatever repository you have open. The speed is actually pretty fast even on a large repository. I could see there being an \"LeanCopilot Server\" VSCode extension that manages this stuff instead of Lean/Mathlib.</p>",
        "id": 407855133,
        "sender_full_name": "llllvvuu",
        "timestamp": 1702522318
    },
    {
        "content": "<p>Decoupled server-client design should make it easier to customize the server side (including the model), no? There should be some protocol that the client and the server both follow, but that's all.</p>\n<p><a href=\"https://leandojo.readthedocs.io/en/latest/user-guide.html#caching\">LeanDojo docs</a> talks about tracing mathlib (or other project) repo and caching the results; they host a number of traced repos on AWS. I'm not sure what the purpose is though; maybe it's just for extraction of training data, because I don't see anything about embedding and storing in a vector database for retrieval.</p>",
        "id": 407855669,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702522517
    },
    {
        "content": "<p>Sorry, I looked now at the types of custom models it supports and they are all text models, so I think I was thinking of a different project (namely <a href=\"https://github.com/leanprover-community/aesop/blob/master/AesopTest/TacGen.lean\">aesop</a>) that could work with non-text-based models that can directly look at the environment.</p>",
        "id": 407855748,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702522550
    },
    {
        "content": "<p>I know this project depends on aesop, so I guess the main parts are aesop, some llm-step stuff for the interface, and the reprover model in the backend.  And I guess the suggestion is to move the reprover model to a separate server.</p>",
        "id": 407855758,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702522555
    },
    {
        "content": "<p>I know of another (unpublished, but soon to be released) project that works like that.  The ITP client sends a whole bunch of goal and environment data to the server to process.  The server keeps a list of vector embeddings for each definition and also handles predicting new tactics.  The client handles the tree search (which I think it would in this case too since aesop handles the search, but it could go the other way as well where the server does the search as in Lean gym).</p>",
        "id": 407855984,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702522632
    },
    {
        "content": "<p>The aesop integration is <a href=\"https://github.com/leanprover-community/aesop/pull/70\">this PR</a>. I think it may well serve as part of the client side.</p>",
        "id": 407856200,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702522719
    },
    {
        "content": "<p>I'm surprised there is still interface code from llm-step. We upstreamed the <code>Try these</code> widget to Std already, but perhaps LeanCopilot is not using that.</p>",
        "id": 407856335,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702522784
    },
    {
        "content": "<p>The word \"embedding\" appears twice in the <a href=\"https://arxiv.org/pdf/2306.15626.pdf\">LeanDojo paper</a>, and they do say it can be precomputed. I agree it's not very useful to keep the mathlib repo around once you have extracted the cache including embeddings.</p>\n<p><a href=\"/user_uploads/3121/kCjEWpbGz70ryc6egfXMtvWl/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/kCjEWpbGz70ryc6egfXMtvWl/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/kCjEWpbGz70ryc6egfXMtvWl/image.png\"></a></div>",
        "id": 407857659,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702523174
    },
    {
        "content": "<p>Even if it is not precomputed, you can just still use the client to send the environment data to the server to compute lemma embeddings or to align with pre-computed embeddings.  (At least that is what the other project I know about does.)</p>",
        "id": 407858166,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702523371
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407793919\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"389019\">Chris Birkbeck</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407216529\">said</a>:</p>\n<blockquote>\n<p>This is probably a stupid question, but how far are we from being able to use something like this to \"clean up\" some code and it get PR ready or close to PR ready? I have a bunch of stuff in a repo I need to PR and while I don't expect something like this to be able to prove the results it contains, it would be nice if it could learn from the proofs that are in the repo and then just tidy it up (i.e.  I say this is the main result I want you to prove, then go, find the proof in the repo, break up into smaller results, remove unused results, fix spacing/layout etc).  Perhaps I'm just way behind and this is already possible, or maybe  this is sci fi and its actually hard to do and we are not there yet.</p>\n</blockquote>\n<p>We think it would be interesting to have something that can learn from your repo (as long as it's public on GitHub and can be built via <code>lake build</code>), automatically fill in <code>sorry</code> and submit pull requests to your repo. I don't see a fundamental difficulty here. It's just that we have limited personnel and have to priortize.</p>\n<p>BTW, I was wondering if \"PR\" here means pull requests or public relations <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>.</p>\n</blockquote>\n<p>Sorry , yes PR was Pull request, to mathlib in this case. Its encouraging that such a thing could maybe be done soon. One of the things that worries me is that there are lots of bits of code out there in repos that people  (including me) haven't had the time to add to mathlib and slowly gets lost. Something like this, even if its not perfect, would really help.</p>",
        "id": 407942374,
        "sender_full_name": "Chris Birkbeck",
        "timestamp": 1702554224
    },
    {
        "content": "<p>OK so if I want to try this out, is it still the case that I can't use it in a project which depends on mathlib master?</p>",
        "id": 408026356,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702581019
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407856335\">said</a>:</p>\n<blockquote>\n<p>I'm surprised there is still interface code from llm-step. We upstreamed the <code>Try these</code> widget to Std already, but perhaps LeanCopilot is not using that.</p>\n</blockquote>\n<p>I'm not aware of it. Can you give me a pointer? Thx!</p>",
        "id": 408036179,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702583969
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408026356\">said</a>:</p>\n<blockquote>\n<p>OK so if I want to try this out, is it still the case that I can't use it in a project which depends on mathlib master?</p>\n</blockquote>\n<p>Lean Copilot only depends on aesop, so you can definitely use it in mathlib or other repos depending on mathlib. <a href=\"https://github.com/yangky11/miniF2F-lean4\">Here</a> is an example of a repo depending on both mathlib and Lean Copilot.</p>",
        "id": 408036430,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702584024
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408036179\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407856335\">said</a>:</p>\n<p>I'm not aware of it. Can you give me a pointer? Thx!</p>\n</blockquote>\n<p>(Sorry, my fault for not updating you on this!)</p>\n<p>See <a href=\"https://github.com/leanprover/std4/blob/main/Std/Tactic/TryThis.lean#L386\">https://github.com/leanprover/std4/blob/main/Std/Tactic/TryThis.lean#L386</a> for the main entry point <code>addSuggestions</code>. This support multiple suggestions, and has customisation hooks for indicating tactics that do or don't succeed.</p>\n<p>Also useful is the <code>suggestion</code> function provided by Mathlib's <code>hint</code> tactic, that takes a piece of syntax representing a tactic invocation, and takes care of running it against the goal, checking if it succeeded, and preparing the <code>Suggestion</code> with appropriate formatting that can be passed to the <code>TryThis</code> widget.</p>\n<p>(To be clear, this is all inspired by llm-step's implementation, just with a bit more engineering work. :-)</p>\n<p>It would be great if this can all be reused.</p>\n<p>(The components that are still in Mathlib have open PRs moving them to Std; if it's helpful let me know and I can hurry those along. :-)</p>",
        "id": 408062701,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702595291
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> ! It's great to see these common frontend utilities are moving to the upstream. We'll re-use it. In addition, do you have a similar function for displaying retrieved premises? For each premise, we have</p>\n<ul>\n<li>fully qualified name</li>\n<li>which module (file) it comes from</li>\n<li>the code for defining it (optional) </li>\n</ul>\n<p>Currently, we simply use the fully qualified name to find its type and doc-string (if available) and use <code>logInfo</code> to print it to the infoview panel. However, it would be great if it can be more interactive, e.g., displaying additional information when the user hovers the mouse over it (similar to hovering over the goals in the infoview panel), or allowing the user to \"go-to definition\". Would that also be useful for the hammer Lean FRO is developing?</p>",
        "id": 408363803,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702749345
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407821098\">said</a>:</p>\n<blockquote>\n<p>But then technically you could be selecting a non-Prop definition as well…</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>  That was the main reason we used the term \"premise\" instead of \"lemma\", though informally (e.g., when explaining it to people coming to our poster), I actually use \"lemma\".</p>",
        "id": 408364126,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702749614
    },
    {
        "content": "<p>I'm not sure that makes it better, people don't call definitions premises either</p>",
        "id": 408364199,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702749664
    },
    {
        "content": "<p>I guess \"constant\" is technically the right term for Lean? But people would be confused.</p>",
        "id": 408364281,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702749718
    },
    {
        "content": "<p>How about we just say \"lemma suggestions\"?</p>",
        "id": 408388206,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702773038
    },
    {
        "content": "<p>We do not have a function for displaying \"lemma suggestions\", but creating one sounds like a great idea. As <span class=\"user-mention\" data-user-id=\"548935\">@Thomas Murrills</span> did such a great job with the <code>Try these</code> widget I wonder if they might be interested? :-)</p>",
        "id": 408388375,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702773128
    },
    {
        "content": "<p>Thanks for thinking of me! You caught me at a good time; I’ve got a few days free! :D I’ll be happy to give it a shot! First I want to think a bit more deeply about the actual use cases of such a widget, and try to understand what information will (or might be) relevant to the user in these cases. I’ll probably ask some questions here as I think through it! :)</p>\n<p>(I also want to consider whether it would make sense to integrate this with <code>Try these</code> or have a standalone widget. The answer might be readily apparent once I think about it…)</p>",
        "id": 408395411,
        "sender_full_name": "Thomas Murrills",
        "timestamp": 1702777517
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> </p>\n<p>I recently came across the <code>mistral-7b</code> model and was surprised by its capability. I ran some initial experiments, including using it to detect whether texts are generated AI model. I found it efficient fine-tuning (fully trained on a TPU in just 2 hours with Lora layers) and impressive performance (0.85 ROC accuracy). </p>\n<p>Has anyone explored using <code>mistral-7b</code> on Leandrojo? That would be an very interesting experiment. <span aria-label=\"grinning face with smiling eyes\" class=\"emoji emoji-1f601\" role=\"img\" title=\"grinning face with smiling eyes\">:grinning_face_with_smiling_eyes:</span></p>",
        "id": 408410967,
        "sender_full_name": "Min-Hsien Weng",
        "timestamp": 1702791105
    },
    {
        "content": "<p>I'm currently trying to install this, and getting the error <code>error: unknown executable </code>«LeanCopilot/download»`; has anyone else come across this?</p>",
        "id": 408451912,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1702828488
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"284160\">Eric Rodriguez</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408451912\">said</a>:</p>\n<blockquote>\n<p>I'm currently trying to install this, and getting the error <code>error: unknown executable \n</code>«LeanCopilot/download»`; has anyone else come across this?</p>\n</blockquote>\n<p>Maybe open an issue on GitHub?</p>",
        "id": 408454347,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702830629
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"638383\">Min-Hsien Weng</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408410967\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> </p>\n<p>I recently came across the <code>mistral-7b</code> model and was surprised by its capability. I ran some initial experiments, including using it to detect whether texts are generated AI model. I found it efficient fine-tuning (fully trained on a TPU in just 2 hours with Lora layers) and impressive performance (0.85 ROC accuracy). </p>\n<p>Has anyone explored using <code>mistral-7b</code> on Leandrojo? That would be an very interesting experiment. <span aria-label=\"grinning face with smiling eyes\" class=\"emoji emoji-1f601\" role=\"img\" title=\"grinning face with smiling eyes\">:grinning_face_with_smiling_eyes:</span></p>\n</blockquote>\n<p>This is something I have always wanted to do but haven't had the chance to do yet :). I probably won't do it in the very near future, but I am interested to see others do it.</p>",
        "id": 408746475,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702956771
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"548935\">Thomas Murrills</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408395411\">said</a>:</p>\n<blockquote>\n<p>Thanks for thinking of me! You caught me at a good time; I’ve got a few days free! :D I’ll be happy to give it a shot! First I want to think a bit more deeply about the actual use cases of such a widget, and try to understand what information will (or might be) relevant to the user in these cases. I’ll probably ask some questions here as I think through it! :)</p>\n<p>(I also want to consider whether it would make sense to integrate this with <code>Try these</code> or have a standalone widget. The answer might be readily apparent once I think about it…)</p>\n</blockquote>\n<p>Thank you! I'm happy to discuss more details when you set out to implement it!</p>",
        "id": 408747310,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702956910
    }
]