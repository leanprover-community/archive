[
    {
        "content": "<p>I gave this talk yesterday at the <a href=\"https://icerm.brown.edu/program/hot_topics_workshop/htw-25-aftwm#section-3\">Autoformalization workshop at ICERM</a>.  (Video will be up in the next few days):</p>\n<ul>\n<li><a href=\"https://docs.google.com/presentation/d/1Eukgx0bzCoKi4bhKOFGUJb9nJNBD0JdYnZliWN7NfnE/edit?usp=sharing\">Slides</a></li>\n<li><a href=\"https://icerm.brown.edu/video_archive/4102\">Video</a> (edit: Apr 29)</li>\n</ul>",
        "id": 514543941,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677577
    },
    {
        "content": "<p>Abstract:</p>\n<blockquote>\n<p>We can’t predict what will happen in AI for math, but we can prepare. Assuming steady progress in capabilities, we could soon see AI that can autoformalize entire mathematical arguments or papers, and AI which can solve previously open mathematical problems. How do we prepare, including recording and benchmark such capabilities? In this talk I will outline my thoughts on why autoformalization is more relevant than ever and propose two specific ways can measure progress. One is a benchmark of easy-to-check autoformalizations of proofs. The other is a public repository of formalized open problems.</p>\n</blockquote>",
        "id": 514543991,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677624
    },
    {
        "content": "<p>I got a really nice and thoughtful response from <span class=\"user-mention\" data-user-id=\"321854\">@Auguste Poiroux</span> regarding things I said in the talk:</p>\n<blockquote>\n<ul>\n<li>\"No automated metrics\": In <a href=\"https://openreview.net/forum?id=hUb2At2DsQ\">Rethinking and Improving Autoformalization</a>, the authors introduced BEq to address this and show that it has a good agreement with human evaluation. The idea is to use the ground truth and symbolic equivalence, with some restrictions, to approximate semantic equivalence. Some other works, like <a href=\"https://arxiv.org/abs/2405.17216\">Autoformalizing Euclidean Geometry</a> also use symbolic equivalence to automatically measure statement autoformalization performance.</li>\n</ul>\n</blockquote>",
        "id": 514544096,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677689
    },
    {
        "content": "<blockquote>\n<ul>\n<li>\"Benchmarks contain a lot of mistakes\": we unfortunately had to face this issue. We have put a great effort in fixing ProofNet, resulting in <a href=\"https://huggingface.co/datasets/PAug/ProofNetSharp\">ProofNet#</a>. Given the history of benchmarks in formal math, I am not going to claim it is completely bug-free, but I believe the amount of errors should be much more acceptable now.</li>\n</ul>\n</blockquote>",
        "id": 514544103,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677695
    },
    {
        "content": "<blockquote>\n<ul>\n<li>\"PutnamBench and MiniF2F have informal proofs\": ProofNet (and ProofNet#) contain informal proofs as well</li>\n</ul>\n</blockquote>",
        "id": 514544111,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677699
    },
    {
        "content": "<blockquote>\n<ul>\n<li>\"Maybe Lean Blueprints are a better benchmark\": While <a href=\"https://github.com/augustepoiroux/RLMEval\">RLMEval</a> is currently focusing on statements in Lean blueprint projects, we are working on extending it to proofs. Your idea of removing lemmas is very interesting.</li>\n</ul>\n</blockquote>",
        "id": 514544113,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677702
    },
    {
        "content": "<p>(I'm posting his comments with his permission.)</p>",
        "id": 514544341,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745677871
    },
    {
        "content": "<p>I think a good source of problems that fits into both categories you mentioned in your talk is : <a href=\"https://www.erdosproblems.com/\">https://www.erdosproblems.com/</a>. There was some discussion a while ago here on zulip about this too, <span class=\"user-mention\" data-user-id=\"458865\">@Thomas Bloom</span> said he would be supportive of such an effort, and has already collected a lot of the metadata you've mentioned.</p>\n<p>What if we just formalize as many of the ~1000 Erdos problems as we can? A good third of them have already been solved (but not formalized) and can be used to test out autoformalization tools. The other two-thirds are unsolved and are generally (?) easily stated. Would people be interested in an open effort to formalize all of these? The main barriers I see (1) potential lack of coverage in Mathlib (2) who would host this effort, maybe the FRO?</p>",
        "id": 514550909,
        "sender_full_name": "George Tsoukalas",
        "timestamp": 1745682385
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"644040\">@George Tsoukalas</span> I know that <span class=\"user-mention\" data-user-id=\"899467\">@Nathan Bowler</span> and Johannes Carmesin are trying to create an initiative of PhD-level solvable problems called <a href=\"user_uploads/3121/epJ-Gl_DW6QTZuc9dtn1mPqo/ProofBench.pdf\">ProofBench</a>. I think that formalizing unsolved statements, such as the Erdos problems, would be great for testing whether we can go beyond this, as a first step into automating mathematics research. I would be happy to help out with this initiative.</p>",
        "id": 514562926,
        "sender_full_name": "Justin Asher",
        "timestamp": 1745691043
    },
    {
        "content": "<p>I thought it best to move the first benchmark proposal which is becoming more of a project to its own topic: <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Hard.20proof-based.20auto-formalization.20benchmark/with/514687585\">#Machine Learning for Theorem Proving &gt; Hard proof-based auto-formalization benchmark</a></p>",
        "id": 514688326,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790891
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"644040\">@George Tsoukalas</span> In my talk, some mentioned Erdos problems as good examples of unsolved problems.  As for the solved ones, how elementary and self contained are the proofs?</p>",
        "id": 514688380,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745790962
    },
    {
        "content": "<p>I'm not really an expert on those areas, I imagine the problem of autoformalizing any paper proving one of the Erdos problems to be similar in expected difficulty of sampling any research maths paper and formalizing it. That being said, there probably are some problems solved via elementary methods which might form an \"easier\" autoformalization subset.</p>",
        "id": 514871058,
        "sender_full_name": "George Tsoukalas",
        "timestamp": 1745862281
    },
    {
        "content": "<p>I'm not sure about this claim that erdos problem papers are no easier [clarification: to autoformalize] than general papers. A general paper in my area may well have the property that literally no theorem in the paper can even be stated in lean in its current state.</p>",
        "id": 514879718,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1745865126
    },
    {
        "content": "<p>There is a huge range in terms of how elementary and self-contained the proofs are. Some solutions are the culmination of decades of work and many successive papers, and so formalising these would be a huge effort (although none quite as large as FLT). Some are essentially one-line 'tricks', sometimes so simple that it's surprising Erdos didn't notice it himself.</p>",
        "id": 514881223,
        "sender_full_name": "Thomas Bloom",
        "timestamp": 1745865650
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Talk.3A.20Preparing.20for.20the.20next.20stage.20in.20autoformalization/near/514879718\">said</a>:</p>\n<blockquote>\n<p>I'm not sure about this claim that erdos problem papers are no easier than general papers. A general paper in my area may well have the property that literally no theorem in the paper can even be stated in lean in its current state.</p>\n</blockquote>\n<p>This is why I included \"expected difficulty\" but I probably should've refrained from estimating it altogether as I'm not very familiar with the solutions</p>",
        "id": 514881545,
        "sender_full_name": "George Tsoukalas",
        "timestamp": 1745865770
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"644040\">George Tsoukalas</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Talk.3A.20Preparing.20for.20the.20next.20stage.20in.20autoformalization/near/514550909\">said</a>:</p>\n<blockquote>\n<p>I think a good source of problems that fits into both categories you mentioned in your talk is : <a href=\"https://www.erdosproblems.com/\">https://www.erdosproblems.com/</a>. There was some discussion a while ago here on zulip about this too, <span class=\"user-mention silent\" data-user-id=\"458865\">Thomas Bloom</span> said he would be supportive of such an effort, and has already collected a lot of the metadata you've mentioned.</p>\n</blockquote>\n<p>I would indeed, although I recently heard from <span class=\"user-mention\" data-user-id=\"471117\">@Thomas Hubert</span>  that the team at AlphaProof have already been formalising many of the problems on the website, so perhaps worth checking with that team first!</p>",
        "id": 514881561,
        "sender_full_name": "Thomas Bloom",
        "timestamp": 1745865774
    },
    {
        "content": "<p>Thank you for the mention.</p>\n<p>We are close to be able to share more details in this direction! Please bear with us a few more weeks :)</p>",
        "id": 515017029,
        "sender_full_name": "Thomas Hubert",
        "timestamp": 1745920976
    },
    {
        "content": "<p>For those interested, the video recording of Jason’s talk, <em><a href=\"https://icerm.brown.edu/video_archive/4102\">Preparing for the Next Stage in Autoformalization</a></em>, is now publicly available.</p>",
        "id": 515169502,
        "sender_full_name": "Pietro Monticone",
        "timestamp": 1745966358
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"556875\">@Pietro Monticone</span>!  I added it to the top of this thread as well.</p>",
        "id": 515172730,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745968211
    },
    {
        "content": "<p>I'm not sure if the Erdos problems that <span class=\"user-mention\" data-user-id=\"471117\">@Thomas Hubert</span> is referring to are solved or unsolved.  If solved, I don't think they would make a great benchmark of the type I have in mind (but maybe a good training set), because they are famous so folks may be trying to formalize the proofs.  If unsolved, they could be a good start to a repo of formally stated unsolved problems.  But either way, I'm looking forward to seeing them!</p>",
        "id": 515189562,
        "sender_full_name": "Jason Rute",
        "timestamp": 1745978680
    },
    {
        "content": "<p>FYI, <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> started <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Hard.20proof-based.20auto-formalization.20benchmark/with/514687585\">#Machine Learning for Theorem Proving &gt; Hard proof-based auto-formalization benchmark</a> for discussion of the first benchmark he proposed in this talk.</p>",
        "id": 515581325,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1746133280
    },
    {
        "content": "<p>As interesting as that benchmark proposal is, I am actually even more interested in the proposal from later in the talk about unsolved problems. I have actually been thinking about something similar, and I've posted problems I can't personally solve to some of the <a href=\"https://theorem-marketplace.com/bounties\">bounty</a> <a href=\"http://www.codeproofarena.com:8000/challenges\">websites</a> that have popped up in the last few months. But I worry those websites have a form factor which is too radically different from what the community is used to, and I think something that looks more like a GitHub repo could be a better way of doing this. Jason, is there a plan to create another thread to discuss that proposal?</p>",
        "id": 515581334,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1746133285
    },
    {
        "content": "<p>I don’t have a concrete plan.  I think there will be a lot of challenges, but also a lot of benefits.  The top challenge is correctly formalizing a conjecture in Lean.  The other challenge is just maintaining it, and what to do when someone claims a solution (or partial solution).</p>",
        "id": 515617070,
        "sender_full_name": "Jason Rute",
        "timestamp": 1746150328
    }
]