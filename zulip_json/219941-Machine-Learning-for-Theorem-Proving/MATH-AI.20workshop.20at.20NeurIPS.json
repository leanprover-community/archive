[
    {
        "content": "<p>There will be a workshop about AI and math at NeurIPS on December 3 in New Orleans: <a href=\"https://mathai2022.github.io/\">https://mathai2022.github.io/</a><br>\nIt looks similar to the MATH-AI workshop that took place at ICLR. There is no list of papers on the website yet.</p>",
        "id": 306413484,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1666864954
    },
    {
        "content": "<p>By the way looks like they have a list now: <a href=\"https://mathai2022.github.io/papers/\">https://mathai2022.github.io/papers/</a></p>",
        "id": 311920225,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669245222
    },
    {
        "content": "<p>It looks like <a href=\"https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/MATH-AI&amp;referrer=%5BHomepage%5D(%2F)\">the papers</a> for MATH-AI 2023 recently dropped on OpenReview.  The <a href=\"https://mathai2023.github.io/papers/\">official website</a> hasn’t been updated yet.</p>",
        "id": 399521259,
        "sender_full_name": "Jason Rute",
        "timestamp": 1698758890
    },
    {
        "content": "<p>(I’m stealing the thread from the 2022 workshop, but this year it is still part of NeurIPS.)</p>",
        "id": 399521475,
        "sender_full_name": "Jason Rute",
        "timestamp": 1698758960
    },
    {
        "content": "<p>I have checked it a few days ago as well, but it seems that there is still no pdf yet (only abstract)<br>\n<a href=\"/user_uploads/3121/AyvCHTF3_rbB6i8DLVyCT1RC/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/AyvCHTF3_rbB6i8DLVyCT1RC/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/AyvCHTF3_rbB6i8DLVyCT1RC/image.png\"></a></div>",
        "id": 399618497,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1698793910
    },
    {
        "content": "<p>Yeah, I think I jumped the gun.  I should have waited until it was on the website including pdfs.</p>",
        "id": 399619187,
        "sender_full_name": "Jason Rute",
        "timestamp": 1698794392
    },
    {
        "content": "<p>Anyway looking forward to it!</p>",
        "id": 399619452,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1698794573
    },
    {
        "content": "<p>Hi all,</p>\n<p>Has anyone here attended a similar NeurIPS workshop in the past? If so, what format are they hosted in? Are they a series of short lectures or more of an open discussion/networking session, or something else entirely? I'd love to attend this year's, but as an undergrad in Australia I'd need to fund my own way there (not cheap for a student <span aria-label=\"sob\" class=\"emoji emoji-1f62d\" role=\"img\" title=\"sob\">:sob:</span>).</p>\n<p>If anyone has any experience from previous workshops/NeurIPS events, I'd greatly appreciate any advice or comments on what it was like - I heard that some NeurIPS events are streamed online as well, but if this workshop isn't (which seems to be the case as far as I can tell) or if people suggest it's more worthwhile to come in person, then I'll see if I can't find an academic at my uni who'll be able to reimburse the travel costs on the uni's behalf for me.</p>\n<p>Thanks!</p>",
        "id": 399915897,
        "sender_full_name": "Michal Novomestsky",
        "timestamp": 1698925116
    },
    {
        "content": "<p>I have attended and organized such workshops, and the format depends a lot on what the organizers want, but most often the program contains talks by invited speakers, talks by some of the authors who contributed papers, and poster sessions. See for example the program of the same workshop last year: <a href=\"https://mathai2022.github.io/schedule/\">https://mathai2022.github.io/schedule/</a> .</p>",
        "id": 399919086,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1698926214
    },
    {
        "content": "<p>I haven't checked with NeurIPS. From past experience, the talks will be live-streamed to virtual NeurIPS attendees during the conference, and the recordings will be available later to the public. The poster sessions will be entirely in-person.</p>",
        "id": 399943872,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1698933898
    },
    {
        "content": "<p>Hey Remy and Kaiyu,</p>\n<p>Thanks for the details! The fact that it's livestreamed is a huge help <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 400120903,
        "sender_full_name": "Michal Novomestsky",
        "timestamp": 1699013359
    },
    {
        "content": "<p>I don’t know when it was updated but it looks like the <a href=\"https://mathai2023.github.io/papers/\">paper list</a> for MathAI was finally uploaded.  Lots of interesting looking papers!</p>",
        "id": 403880144,
        "sender_full_name": "Jason Rute",
        "timestamp": 1700791952
    },
    {
        "content": "<p>There is a mix-up in the official website's list of <a href=\"https://mathai2023.github.io/papers\">papers</a>, where the <del>author details and</del> link for paper 29 titled \"Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts\" points to the fifth paper (Chameleon). The correct version is available on arXiv at <a href=\"https://arxiv.org/pdf/2310.14628.pdf\">2310.14628.pdf</a>.  May I report the issue here?</p>\n<hr>\n<p>Additionally, the link to the first paper(TinyGSM) has gone missing, and I can't find one online.</p>",
        "id": 404448331,
        "sender_full_name": "RexWang",
        "timestamp": 1701102740
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"572535\">@RexWang</span> I would reach out the the organizers (although many of them are on this Zulip so you could DM them).</p>",
        "id": 404454912,
        "sender_full_name": "Jason Rute",
        "timestamp": 1701104867
    },
    {
        "content": "<p>I have briefly read the abstracts of the listed papers, among which 7 are related to formalization. Excluding Paper No. 22, <a href=\"https://mathai2023.github.io/papers/28.pdf\">lemur</a>, which discusses formal program verification, there are 6 papers in this category. Several of these have already been discussed in the lean zulip forum.</p>\n<ol>\n<li>\n<p>llmstep: The paper <a href=\"https://mathai2023.github.io/papers/40.pdf\">LLM proofstep suggestions in Lean</a>, with repository at <a href=\"https://github.com/wellecks/llmstep\">wellecks/llmstep</a> and the community post <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep\">llmstep</a>. The work is based on LeanDojo and provides practical tools, as well as releasing code for fine-tuning and evaluation.</p>\n</li>\n<li>\n<p>COPRA: The paper <a href=\"https://mathai2023.github.io/papers/29.pdf\">A Language-Agent Approach to Formal Theorem-Proving</a>, with repository at <a href=\"https://github.com/trishullab/copra\">trishullab/copra</a>, the community post <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/COPRA\">COPRA</a> and an openreview <a href=\"https://openreview.net/forum?id=XCMbagV0No\">here</a>. This paper has also been submitted to this year's ICLR. </p>\n</li>\n<li>\n<p>The paper <a href=\"https://mathai2023.github.io/papers/25.pdf\">Temperature-scaled large language models for Lean proofstep prediction</a> by Meta. It appears that no code has been released, and the core idea is \"<em>temperature scaling as a regularization method for multi-epoch training on small datasets.</em>\"</p>\n</li>\n<li>\n<p>Magnushammer: The paper <a href=\"https://mathai2023.github.io/papers/23.pdf\">A Transformer-Based Approach to Premise Selection</a> with dataset: <a href=\"https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle\">premise_selection_in_isabelle</a>, and the community post: <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.3A.20Magnushammer\">New paper: Magnushammer</a>. This work focuses on premise selection methods, further improving upon Sledgehammer.</p>\n</li>\n<li>\n<p>LeanCopilot: The paper <a href=\"https://mathai2023.github.io/papers/4.pdf\">Towards Large Language Models as Copilots for Theorem Proving in Lean</a>. Similar to llmstep, this can be highly useful for Lean users, but it seems that the code has not been made publicly available.</p>\n</li>\n<li>\n<p>The paper <a href=\"https://mathai2023.github.io/papers/19.pdf\">LLM vs ITP</a> and the website: <a href=\"https://www.snfrieder.org/\">Simon Frieder</a>. This work references <a href=\"https://www.cs.ru.nl/~freek/100/\">Formalizing 100 Theorems</a> and has released the GHOSTS dataset: <a href=\"https://github.com/friederrr/GHOSTS\">friederrr/GHOSTS</a> and LLMKnow. However, the link for \"LLMKNOW\" at <a href=\"https://llmknow.friederrr.org\">https://llmknow.friederrr.org</a> is not accessible (currently indicating \"coming soon\"). The study discusses \"<em>analyzing whether the knowledge contained in LLMs matches that encoded in ITPs</em>\", which is quite interesting; this essentially explores evidence that LLMs could be used for ITP.</p>\n</li>\n</ol>",
        "id": 404902329,
        "sender_full_name": "RexWang",
        "timestamp": 1701274562
    },
    {
        "content": "<p>I would also include <a href=\"https://mathai2023.github.io/papers/45.pdf\">Llemma</a>, which while mostly about natural language math, does have a couple experiments about formal math in Lean and Isabelle, and they put a lot of work into making sure it had good coverage of ITP data (both at the level of code and also proofstate-tactic pairs).</p>",
        "id": 404916070,
        "sender_full_name": "Jason Rute",
        "timestamp": 1701278717
    },
    {
        "content": "<p>Note, this is just a workshop, so these papers are ridiculously short, with most of the details in the appendix.  These usually will not be the only version.  Many have fuller length versions on arXiv or submitted to conferences.</p>",
        "id": 404917125,
        "sender_full_name": "Jason Rute",
        "timestamp": 1701278995
    },
    {
        "content": "<p>Some of these have already been talked about here <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a>, but others haven’t and probably should be.</p>",
        "id": 404917157,
        "sender_full_name": "Jason Rute",
        "timestamp": 1701279002
    },
    {
        "content": "<p>Also most of the authors of the above papers are on Zulip, and if they haven’t already, should probably share more details about their work. :)</p>",
        "id": 404917427,
        "sender_full_name": "Jason Rute",
        "timestamp": 1701279102
    },
    {
        "content": "<p>I notice <span class=\"user-mention\" data-user-id=\"572535\">@RexWang</span>  mentioned one of these was submitted to ICLR, but actually there are a number of these (and more about ITP) currently being reviewed for ICLR 2024.  Go to <a href=\"https://openreview.net/group?id=ICLR.cc/2024/Conference\">https://openreview.net/group?id=ICLR.cc/2024/Conference</a> , select the active submissions tab, and search for “theorem proving” (or more specific keywords like “autoformalization”, “Lean”, “Coq”, or “Isabelle”.)</p>",
        "id": 404918608,
        "sender_full_name": "Jason Rute",
        "timestamp": 1701279464
    },
    {
        "content": "<p>Thanks a lot for the suggestions!! <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> <span aria-label=\"grinning\" class=\"emoji emoji-1f600\" role=\"img\" title=\"grinning\">:grinning:</span> Indeed, I should not have neglected the paper llemma. I will give more thoughts and feedback after delving further into them.</p>",
        "id": 404922103,
        "sender_full_name": "RexWang",
        "timestamp": 1701280789
    },
    {
        "content": "<p>Who here will attend NeurIPS this week? Answer with <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span> to indicate that you will be there!</p>\n<p>I am starting the long trip from France to New Orleans.</p>",
        "id": 407018969,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1702191023
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"258175\">@Albert Jiang</span> <a href=\"https://x.com/AlbertQJiang/status/1735707606063128634\">live tweeted the MATH-AI panel</a>, featuring Sir Timothy Gowers, Talia Ringer, and Tony Wu.</p>",
        "id": 408301940,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702701608
    },
    {
        "content": "<p>Yesterday I bumped into someone who attended Neurips  (Sara Veneziale, who presented on her work doing algebraic geometry inspired by neural networks) and she expressed surprise that so many talks mentioned lean \"even though there were hardly any mathematicians there\" :-)</p>",
        "id": 408322390,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702717597
    },
    {
        "content": "<p>I think Lean for whatever reason (I have a few speculative thoughts as to why) has become the go to place for neural theorem proving research, with Isabelle in second place, and Coq in third.</p>",
        "id": 408349116,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702737036
    },
    {
        "content": "<p>I <em>thiink</em> the Coq tools that predate the LLM proving are still SOTA though, hard to tell.</p>",
        "id": 408368662,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702753486
    },
    {
        "content": "<p>My impression is that the new lean+llm tools are still playing catch up, happy to be corrected though.</p>",
        "id": 408369092,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702753830
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"659851\">Alex Sanchez-Stern</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408368662\">said</a>:</p>\n<blockquote>\n<p>I <em>thiink</em> the Coq tools that predate the LLM proving are still SOTA though, hard to tell.</p>\n</blockquote>\n<p>My belief is quite the opposite <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>. It also depends on the evaluation strategy. If I make the walltime limit small enough, <code>tidy</code> should outperform both of them?</p>",
        "id": 408369820,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702754601
    },
    {
        "content": "<p>What papers from the broader NeurIPS conference/workshops (other than math-ai) did people find potentially useful for theorem proving? I found the tree of thought paper (<a href=\"https://arxiv.org/abs/2305.10601\">https://arxiv.org/abs/2305.10601</a>)  promising for improving the backtracking tree searches in theorem proving: specifically, the idea of the LLM reflecting and self-critiquing its various proof strategies+steps and THEN picking the most promising next strategy+step</p>",
        "id": 408402307,
        "sender_full_name": "Abhishek Anand",
        "timestamp": 1702783240
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408369820\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"659851\">Alex Sanchez-Stern</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408368662\">said</a>:</p>\n<blockquote>\n<p>I <em>thiink</em> the Coq tools that predate the LLM proving are still SOTA though, hard to tell.</p>\n</blockquote>\n<p>My belief is quite the opposite <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>. It also depends on the evaluation strategy. If I make the walltime limit small enough, <code>tidy</code> should outperform both of them?</p>\n</blockquote>\n<p>I'd love to revisit this conversation in a week... :)</p>",
        "id": 408407716,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702788356
    },
    {
        "content": "<blockquote>\n<p>I'd love to revisit this conversation in a week... :)</p>\n</blockquote>\n<p>Are you preparing a manuscript \"<code>tidy</code> is all you need\"?</p>",
        "id": 408409189,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702789317
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408349116\">said</a>:</p>\n<blockquote>\n<p>I think Lean for whatever reason (I have a few speculative thoughts as to why) has become the go to place for neural theorem proving research, with Isabelle in second place, and Coq in third.</p>\n</blockquote>\n<p>The very fact that this discussion is so active on the Lean zulip and not the other two is very telling.</p>",
        "id": 408475539,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1702842432
    },
    {
        "content": "<p>I know the history of that since I started this stream.  I was having some private discussions with Brando Miranda, Jesse Michael Han, and Stan Polu here on Zulip as well as a long thread on the general channel (which was mostly an argument between me and Tim Daly).  Then I met Markus Rabe at Lean Together in Jan 2020.  We generally agreed it would be good to have a dedicated place to discuss this stuff.  While at that point none of the work was done in Lean, it seemed good anyway to start a stream here and everyone agreed.  Many of the early discussions were about HOList actually.  (It still surprises me that even with the popularity of Isabelle for AI research for theorem proving, that there is almost no discussion on the Isabelle Zulip.)</p>",
        "id": 408477242,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702843805
    },
    {
        "content": "<p>There has started to be more discussion on the Coq Zulip.</p>",
        "id": 408477467,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702844026
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408477242\">said</a>:</p>\n<blockquote>\n<p>I know the history of that since I started this stream.  I was having some private discussions with Brando Miranda, Jesse Michael Han, and Stan Polu here on Zulip as well as a long thread on the general channel (which was mostly an argument between me and Tim Daly).  Then I met Markus Rabe at Lean Together in Jan 2020.  We generally agreed it would be good to have a dedicated place to discuss this stuff.  While at that point none of the work was done in Lean, it seemed good anyway to start a stream here and everyone agreed.  Many of the early discussions were about HOList actually.  (It still surprises me that even with the popularity of Isabelle for AI research for theorem proving, that there is almost no discussion on the Isabelle Zulip.)</p>\n</blockquote>\n<p>After an initial setup, I guess there isn't much difference between the ML model for different ITPs -- models are not instructed to tell the difference between simple type theory and dependent type theory. In this case, it does not matter which Zulip to have those discussions. Also, I believe <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> here  has done an excellent job of compiling progress in this area and moderating the discussions :-)</p>",
        "id": 408593627,
        "sender_full_name": "Wenda Li",
        "timestamp": 1702900064
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408369820\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"659851\">Alex Sanchez-Stern</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408368662\">said</a>:</p>\n<blockquote>\n<p>I <em>thiink</em> the Coq tools that predate the LLM proving are still SOTA though, hard to tell.</p>\n</blockquote>\n<p>My belief is quite the opposite <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>. It also depends on the evaluation strategy. If I make the walltime limit small enough, <code>tidy</code> should outperform both of them?</p>\n</blockquote>\n<p>Hmm, interesting. What evaluation strategy do you think benefits one versus the other? It seems like in terms of number of predictions/model queries a tool like COPRA might win out, but if you measure total time needed to complete the proofs Proverbot9001 and Diva are still ahead, since their predictions are orders of magnitude faster.</p>",
        "id": 408658046,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702921061
    },
    {
        "content": "<p>Unless the eval times for the newer tools are particularly short, I don't think they win out on speed; the proverbot9001 evaluation runs in a few hours on university resources. And in terms of total numbers, they seem to be behind (although again, we only have transitive evidence so far through COPRA, since the benchmarks are different).</p>",
        "id": 408659235,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702921485
    },
    {
        "content": "<p>Maybe there's some eval evidence I haven't seen?</p>",
        "id": 408668297,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702924944
    },
    {
        "content": "<p>I think you two aren’t disagreeing as much as you think.  Of the models which predict tactic steps and do a tree search, there is a trade off between per-step prediction strength and per-step speed.  Roughly heuristics &gt; kNN models &gt; tree models &gt; graph models &gt; transformers &gt; large language models when it comes to speed.  Hopefully it would be the opposite when it comes to per prediction usefulness.  It seems that if the wall clock time was 1 hours and one had unlimited access to GPT-4 then a model like COPRA might do quite well since each step is really good even if it takes a long time (and a lot of money!) to run.  On the fast end, if you just have 1/10 of a second on a laptop then a fast heuristic-based model like aesop might be the best option.  The question is what is the current balance of time per prediction strength (and resources/money) and that can only be determined by experimentation.  It isn’t obvious.  (Hopefully we’ll release something in a few days which gives a bit more data in this direction, but again more experiments are likely needed.)</p>",
        "id": 408677291,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702928857
    },
    {
        "content": "<p>Hmm, that could prove to be the case. But right now as far as I can tell, the evidence points to the conclusion that, for <em>any</em> particular time limit, Proverbot9001 would prove more theorems than, say, ReProver or COPRA. Is there a time scale in particular that you think the lean tools would perform better at than the Coq tools?</p>",
        "id": 408679787,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702929827
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408677291\">said</a>:</p>\n<blockquote>\n<p>It seems that if the wall clock time was 1 hours and one had unlimited access to GPT-4 then a model like COPRA might do quite well since each step is really good even if it takes a long time (and a lot of money!) to run. </p>\n</blockquote>\n<p>It looks to me like COPRA was given <em>much</em> longer than an hour to run on it's compcert subset. Did you mean a (single-threaded) hour per-proof? In that case Proverbot90001 would have about 4x as much time as in it's normal eval, so it's unlikely that COPRA would reach it's success levels. But one would have to do the eval to be sure.</p>",
        "id": 408680887,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702930346
    },
    {
        "content": "<p>And then once you consider the hardware differences between using Open-AI's advanced hardware and few university GPU's the gap widens.</p>",
        "id": 408681043,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702930436
    },
    {
        "content": "<p>Yes, I meant one hour per proof.</p>",
        "id": 408681213,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702930514
    },
    {
        "content": "<p>And that Copra change some of its hyper parameters to account for this like returning more suggestions.  Similarly proverbot9001 could change its hyper parameters and use more hardware.  It was just a thought experiment to prove a point.</p>",
        "id": 408681457,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702930645
    },
    {
        "content": "<p>Yeah it's unclear, since you hit diminishing returns pretty quickly. But even under that thought experiment it still looks like Proverbot9001 would prove more theorems. If we allow Proverbot9001 to use 64 threads to compensate for the hardware differences in OpenAI, then giving it an hour per proof would correspond to a 500x increase in time budget (the first 500 proofs finish in an hour, there's a long tail of 8 proofs that take another hour or two), but only a 10x increase in time budget for COPRA (the paper reports an average time of about 6 minutes per-proof, so being generous we could call that the max).</p>",
        "id": 408682311,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702931047
    },
    {
        "content": "<p>Do you think that the non-LLM based ML approaches of Proverbot9001 would have done as well as ChatGPT in the lemur work in coming up with code/loop invariants? <a href=\"https://arxiv.org/pdf/2310.04870.pdf\">https://arxiv.org/pdf/2310.04870.pdf</a></p>",
        "id": 408682744,
        "sender_full_name": "Abhishek Anand",
        "timestamp": 1702931260
    },
    {
        "content": "<p>Hmm not sure, I haven't tried loop invariant generation, it's a pretty different task. But i noticed that paper doesn't compare to any of the classic invariant generation tools like Daikon (from the 90's), not sure how well it performs in that context.</p>",
        "id": 408683548,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702931650
    },
    {
        "content": "<p>IIUC, in Daikon, you need to run your program many times to discover invariants. Lemur does it without running the program. But I agree that could have been a good comparison.</p>",
        "id": 408684075,
        "sender_full_name": "Abhishek Anand",
        "timestamp": 1702931955
    },
    {
        "content": "<p>Hmm that's a fair point. But they didn't do a quantitative comparison to <em>any</em> prior tools, there are certainly more than a few prior loop invariant generation tools that don't require running the program.</p>",
        "id": 408684393,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1702932147
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"659851\">Alex Sanchez-Stern</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/MATH-AI.20workshop.20at.20NeurIPS/near/408679787\">said</a>:</p>\n<blockquote>\n<p>Hmm, that could prove to be the case. But right now as far as I can tell, the evidence points to the conclusion that, for <em>any</em> particular time limit, Proverbot9001 would prove more theorems than, say, ReProver or COPRA. </p>\n</blockquote>\n<p>What evidence are you referring to? The experiments in COPRA?</p>",
        "id": 408748863,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702957170
    },
    {
        "content": "<p>Yeah, that seems to be the only comparison point</p>",
        "id": 408991563,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1703009754
    }
]