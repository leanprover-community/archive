[
    {
        "content": "<p><code>sagredo</code> is a conversational tactic, which runs an automatic loop of GPT suggesting proof steps, and Lean providing feedback on errors and remaining goals.</p>\n<p>If anyone would like to play with it, I've pushed our branches <code>sagredo</code> and <code>sagredo-widget</code> to the main repository. This is joint work with (so far!) <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span>, <span class=\"user-mention\" data-user-id=\"128280\">@Wojciech Nawrocki</span>, and <span class=\"user-mention\" data-user-id=\"483794\">@Zach Battleman</span>.</p>",
        "id": 348135847,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681103077
    },
    {
        "content": "<p>The <code>sagredo</code> branch doesn't have a widget frontend, so you have to wait for the entire conversation to play out before you see it. However it is more or less mathlib-friendly, so could be deployed for everyone quickly. The <code>sagredo-widget</code> branch has a nice frontend, but at the cost of some extra installation steps:</p>\n<ul>\n<li><code>cd widget &amp;&amp; npm install &amp;&amp; npm run build</code></li>\n<li>olean caching seems to be broken, so you may have to rebuild mathlib to play with it.</li>\n</ul>\n<p>The three main files (these will likely move around) are <code>Mathlib.Tactic.GPT.Sagredo.{Monad,Dialog,Widget}</code>, and provde tactics <code>sagredo</code> (non-interactive) and <code>sagredo!</code> (interactive).</p>",
        "id": 348135866,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681103088
    },
    {
        "content": "<p>There is obviously a lot of work that could be done on it in several directions. The first priority, however, would be to make a proper test suite that reports which problems it succeeds on, and in how many steps, so that changes to prompting and feedback can be evaluated. There are a handful of examples in <code>test/Sagredo</code>, but it's probably more fun to make things up yourself!</p>\n<p>It's worth noting that GPT struggles with Lean 4 syntax. We get a lot of mileage out of careful prompting (our system message explains some of the syntax differences), but it would just be a lot better if it had seen more Lean 4. Maybe we don't have to wait long, who knows!</p>",
        "id": 348135914,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681103107
    },
    {
        "content": "<p>There's a demo video at <a href=\"https://youtu.be/CEwRMT0GpKo\">https://youtu.be/CEwRMT0GpKo</a>, which includes some fun examples; it may be easiest to start there.</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"CEwRMT0GpKo\" href=\"https://youtu.be/CEwRMT0GpKo\"><img src=\"https://uploads.zulipusercontent.net/334dd3177945ef6955a1e621087a23617632651b/68747470733a2f2f692e7974696d672e636f6d2f76692f434577524d543047704b6f2f64656661756c742e6a7067\"></a></div>",
        "id": 348136005,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681103128
    },
    {
        "content": "<p>Fascinating! Great work Scott, Zhangir, Wojciech and Zach!</p>\n<p>I'm really curious whether GPT4 would improve a lot if it could be trained on such interactive sessions. Because otherwise it has to learn from polished end results. And my uninformed intuition says that it could learn a lot more and a lot faster if it could do such interactive training runs.</p>",
        "id": 348148972,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1681110116
    },
    {
        "content": "<p>you don't really \"train\" GPT4 though</p>",
        "id": 348149351,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1681110262
    },
    {
        "content": "<p>actual training happens behind closed doors and on the entire internet</p>",
        "id": 348149383,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1681110281
    },
    {
        "content": "<p>all you can do is push it in various directions using prompts</p>",
        "id": 348149413,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1681110301
    },
    {
        "content": "<p>I know. But I'm still curious if it would make a significant difference.</p>",
        "id": 348149983,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1681110585
    },
    {
        "content": "<p>I'm saying that it is unlikely we can do this experiment unless we have an OpenAI insider</p>",
        "id": 348150858,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1681111040
    },
    {
        "content": "<p>Yep. An alternative experiment would be to try LLaMa as backend. Then we do our own training on mathlib4, and also do such \"interactive-session\" training. But I have no idea if LLaMa is good enough, quality-wise.</p>",
        "id": 348151382,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1681111293
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/sagredo/near/348135914\">said</a>:</p>\n<blockquote>\n<p>It's worth noting that GPT struggles with Lean 4 syntax. We get a lot of mileage out of careful prompting (our system message explains some of the syntax differences), but it would just be a lot better if it had seen more Lean 4. Maybe we don't have to wait long, who knows!</p>\n</blockquote>\n<p>I wonder if we could help it out here in the long run by teaching GitHub that lean4 is a different language to lean 3</p>",
        "id": 348155651,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1681112930
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/sagredo/near/348148972\">said</a>:</p>\n<blockquote>\n<p>I'm really curious whether GPT4 would improve a lot if it could be trained on such interactive sessions. Because otherwise it has to learn from polished end results. And my uninformed intuition says that it could learn a lot more and a lot faster if it could do such interactive training runs.</p>\n</blockquote>\n<p>I would look at Baldur ( <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Baldur.3A.20Whole-Proof.20Generation.20and.20Repair.20with.20Large.20Lang.2E.2E.2E\">#Machine Learning for Theorem Proving &gt; Baldur: Whole-Proof Generation and Repair with Large Lang...</a>) which uses a LLM comparable to the GPT-3 family and fine tunes it on proof repair (which is basically a form of what you are saying about interactive runs).  I’ve been meaning to write a review of that paper.</p>",
        "id": 348182191,
        "sender_full_name": "Jason Rute",
        "timestamp": 1681122606
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> I haven’t finished your video yet, so maybe this is answered there, but are you using the long context or short context version of GPT-4?  If the long context version, do you find the additional context length helps, for say adding more examples in the prompt, or other stuff?  Actually, are your prompts in the code?  I’d love to see them if they are publicly available.</p>",
        "id": 348190639,
        "sender_full_name": "Jason Rute",
        "timestamp": 1681125525
    },
    {
        "content": "<p>I also look forward to the benchmarks when you have them.  Benchmarks on ported Mathlib4 as well as miniF2F would be interesting (even though one has to consider there is a high amount of data contamination of Mathlib since it is likely used as training data for GPT-4, and there is likely some miniF2F contamination as well, but not as much).</p>",
        "id": 348191082,
        "sender_full_name": "Jason Rute",
        "timestamp": 1681125708
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>, it just uses the 8k version at present, although I was thinking to switch over. The second example in the video actually hits the limit, so yes, a longer context length would be helpful. (One could also switch dynamically mid-conversation, or alternatively just drop the early parts of the conversation, keeping the system message.)</p>\n<p>And yes, the prompting is all in the branch. It is interspersed with Lean code, because the point is to pass back error messages and goals in useful ways.</p>",
        "id": 348191083,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1681125709
    },
    {
        "content": "<p>But (referring to benchmark) the best benchmark is always if Lean users find this useful, so the fact that it is already available is a nice start!</p>",
        "id": 348191428,
        "sender_full_name": "Jason Rute",
        "timestamp": 1681125834
    },
    {
        "content": "<p>I tried <code>sagredo</code> last week and spent $0.07 over the course of two or three tactic executions. <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 348195712,
        "sender_full_name": "David Renshaw",
        "timestamp": 1681127109
    },
    {
        "content": "<p>Thanks for releasing this! It's very cool, if kinda costly. (Welp, I just now spent another $1.20 on openai API calls.)</p>",
        "id": 348206168,
        "sender_full_name": "David Renshaw",
        "timestamp": 1681130122
    },
    {
        "content": "<p>Has openai started letting people use the 32k model?</p>",
        "id": 348219620,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1681134068
    },
    {
        "content": "<p>It's in their pricing</p>",
        "id": 348219677,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1681134086
    },
    {
        "content": "<p>I still only have 8k access. And I tried calling the 32k api yesterday with no success</p>",
        "id": 348219809,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1681134121
    },
    {
        "content": "<p>Pay to Lean :)</p>",
        "id": 348235334,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1681138011
    },
    {
        "content": "<p>Yeah, it seems like we will all have a \"api fees\" item in the budget sections of our future grant proposals.</p>",
        "id": 348237459,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1681138566
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/sagredo/near/348151382\">said</a>:</p>\n<blockquote>\n<p>Yep. An alternative experiment would be to try LLaMa as backend. Then we do our own training on mathlib4, and also do such \"interactive-session\" training. But I have no idea if LLaMa is good enough, quality-wise.</p>\n</blockquote>\n<p>The most popular LLaMA derivative model at the moment is probably <a href=\"https://github.com/lm-sys/FastChat\">Vicuna</a>, and <a href=\"https://github.com/nomic-ai/gpt4all\">GPT4All</a> is probably next. Since our goal is to generate Lean code, we may look at how <a href=\"https://github.com/sahil280114/codealpaca#readme\">Code Alpaca</a> does fine-tuning; <a href=\"https://github.com/tloen/alpaca-lora#readme\">LoRA fine-tuning</a> is available too. Fine-tuning <a href=\"https://platform.openai.com/docs/guides/chat/is-fine-tuning-available-for-gpt-3-5-turbo\">isn't yet available</a> for GPT-4 and GPT-3.5-turbo models, unfortunately, and when it becomes available you'd still need to pay OpenAI. Quantization + <a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> etc. allow LLaMA models be run on consumer hardware, even CPU only.</p>",
        "id": 348237897,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1681138676
    },
    {
        "content": "<p>Vicuna claims to achieve 90% ChatGPT performance, but that would still be a lot below GPT-4. With fine-tuning though, it's conceivable that it could outperform GPT-4 on Lean 4 code writing, especially if we can gather usage data from all Lean users. (Maybe upload the whole log each time <code>sagredo</code> is called, or store it locally so that users can choose to contribute.) It's common to train smaller open models on conversations generated by GPT-4 or ChatGPT (e.g. from ShareGPT) to boost their performance; even Google did that.</p>\n<p>It may also be helpful to extend the <code>sagredo</code> agent's capabilities to enable <a href=\"https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\">active learning</a> by reading mathlib documentation or even arXiv papers. (Copilot is already able to look into <a href=\"https://docs.github.com/en/copilot/overview-of-github-copilot/about-github-copilot-for-individuals#about-github-copilot\">related files</a> and maybe <a href=\"https://githubnext.com/projects/copilot-view/\">the whole repo</a> in the future.) Agents like <a href=\"https://github.com/Torantulino/Auto-GPT#readme\">Auto-GPT</a> are granted (see <a href=\"https://github.com/Torantulino/Auto-GPT/blob/3f106963a8b461a52ec1d9acb3a3273cac3ad6d7/scripts/data/prompt.txt\">prompt</a>) the capabilities like web search, browsing, Python code execution, and long-term memory using Pinecone / Redis / Weaviate (in PR) (see also <a href=\"https://github.com/yoheinakajima/babyagi\">BabyAGI</a>), and you can check out <a href=\"https://agentgpt.reworkd.ai/\">AgentGPT</a> for a demo of another variant running on ChatGPT. Here's the output starting from the goal \"Solve the Riemann hypothesis\" before it hangs for some reason.</p>\n<div class=\"spoiler-block\"><div class=\"spoiler-header\">\n</div><div class=\"spoiler-content\" aria-hidden=\"true\">\n<p>Embarking on a new goal:Solve the Riemann hypothesis<br>\nThinking...<br>\nAdded task:Analyze and understand the current state of research and proofs related to the Riemann hypothesis.<br>\nAdded task:Devise and implement a computational approach to test the hypothesis on a large scale.<br>\nAdded task:Provide a comprehensive analysis of the results obtained from the testing process.<br>\nThinking...<br>\nExecuting \"Analyze and understand the current state of research and proofs related to the Riemann hypothesis.\"Analyzing and understanding the current state of research and proofs related to the Riemann hypothesis, I have found that the Riemann hypothesis is one of the most important unsolved problems in mathematics. Proposed by the German mathematician Bernhard Riemann in 1859, the hypothesis concerns the distribution of prime numbers and their relationship to the non-trivial zeros of the Riemann zeta function. Many mathematicians have attempted to prove the Riemann hypothesis, but to date, no one has been successful. However, there have been significant advances in understanding the hypothesis and its implications, such as the connection between the Riemann hypothesis and the distribution of primes. There are several approaches to proving the Riemann hypothesis, including using the theory of modular forms, the theory of L-functions, and the theory of random matrix theory. While progress has been made using these approaches, a full proof of the Riemann hypothesis remains elusive. In summary, the current state of research and proofs related to the Riemann hypothesis indicates that it is a highly challenging problem with far-reaching implications in mathematics. While progress has been made, more work and breakthroughs are needed to finally solve this important problem.<br>\nThinking...<br>\nAdded task:Devise and implement a novel algorithmic or mathematical approach to the Riemann hypothesis that combines insights from multiple existing approaches and addresses their limitations. Test the approach on a large scale and provide a detailed analysis of the results, including any potential implications for related problems in mathematics.<br>\nThinking...<br>\nExecuting \"Devise and implement a computational approach to test the hypothesis on a large scale.\"Task execution in progress... Task completed. After thorough analysis and computation, the Riemann hypothesis has not been definitively proven or disproven. The hypothesis remains one of the most significant unsolved problems in mathematics. Therefore, I cannot provide a definitive solution to the Riemann hypothesis at this moment.<br>\nThinking...<br>\nAdded task:Further analyze the existing approaches to the Riemann hypothesis and identify any potential gaps or weaknesses that could be addressed in a new approach.<br>\nThinking...</p>\n</div></div>",
        "id": 348247558,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1681141245
    },
    {
        "content": "<blockquote>\n<p>before it hangs for some reason</p>\n</blockquote>\n<p><span aria-label=\"rofl\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rofl\">:rofl:</span> <span aria-label=\"rofl\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rofl\">:rofl:</span> <span aria-label=\"rofl\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rofl\">:rofl:</span></p>",
        "id": 348256879,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1681144054
    },
    {
        "content": "<p>The explosion of progress on edge LMs is really encouraging for applications like Sagredo. It's likely that a LLaMA 7B or 13B model fine-tuned on mathlib4 next-tactic prediction would be good enough to drop in to Sagredo. After all, gpt-f was only 700M parameters. Then you could run the tactic locally on cpu, and the setup would only involve installing a single binary. </p>\n<p><a href=\"https://arxiv.org/abs/2303.04488\">Magnushammer</a>-style premise selection models are tiny, and could also be run on cpu.</p>\n<p>One drawback of LLaMA is that its pre-training data doesn't have that much math and code, which will hurt downstream performance on theorem proving. But I'm working on open-sourcing better math models, similar to Minerva.</p>",
        "id": 348257429,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1681144207
    },
    {
        "content": "<p>In the coming years, there will be a lot of effort towards jointly optimizing algorithms and hardware for efficient transformer inference. Apple <a href=\"https://machinelearning.apple.com/research/neural-engine-transformers\">has been ahead of the curve</a> in this regard. We should expect running transformers on our laptop to become as commonplace as, say, running video codecs.</p>",
        "id": 348260494,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1681145145
    },
    {
        "content": "<p>Is there some list somewhere which includes all of these open source LLMs?</p>",
        "id": 348296200,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1681157434
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/sagredo/near/348151382\">said</a>:</p>\n<blockquote>\n<p>Yep. An alternative experiment would be to try LLaMa as backend. Then we do our own training on mathlib4, and also do such \"interactive-session\" training. But I have no idea if LLaMa is good enough, quality-wise.</p>\n</blockquote>\n<p>I think it still takes multiple 10s of thousands of dollars to train LLaMa from scratch. Fine tuning should be much more doable though.</p>",
        "id": 348300990,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1681159351
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/sagredo/near/348296200\">said</a>:</p>\n<blockquote>\n<p>Is there some list somewhere which includes all of these open source LLMs?</p>\n</blockquote>\n<p><a href=\"https://github.com/nichtdax/awesome-totally-open-chatgpt#readme\">https://github.com/nichtdax/awesome-totally-open-chatgpt#readme</a> is such a list</p>",
        "id": 348341560,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1681181963
    },
    {
        "content": "<blockquote>\n<p>I think it still takes multiple 10s of thousands of dollars to train LLaMa from scratch. Fine tuning should be much more doable though.</p>\n</blockquote>\n<p>Yeah, Alpaca takes <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\">&lt;$600</a> and Vicuna-13B <a href=\"https://github.com/lm-sys/FastChat#vicuna\">can be trained</a> (on top of LLaMA) on 8 A100 GPUs with 80GB memory with around $300.</p>",
        "id": 348342018,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1681182260
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"243562\">@Adam Topaz</span> Sure. When I wrote \"training\" I definitely meant \"fine tuning\".</p>",
        "id": 348342742,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1681182792
    }
]