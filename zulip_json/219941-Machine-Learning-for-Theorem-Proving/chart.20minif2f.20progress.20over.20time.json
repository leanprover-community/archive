[
    {
        "content": "<p>has anyone seen a chart of minif2f progress over time? it's ok if it's fast and loose with the refactors/version control that it's undergone</p>",
        "id": 485684899,
        "sender_full_name": "Quinn",
        "timestamp": 1733164113
    },
    {
        "content": "<ul>\n<li><a href=\"https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-test\">https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-test</a></li>\n<li><a href=\"https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-valid\">https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-valid</a></li>\n</ul>\n<p>They are missing MANY results, especially almost all newer ones.  (That can be fixed by going through and adding them.  A good but not complete list of recent models is here: https://leanprover.zulipchat.com#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Proper.20LLM.20Lean4.20Integration.20with.20recursive.20checks.20for.20error/near/474449773).</p>\n<p>Also they are grouped by pass@k, which is isn’t a good way to group things.  In theorem proving, pass@1 with a tree search calling GPT-4 at every node and pass@100 with just GPT-4 (no tree search) are using similar amounts of compute/tokens. Whereas, a tree search using a smaller neural network using pass@100 is using far less compute than either.  (The two DeepSeek-Prover papers have a slightly better way, where they approximately count calls to the model.)</p>\n<p>Also, it is not clear what to do with current models using especially large amounts of compute like DeepSeek-Prover, InternLM2-Step-Prover, or (probably) Aristotle.  This seems to be one of the ways models can improve there score on Minif2f.  And Aristotle is just a blog post that sounds too-good-to-be-true so it is likely using a lot of compute.  (If AlphaProof ever evaluated on MiniF2F it would also be using a lot of compute.)  Also InternLM2-Step-Prover many to be consider as cheating since it trains on compfiles (although in fairness we don’t know if compfiles is in the training data of other LLMs).</p>\n<p>And of course most of the best models are evenly split between Lean and Isabelle.</p>\n<p>Maybe an even better source of recent numbers is just the tables in recent papers like InternLM2-Step-Prover, DeepSeek-Prover-v1.5, and ABLE which include their results and previous ones.</p>",
        "id": 485706287,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733172559
    },
    {
        "content": "<p>wow. fantastic answer, thanks so much!</p>",
        "id": 485719317,
        "sender_full_name": "Quinn",
        "timestamp": 1733178424
    }
]