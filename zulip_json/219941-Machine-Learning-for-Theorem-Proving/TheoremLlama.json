[
    {
        "content": "<p><a href=\"https://arxiv.org/abs/2407.03203\">TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts</a></p>\n<ul>\n<li>Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang (none of them appear to be on this Zulip)</li>\n</ul>",
        "id": 451357530,
        "sender_full_name": "Jason Rute",
        "timestamp": 1720966139
    },
    {
        "content": "<p>I haven't read it in detail, but like DeepSeek-Prover, it uses LLMs for whole proof synthesis (with 128 samples like in DeepSeek-Prover).  It also appears to use autoformalization to get more data, and natural language reasoning (I'm not sure if that is similar to how it is used in Draft, Sketch, Prove).</p>",
        "id": 451357541,
        "sender_full_name": "Jason Rute",
        "timestamp": 1720966148
    },
    {
        "content": "<p>While it lags behind DeepSeek-Prover on MiniF2F, it appears that the <a href=\"https://github.com/RickySkywalker/TheoremLlama\">code</a>, <a href=\"https://huggingface.co/RickyDeSkywalker/TheoremLlama\">model</a> and <a href=\"https://huggingface.co/datasets/RickyDeSkywalker/OpenBootstrappedTheorem\">dataset</a> are all open sourced!  Also, I think there are some interesting ideas which I need to read more to understand.</p>",
        "id": 451357547,
        "sender_full_name": "Jason Rute",
        "timestamp": 1720966155
    },
    {
        "content": "<p>Since it is open-sourced, I'd love it if this was benchmarked on some of the recent benchmarks coming out when they come out.  Again, like DeepSeek-Prover, it isn't clear to me yet if it is specific just to competition problems like MiniF2F or if it also does well on \"real-world\" Lean proofs.  (I don't think it has a way to handle new definitions/theorems.)</p>",
        "id": 451357553,
        "sender_full_name": "Jason Rute",
        "timestamp": 1720966160
    },
    {
        "content": "<p>Also, this paper, DeepSeek-Prover, and the earlier LLM papers like DSP (and successors) makes it seem more and more likely that LLMs are not bad for whole proof synthesis (even without seeing the intermediate goal states).  It seems that a huge part is just to fine-tune on better data (through say auto-formalization or manual data curation).</p>",
        "id": 451357554,
        "sender_full_name": "Jason Rute",
        "timestamp": 1720966163
    },
    {
        "content": "<blockquote>\n<p>Open-sourced</p>\n</blockquote>\n<p>This would be nice, but currently all I see on that GitHub page is a readme</p>",
        "id": 451383674,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1720993419
    }
]