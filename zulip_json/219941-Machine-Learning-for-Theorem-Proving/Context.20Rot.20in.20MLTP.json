[
    {
        "content": "<p>I was going through some of the benchmarks used for MLTP (ML based Theorem Proving). MiniF2F, ProofNet and competition oriented datasets (IMO, Putnam problems) typically seem to involve relatively self-contained problems with short proof lengths.</p>\n<p>I am new to this, so I am not sure but how do LLM-based proof assistants handle context degradation in long-form theorem proving vs. competition-style benchmarks?</p>\n<p>Does context rot affect theorem proving any differently from other cases?</p>\n<ol>\n<li>\n<p>Has anyone has done systematic measurements comparing proof success rates as a function of dependency chain length or required context size?</p>\n</li>\n<li>\n<p>What strategies have been effective for managing this? Chunking proof goals differently, using RAG approaches for relevant lemmas, or finding that certain proof structuring patterns help maintain coherence?</p>\n</li>\n</ol>",
        "id": 536921292,
        "sender_full_name": "Arjo",
        "timestamp": 1756567331
    },
    {
        "content": "<p>From what I've read there are several methods can address context degradation in long-form theorem proving:</p>\n<ol>\n<li>Generate a purported proof, pass that purported  proof to Lean for validation. If the purported proof is invalid, retain the valid portion of the purported proof and continue proving from some leaf or non-leaf proof state of the partial purported proof. For example this is utilized by <a href=\"https://arxiv.org/abs/2408.08152\">DeepSeek-Prover-V1.5</a>.</li>\n<li>Generate a purported proof, pass that purported proof to Lean for validation. If the purported proof is invalid, pass the purported proof and error message to the model allowing it to correct the purported proof.  Repeat as needed. For example this is utilized by <a href=\"https://arxiv.org/abs/2508.03613\">Goedel-Prover-V2</a> and <a href=\"https://arxiv.org/abs/2507.15225\">Deltaâ€‘Prover</a>.</li>\n<li>Generate the proof a tactic/proof step at a time and use a more standard algorithm, e.g. Monte Carlo Tree Search (MCTS), to organize the large scale arc of the proof. For example <a href=\"https://arxiv.org/abs/2205.11491\">HTPS</a> and to some extent <a href=\"https://arxiv.org/abs/2408.08152\">DeepSeek-Prover-V1.5</a> utilize this technique.</li>\n</ol>\n<p>Note that these methods were not necessarily explicitly constructed to  address context degradation in long-form theorem proving. And surely there are other examples, but these were the first that came to mind.</p>\n<p>As to your other questions:</p>\n<ol>\n<li>There are some measurements done in the <a href=\"https://arxiv.org/abs/2504.11354\">Kimina-Prover Preview</a> paper, i.e. Figure 4,  showing how miniF2F test accuracy increases with an increasing number of tokens. (There are likely other studies, but that was the first that came to mind.) </li>\n<li>Some of the things I mentioned earlier are effective for managing this, but there are also others, e.g. <a href=\"https://arxiv.org/abs/2310.00656\">LEGO-Prover</a> creates a growing library of lemmas that it can draw upon; <a href=\"https://arxiv.org/abs/2210.12283\">Draft, Sketch, and Prove</a> first creates an informal proof, which is made into a formal proof sketch, which is then used to formally prove the target result. </li>\n</ol>\n<p>Hope this helps, though this is only the tip of the iceberg.</p>",
        "id": 536975726,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1756638969
    },
    {
        "content": "<p>manage context-select a subset of the context to use in every query.</p>",
        "id": 536989847,
        "sender_full_name": "Jared green",
        "timestamp": 1756653856
    }
]