[
    {
        "content": "<p>I noticed the abstracts for <a href=\"http://aitp-conference.org/2020/\" title=\"http://aitp-conference.org/2020/\">AITP 2020</a> got posted recently (not sure when).  I'm really looking forward to reading through them.  I thought I'd also add any comments I have here.  Feel free to add you own as well.</p>",
        "id": 192466509,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585697501
    },
    {
        "content": "<p>(I also assume there is no other public place on the internet where people talk about AITP 2020, right?)</p>",
        "id": 192466577,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585697534
    },
    {
        "content": "<p>The first abstract I want to talk about is <a href=\"http://aitp-conference.org/2020/abstract/paper_7.pdf\" title=\"http://aitp-conference.org/2020/abstract/paper_7.pdf\">Reinforcement Learning for Interactive Theorem Proving in HOL4</a> by <span class=\"user-mention\" data-user-id=\"110187\">@Minchao Wu</span>, Michael Norrish, Christian Walder and Amir Dezfouli.  They built a gym environment for HOL4.  I don't think the code or paper is public yet, but the abstract has a lot of details.  The main idea is that they created a Python environment for reinforcement learning of theorem proving in HOL4.  There are a number of points which make it different from HOList and the other projects.  Rather than go through all the details, I'll give a few quick thoughts:</p>\n<ul>\n<li>\n<p>The main thing I found interesting about this is that they set this up as a Gym compatible environment (so that they could use the REINFORCE algorithm).  By Gym compatible I mean Open AI's Gym interface which is a really simple interface.  You have to be able to start an environment, perform a step in that environment, and the read an observation about the state of that environment.  What makes this different from other interactive theorem proving \"gyms\" that I'm aware of is that it doesn't seem to allow backtracking.  Backtracking (in the form of a tree search) is very important for projects like AlphaGo, solving the Rubik's cube, and HOList.  From what I can tell, they allow for two ways around this:</p>\n<ol>\n<li>They actually do have back-tracking in a sense, since a \"state\" is not single goal stack (with a local context), but instead a \"fringe\".  I'm not entirely clear what a fringe is.  They describe a fringe as all unvisited goal stacks, but it isn't clear to me what \"visited\" means in this context.  Unlike, say, A* search, each goal stack has a very large number of \"neighbors\" since there are a very large number of tactic applications that can be applied to a goal.  It would be unreasonable to visit all of the neighbors.  So maybe every goal set remains in the fringe, but then I don't know what \"unvisited\" means.  (Also later they talk about \"winning\" if the fridge becomes empty, but that also wouldn't make sense since I only need to solve some goal stack, not all of them.)  Another interpretation is that a goal stack is visited when any tactic is applied to it, but then the fringe would only have size 1, right, because each tactic application only creates one new goal stack?  (Clearly I don't understand this well enough.)  Anyway, I think there is some notion of being able to choose which goal stack to explore more.</li>\n<li>The default behavior is to completely ignore the fringe and to just apply your tactic application to the first goal stack in the fringe.  In this case it is basically pure model-free reinforcement learning.  (Also since they are using REINFORCE, it seems they are using this default behavior.)  This however is surprising since I was under the impression that the model-based reinforcement learning used in theorem proving (i.e. using the fact that the agent has access to an accurate simulator of the theorem learning environment) is very important to getting good results in theorem proving.  If that is not the case, then that would be surprising.  (I was just having <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList/near/192462558\" title=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList/near/192462558\">a conversation about this</a> for HOList.)</li>\n</ol>\n</li>\n<li>\n<p>Unlike a lot of theorem proving RL setups which only assign rewards at the end (solved or not), they assign partial negative rewards for picking bad tactics (which fail or don't change the goal) and for running out of time.  This probably helps the agent learn faster.</p>\n</li>\n<li>It is interesting that environments written in Python (like this one) sell it as being easier to use machine learning tools like PyCharm/Tensorflow and environments written in OCaml (like the new <a href=\"https://arxiv.org/abs/2003.09140\" title=\"https://arxiv.org/abs/2003.09140\">TacticToe for Coq</a> paper) sell it as being closer to the theorem prover.  It gets at the following dichotomy.  If we ever hope to use these tools in practice, we need to have them available in the language the theorem prover is written in, but if we want to try things quickly or get machine learning experts involved, then we need to have it in Python environments with clean and well documented APIs.  I think we need both.</li>\n<li>While the environment seems fairly grand, what they have done with it so far seems fairly limited.  I think they have only used it on 10 theorems so far.  Also, I think each theorem is like a whole new world where the agent has to relearn how to prove theorems from scratch.  This is very different from the DeepHOL Zero work where the agent learns stuff from one theorem which it can apply to another.  I hope that this work can be eventually brought to that level.  However, even if that is all that they have planned, I think it will serve as a good Gym environment for Machine Learning researchers to try out theorem proving, but not as competition for say DeepHOL Zero or any of the supervised-learned ML agents, like TacticToe which is currently the state of the art for ML in HOL4.</li>\n</ul>",
        "id": 192603711,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585782569
    },
    {
        "content": "<p>Also, if anyone can fix this, the name of their project is spelled incorrectly at <a href=\"http://aitp-conference.org/2020/\" title=\"http://aitp-conference.org/2020/\">http://aitp-conference.org/2020/</a> : \"leaerning\" -&gt; \"learning\"</p>",
        "id": 192603839,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585782661
    },
    {
        "content": "<p>A fringe is a set of goals (with their corresponding local contexts) that haven’t been proved during a single proof attempt. One starts with the main goal. As one applies tactics, the fringe is expanded with new (sub)goals generated. The main goal is proved when the fringe becomes empty. There is no backtracking because this way is thinking of theorem proving as a single-pass exploration of the tree. The hope is that the “backtracking flavor” should be learned by the policy. In the current setting of environment, one can still implement backtracking by themselves if they wanted to. However, we are working on a new version that would support backtracking in a neater way.</p>",
        "id": 192638469,
        "sender_full_name": "Minchao Wu",
        "timestamp": 1585818618
    },
    {
        "content": "<p>Ok.  I clearly misunderstood.  So a \"fringe\" is what is called a \"goal stack\" in HOL4, right?  (Looking back, I misread that sentence.  Although, it is confusing to say that an edge of the tree is a tactic application, since if a node is a single (sub)goal, all the edges coming out of that node are going to be the same tactic application.)</p>",
        "id": 192655453,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585829520
    },
    {
        "content": "<blockquote>\n<p>There is no backtracking because this way is thinking of theorem proving as a single-pass exploration of the tree.</p>\n</blockquote>\n<p>Is this \"single pass exploration\" approach viable?  It would be as if AlphaZero was only allowed to play go and chess with a single pass through (no MCTS).  Do you think one can train a state of the art HOL4 theorem prover without a tree search?  (Or maybe I'm misunderstanding the goal of this project.)  However, I do admit that by restricting to model-free reinforcement learning that you are opening up a wide field of prebuilt and tuned algorithms which you can apply (as long as you can handle embedding the formulas since that is nonstandard).</p>",
        "id": 192656375,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585830058
    },
    {
        "content": "<blockquote>\n<p>In the current setting of environment, one can still implement backtracking by themselves if they wanted to.</p>\n</blockquote>\n<p>Does the gym-like environment have a mechanism to backtrack?  Maybe <code>get_state</code>and <code>set_state</code> methods (which I've seen in other Gym-like environments)?  (Of course the user could program their own HOL4 simulator, but that seems like overkill. <span aria-label=\"slight smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"slight smile\">:slight_smile:</span>)</p>",
        "id": 192656661,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585830215
    },
    {
        "content": "<blockquote>\n<p>However, we are working on a new version that would support backtracking in a neater way.</p>\n</blockquote>\n<p>Looking forward to seeing this!  As well any other papers, presentations, code, and most of all, public gym-like APIs that come out of this project!</p>",
        "id": 192656868,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585830323
    },
    {
        "content": "<blockquote>\n<p>Is this \"single pass exploration\" approach viable?  It would be as if AlphaZero was only allowed to play go and chess with a single pass through (no MCTS).  Do you think one can train a state of the art HOL4 theorem prover without a tree search? </p>\n</blockquote>\n<p>I wouldn't say that there is no tree search, but the tree search is the learned policy. If you really want some explicit tree search (say MCTS), you still have the option to add them afterwards by treating them as policy improvement operators.</p>\n<blockquote>\n<p>However, I do admit that by restricting to model-free reinforcement learning that you are opening up a wide field of prebuilt and tuned algorithms which you can apply (as long as you can handle embedding the formulas since that is nonstandard).</p>\n</blockquote>\n<p>I don't see how prebuilt and tuned algorithms can be applied directly here at this stage. The learning of the argument policy is non-standard and would require quite some flexibility from the prebuilt algorithms. We wrote our own training algorithms for the AITP version. </p>\n<blockquote>\n<p>Does the gym-like environment have a mechanism to backtrack? Maybe get_state and set_state methods (which I've seen in other Gym-like environments)?</p>\n</blockquote>\n<p>There is <code>get_state</code> but no <code>set_state</code> in the current AITP version. The one we are working on would make it available.  Some of us still believe that the non-backtracking version would be much easier to understand and train (and shouldn't perform too bad), though.</p>",
        "id": 192660432,
        "sender_full_name": "Minchao Wu",
        "timestamp": 1585832027
    },
    {
        "content": "<blockquote>\n<p>If you really want some explicit tree search (say MCTS), you still have the option to add them</p>\n</blockquote>\n<p>To have MCTS (or any tree search), you need a way to jump between states of the environment.  Do you have that?  If I apply a tactic and later decide I didn't want to do that, can I back up (or say \"go to such and such state\")?  Most of the Open AI gym environments don't support this functionality, so it isn't obvious that yours does either?  (EDIT: Maybe they support it in that you can make a deep copy of the state, but I don't know that would be possible with an ITP like HOL4 in the backend.)</p>",
        "id": 192661005,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585832290
    },
    {
        "content": "<blockquote>\n<p>I don't see how prebuilt and tuned algorithms can be applied directly here at this stage.</p>\n</blockquote>\n<p>That is a good point.  I guess you were saying you used REINFORCE, so I didn't think about it too strongly, but yes, the action space is quite unique.</p>",
        "id": 192661129,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585832372
    },
    {
        "content": "<blockquote>\n<p>Most of the Open AI gym environments don't support this functionality, so it isn't obvious that yours does either? </p>\n</blockquote>\n<p>No, at least not at this stage. One would need to do it themselves. The environment can tell you whatever the resulting state is, but the user needs to manage the states themselves.</p>",
        "id": 192661843,
        "sender_full_name": "Minchao Wu",
        "timestamp": 1585832732
    },
    {
        "content": "<p>Can a user save a copy of state and later apply actions to it ? In functional programming terms, I'm trying to figure out if the states are persistent data structures.  If not, I don't see how the user can manage the tree search themselves.  (Although, I guess the user can also just start over and apply the same sequence of tactics again to get back to a previously visited state.)</p>",
        "id": 192662251,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585832915
    },
    {
        "content": "<p>For that you just need something like set_states which is not too difficult to implement. Then one can maintain a history of states on their own and call the environment to rollout. But as you said in (), one can always do it with the minimal help from the environment.</p>",
        "id": 192664342,
        "sender_full_name": "Minchao Wu",
        "timestamp": 1585833891
    },
    {
        "content": "<p>Thanks for posting this!</p>",
        "id": 192991365,
        "sender_full_name": "Rongmin Lu",
        "timestamp": 1586142344
    },
    {
        "content": "<p>I found <a href=\"http://aitp-conference.org/2020/abstract/paper_18.pdf\" title=\"http://aitp-conference.org/2020/abstract/paper_18.pdf\">Wu/Jiang/Grosse/Ba's abstract on proving inequalities</a> very interesting. Their generator is straightforward to reproduce and while the proven equalities/inequalities appear still a bit easy (for the k/l values they took at least), the result and study is pragmatic and well covered.</p>",
        "id": 193050620,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1586183674
    }
]