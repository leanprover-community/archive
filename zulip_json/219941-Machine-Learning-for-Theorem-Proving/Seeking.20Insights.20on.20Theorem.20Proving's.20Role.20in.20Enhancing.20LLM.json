[
    {
        "content": "<p>Hello everyone! I'm fairly new here and haven't had the opportunity to use Lean yet, but I've recently developed a fascination with the application of large language models (LLMs) for reasoning tasks. I've been delving into several papers in the theorem proving domain, including GPT-F, Curriculum Learning, Hyper Tree Search, Autoformalization, Thor, DSP, and MiniF2F, to name a few.</p>\n<p>I'm aware that some of the authors of these papers frequent this chat, so please bear with me as I grapple with some beginner questions. I would be extremely grateful if anyone could shed some light on the significance of theorem proving from their perspective. Specifically, I'm struggling with two questions:</p>\n<p>1  How does theorem proving stack up against program synthesis, especially considering that the latter also necessitates reasoning skills and seems to have a broader range of applications?</p>\n<p>2  Given that current LLMs are still grappling with understanding and solving elementary school-level problems, is theorem proving a necessary step if my aim is purely to enhance the reasoning abilities of LLMs? The learning curve to understand Lean/Isabelle feels quite steep, and I'm wondering if it's an essential investment.</p>",
        "id": 357940813,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1683911725
    },
    {
        "content": "<p>there's a stream <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a> , maybe your post will be more visible to that audience there?</p>",
        "id": 357945309,
        "sender_full_name": "Andrés Goens",
        "timestamp": 1683912858
    },
    {
        "content": "<p>As a beginner myself in this area, here’s my perspective:<br>\nThe papers you mentioned often use some configuration of LLM to suggest proofs to Lean, which then verifies their correctness. <br>\nIn this way, LLMs are still operating as pattern recognition machines, and they learn to suggest good tactics because the corpus of proofs on which they were trained has patterns, and reproducing those patterns sometimes leads to valid proofs, and sometimes not. By having Lean checking the answers, the correctness of the guesses can be verified. </p>\n<p>But you might also be interested in how LLMs answer in chain of thought prompting or when told to “think step by step.” This kind of reasoning is more emergent and flexible than Lean, but it isn’t as trustworthy, and cannot be checked automatically. Folks are still studying how and why this happens. These emergent reasoning abilities more often appear in the LLMs trained on executable code than those trained mainly on text. </p>\n<p>To your questions:<br>\nI’m trying to learn more about program synthesis myself. At least one big difference is that if you do program synthesis in, for example, Python, the resulting code cannot be formally verified. Programs written in Lean can be proven correct.<br>\nGood question! Since LLMs don’t yet reason robustly, I don’t think anyone really knows “what should we do to get LLMs to reason” - it’s an active area of research. Theorem proving may play a role, or may not. One thing I am sure of - learning Lean is harder than learning the basics of ML. Learn Lean and you get a niche skill that will make you stand apart (to those who are interested in hiring people who know Lean). But I’d suggest you particularly ask yourself if you enjoy it before investing into it </p>\n<p>PS: you might also like the papers and talks about “Neural Algorithmic Reasoning” by Petar Veličković. There’s a lot of interesting work on reasoning + NNs outside of Lean, and outside of LLMs, even.</p>",
        "id": 357993052,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1683928550
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"315434\">Andrés Goens</span> <a href=\"#narrow/stream/113489-new-members/topic/Seeking.20Insights.20on.20Theorem.20Proving's.20Role.20in.20Enhancing.20LLM/near/357945309\">said</a>:</p>\n<blockquote>\n<p>there's a stream <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a> , maybe your post will be more visible to that audience there?</p>\n</blockquote>\n<p>Thanks for pointing this out! I'll move the discussion to the new stream</p>",
        "id": 358031513,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1683949984
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 358032719,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1683951150
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"424214\">Tyler Josephson ⚛️</span> <a href=\"#narrow/stream/113489-new-members/topic/Seeking.20Insights.20on.20Theorem.20Proving's.20Role.20in.20Enhancing.20LLM/near/357993052\">said</a>:</p>\n<blockquote>\n<p>As a beginner myself in this area, here’s my perspective:<br>\nThe papers you mentioned often use some configuration of LLM to suggest proofs to Lean, which then verifies their correctness. <br>\nIn this way, LLMs are still operating as pattern recognition machines, and they learn to suggest good tactics because the corpus of proofs on which they were trained has patterns, and reproducing those patterns sometimes leads to valid proofs, and sometimes not. By having Lean checking the answers, the correctness of the guesses can be verified. </p>\n<p>But you might also be interested in how LLMs answer in chain of thought prompting or when told to “think step by step.” This kind of reasoning is more emergent and flexible than Lean, but it isn’t as trustworthy, and cannot be checked automatically. Folks are still studying how and why this happens. These emergent reasoning abilities more often appear in the LLMs trained on executable code than those trained mainly on text. </p>\n<p>To your questions:<br>\nI’m trying to learn more about program synthesis myself. At least one big difference is that if you do program synthesis in, for example, Python, the resulting code cannot be formally verified. Programs written in Lean can be proven correct.<br>\nGood question! Since LLMs don’t yet reason robustly, I don’t think anyone really knows “what should we do to get LLMs to reason” - it’s an active area of research. Theorem proving may play a role, or may not. One thing I am sure of - learning Lean is harder than learning the basics of ML. Learn Lean and you get a niche skill that will make you stand apart (to those who are interested in hiring people who know Lean). But I’d suggest you particularly ask yourself if you enjoy it before investing into it </p>\n<p>PS: you might also like the papers and talks about “Neural Algorithmic Reasoning” by Petar Veličković. There’s a lot of interesting work on reasoning + NNs outside of Lean, and outside of LLMs, even.</p>\n</blockquote>\n<p>Thank you so much for your detailed and informative response! It's quite enlightening to hear from someone also navigating through this complex field.</p>\n<p>1  I must admit, I hadn't fully considered the extent of reasoning involved in this area, especially beyond the realms of neural networks. I'll be sure to read the paper you mentioned.<br>\n2  The point you made about the verifiability of theorem proving as compared to program synthesis was particularly eye-opening.<br>\n3  I will certainly be trying out Lean :)</p>",
        "id": 358033992,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1683952525
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"616750\">Dongwei Jiang</span> has marked this topic as resolved.</p>",
        "id": 358034014,
        "sender_full_name": "Notification Bot",
        "timestamp": 1683952558
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> has marked this topic as unresolved.</p>",
        "id": 358083923,
        "sender_full_name": "Notification Bot",
        "timestamp": 1683975463
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"113489\" href=\"/#narrow/stream/113489-new-members/topic/.E2.9C.94.20Seeking.20Insights.20on.20Theorem.20Proving.27s.20Role.20in.20Enhancing.2E.2E.2E\">#new members &gt; ✔ Seeking Insights on Theorem Proving's Role in Enhancing...</a> by <span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span>.</p>",
        "id": 358083924,
        "sender_full_name": "Notification Bot",
        "timestamp": 1683975464
    },
    {
        "content": "<p>I've been pondering over our past discussion regarding the role of theorem proving in Language Learning Models (LLMs) research. After brainstorming with some friends, I've refined my thoughts. I believe theorem proving can help LLM on three fronts: understanding, reasoning, and planning.</p>\n<p>1  The understanding challenge refers to the capability of LLMs to translate natural language problems into mathematical equations. Take this example: \"Rachel has 17 apples. She gives 9 to Sarah. How many apples does Rachel have now?\" which should translate to \"17 - 9 = ?\".  In a similar but more complex vein, Autoformulation with LLMs is working on this kind of translation.</p>\n<p>2  The reasoning challenge involves the LLM's ability to function within a rule-based system like solving equations or proving theorems. It's important to note that equation solving at a grade-school level isn't as interesting because the rules there are pretty straightforward. Theorem proving, on the other hand, is an interesting case as it requires strict adherence to logic and definitions.</p>\n<p>3  The planning challenge, as discussed by Bubeck in his GPT-4 talk, concerns the LLM's ability to backtrack and execute efficient search strategies. The idea is to have a backup plan or to plan so effectively that it rarely fails, which also applies to theorem proving.</p>\n<p>I should note that I'm not 100% confident in the accuracy of what I've said. My use of definitions or terms might be off, as I'm still learning about LLMs and Lean. I'm open to feedback and more than willing to learn.</p>",
        "id": 359924652,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1684599813
    }
]