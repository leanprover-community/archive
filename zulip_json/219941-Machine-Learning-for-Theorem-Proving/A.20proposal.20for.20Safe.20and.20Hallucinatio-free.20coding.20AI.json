[
    {
        "content": "<p>I have written an essay \"A Proposal for Safe and Hallucination-free Coding AI\" (<a href=\"https://gasstationmanager.github.io/ai/2024/11/04/a-proposal.html\">https://gasstationmanager.github.io/ai/2024/11/04/a-proposal.html</a>), in which I propose an open-source collaboration on a research agenda that I believe will eventually lead to coding AIs that have superhuman-level ability, are hallucination-free, and safe.</p>\n<p>By the fact that I'm posting here you can have a pretty good guess of what I'm proposing. Some of the stuff I say about Lean will be familiar to you and feel free to skim over. </p>\n<p>I propose a concrete path that combines ML with lean for the task of code generation. In the essay I admit this approach faces some significant obstacles, including the lack of existing data. I discuss ideas to overcome this with autoformalization and with crowdsourcing. I am also open-sourcing some preliminary work: Code with Proofs: the Arena (<a href=\"https://github.com/GasStationManager/CodeProofTheArena\">https://github.com/GasStationManager/CodeProofTheArena</a>) and Code with Proofs Benchmark(<a href=\"https://github.com/GasStationManager/CodeProofBenchmark\">https://github.com/GasStationManager/CodeProofBenchmark</a>).</p>\n<p>Comments are welcome!</p>",
        "id": 480574308,
        "sender_full_name": "GasStationManager",
        "timestamp": 1730780647
    },
    {
        "content": "<p>I don't see what's new in this over what the community already knows, and what is being attempted across multiple papers in AI conferences in the last 3 years. Also the first two points on the FAQ pretty much contradict each other. On the one hand you want to generate useful programs which are almost always impure (often in the IO monad). On the other hand you want to generate pure functions for proof reasons.</p>",
        "id": 480675406,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1730798830
    },
    {
        "content": "<p>Thanks for the comments! I am not surprised that people are trying these kinds of ideas; it is not hard to see that this is a promising direction, I am not claiming novelty in this sense. On the other hand it is also not hard to see that a major obstacle is the lack of existing data. Can the community do something about that. My claim is that a community wide collaboration on data creation may be needed, at least on the open source side.</p>\n<p>Regarding your second comment, I think that is a fair point. A partial answer, in my (non-expert) opinion:  my impression is that the point of IO monad is so that the rest of the code can be pure. If all the pure parts can be machine verified, and the remaining IO monad parts are small enough so that I can check it manually, then I might be prepared to trust the whole thing.</p>",
        "id": 480717984,
        "sender_full_name": "GasStationManager",
        "timestamp": 1730812394
    },
    {
        "content": "<p>My impression is that people are thinking about synthetically generating data to train on, this is the sort of bonkers-sounding idea which will \"obviously not work\" but which then goes on to work. But as far as I know there's nothing public here</p>",
        "id": 480722221,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1730813724
    },
    {
        "content": "<p>If you are not claiming novelty then I would recommend a good lit-survey to be included in the article (for which the threads on this Zulip will be useful). The survey should explain the lines of research currently being pursued and assess how far they go on achieving the general community consensus on keeping AI from hallucinations using ITPs and AI assistance in proof and program synthesis. There is also a non trivial amount of non AI work on program synthesis matching a spec.</p>",
        "id": 480722601,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1730813841
    },
    {
        "content": "<p>Also, for data generation, I see an economic problem. Those who train and offer AI models as a service benefit disproportionately (in fact in many cases infinitely more) than those who create and curate the content that is fed as data to the AI. This is already seen in the art community. Before lean users start creating these data sets, there should be a clear and proportionate compensation model to those who create the data. Given the cost of running these models, it is not sufficient to offer open access to the models. Shared ownership of the resulting revenues in the form of, say, guaranteed investment into ITP research and math should be the bare minimum. Otherwise this is a lose-lose scenario for those who create this data</p>",
        "id": 480722977,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1730813933
    },
    {
        "content": "<p>Re: Shreyas, it will be up to the community to set the data license to whichever one they are comfortable with. For example if people don't want their data to be used for commercial purpose at all then use a license with a non-commercial use clause, like CC BY-NC-SA. I think people most motivated to contribute will be those that are already interested in the advancement of (open-source) coding AI.</p>\n<p>There could also be demand for a separate, compensation-based model, e.g. a bounty system. The licensing terms will likely be dictated by whoever post the bounties though.</p>",
        "id": 480815399,
        "sender_full_name": "GasStationManager",
        "timestamp": 1730849058
    },
    {
        "content": "<p>True, but people who contribute to the advancement of open source AI implicitly do so in order to allow themselves and other members of the community to benefit from it. In the last two years it has become clear that relying purely on the good faith of the AI model owners can be very inadequate as a guarantee. I am raising this issue here so that any contributors who might read this might consider in depth about the negotiating position they are putting themselves in vis a vis-a-vis the AI model trainers (stackoverflow, GitHub, and chatgpt come to mind)</p>",
        "id": 480831644,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1730859193
    },
    {
        "content": "<p>About the bounty model, this depends on how the community organises around this issue.</p>",
        "id": 480831665,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1730859231
    },
    {
        "content": "<p>In either case, bringing this issue to the forefront and making people aware is a necessary first step</p>",
        "id": 480831795,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1730859346
    },
    {
        "content": "<p>I don’t think a huge amount of data is necessary. Otherwise this implies a sad future for autoformalization of mathematics. Well, let’s see what we can cook in the next few years.</p>",
        "id": 483434429,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1732093320
    },
    {
        "content": "<p>I have posted a couple of follow-ups, expanding on the various sub-projects, including the Arena website and autoformalization. </p>\n<p>In the <a href=\"https://gasstationmanager.github.io/ai/2024/12/03/memoization1.html\">most recent post</a>, I tried to come up with a nice proof that memoization (the algorithmic technique) is correct. After I had a proof I was satisfied with, I tried to teach it to Sonnet so that it can do it for arbitrary functions. </p>\n<p>This is actually exploring an idea I mentioned in the earlier essay, that in my opinion, programming with dependent types (putting proofs of correctness inside the code) is a suitable style for LLMs to adopt.</p>\n<p>How did Sonnet do? After prompting it with a worked example, and asking it to memoize a new function, it produced a very respectable first attempt. A couple of errors of the type \"simp didn't completely close the goal\"; after pasting the error message to Sonnet it was able to fix the errors and produce a correct proof.</p>\n<p>Takeaways</p>\n<ul>\n<li>Sonnet is already very strong in writing Lean code;</li>\n<li>it would be even stronger once it is equipped with ability to run code in Lean and fix its own errors, </li>\n<li>and the ability to utilize a stronger hammer to close simple goals.</li>\n<li>The style of programming with dependent types naturally allows the LLM to produce a good proof sketch; sometimes even better, an almost correct proof still in the proof sketch structure, so that subgoals can be repaired.</li>\n</ul>",
        "id": 486198509,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733355962
    },
    {
        "content": "<p>Does Sonnet mix up Lean 3 and Lean 4 syntax?</p>",
        "id": 486203207,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733358791
    },
    {
        "content": "<p>With all the academic papers around, I sometimes wonder if we also just need to have tutorials showing how to use all the existing tools to their full advantage in Lean (or Coq, Isabelle), and using tools which are practical and cheap enough to use without breaking the bank.</p>",
        "id": 486203424,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733358952
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/A.20proposal.20for.20Safe.20and.20Hallucinatio-free.20coding.20AI/near/486203207\">said</a>:</p>\n<blockquote>\n<p>Does Sonnet mix up Lean 3 and Lean 4 syntax?</p>\n</blockquote>\n<p>I use the Cursor editor all the time now, and I <em>think</em> their default model is Sonnet. I have not seen any Lean 3isms from it, at all.</p>",
        "id": 486207520,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1733361669
    },
    {
        "content": "<p>Yes Sonnet has been very good with sticking to Lean 4 syntax; better than GPT 4o in my experience.<br>\nFor this exercise, just to make sure, I made Sonnet first read Sagredo's prompt of Lean 4 syntax posted in <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/plaintext.2FAI-readable.20lean4.20syntax.20guide/near/447017161\">this other thread</a></p>",
        "id": 486211422,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733364383
    },
    {
        "content": "<p>In my <a href=\"https://gasstationmanager.github.io/ai/2024/12/09/dp2.html\">next experiment with Sonnet</a>, asked Sonnet to use bottom-up dynamic programming to solve rod-cutting problem (and prove its correctness). This time did it in a command-line chat interface where Sonnet can directly pass its code to the lean executable and get feedback. </p>\n<ul>\n<li>Sonnet did very well considering it was not given a worked example for the same type of problems; only examples of memoization were given.</li>\n<li>A flaw in my <a href=\"https://github.com/GasStationManager/LeanTool\">LeanTool</a> implementation: the lean executable does not extract goals for sorrys. While Sonnet often likes to output proof sketches with sorrys before going further. Looks like <a href=\"https://github.com/lenianiva/PyPantograph/blob/main/examples/sketch.py\">Pantograph's load_sorry</a> would be great for this. <span class=\"user-mention\" data-user-id=\"599027\">@Leni Aniva</span> what is the best way to install Pantograph in another project?</li>\n</ul>",
        "id": 487331056,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733860793
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/A.20proposal.20for.20Safe.20and.20Hallucinatio-free.20coding.20AI/near/487331056\">said</a>:</p>\n<blockquote>\n<p>In my <a href=\"https://gasstationmanager.github.io/ai/2024/12/09/dp2.html\">next experiment with Sonnet</a>, asked Sonnet to use bottom-up dynamic programming to solve rod-cutting problem (and prove its correctness). This time did it in a command-line chat interface where Sonnet can directly pass its code to the lean executable and get feedback. </p>\n<ul>\n<li>Sonnet did very well considering it was not given a worked example for the same type of problems; only examples of memoization were given.</li>\n<li>A flaw in my <a href=\"https://github.com/GasStationManager/LeanTool\">LeanTool</a> implementation: the lean executable does not extract goals for sorrys. While Sonnet often likes to output proof sketches with sorrys before going further. Looks like <a href=\"https://github.com/lenianiva/PyPantograph/blob/main/examples/sketch.py\">Pantograph's load_sorry</a> would be great for this. <span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span> what is the best way to install Pantograph in another project?</li>\n</ul>\n</blockquote>\n<p>You don't install Pantograph in another project. Pantograph reads the project for you. The instructions are given here <a href=\"https://lenianiva.github.io/PyPantograph/setup.html\">https://lenianiva.github.io/PyPantograph/setup.html</a></p>",
        "id": 487335028,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733862341
    },
    {
        "content": "<p>Thanks, that helps! I was trying to see if I can call Pantograph from my python project; but I see Pantograph needs to be run in its own lean environment, so that can be tricky. For now I can put my scripts inside Pantograph's installation, that should work</p>",
        "id": 487347791,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733867758
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/A.20proposal.20for.20Safe.20and.20Hallucinatio-free.20coding.20AI/near/487347791\">said</a>:</p>\n<blockquote>\n<p>Thanks, that helps! I was trying to see if I can call Pantograph from my python project; but I see Pantograph needs to be run in its own lean environment, so that can be tricky. For now I can put my scripts inside Pantograph's installation, that should work</p>\n</blockquote>\n<p>You can definitely call Pantograph from another Python project. It sets up the Lean envvars when it calls repl via a subprocess.</p>\n<p>If you have any issues please reach out to me or see the examples I have in PyPantograph</p>",
        "id": 487350391,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733869123
    },
    {
        "content": "<p>Update of <a href=\"https://github.com/GasStationManager/LeanTool\">LeanTool</a> with new features:</p>\n<ul>\n<li>extracting goal states for <code>sorry</code>s by calling Pantograph (Thanks <span class=\"user-mention\" data-user-id=\"599027\">@Leni Aniva</span> for the help!)</li>\n<li>system prompts to encourage the LLM to use some of the interactive featues of Lean that are less represented in training data, including <code>exact?</code>, and searching <a href=\"http://Moogle.ai\">Moogle.ai</a> via the command <a href=\"https://www.moogle.ai/\">#moogle</a> provided by LeanSearchClient. Some of these are inspired by discussion from <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Simple.20but.20hard.20puzzler.20for.20LLMs\">another thread</a>. This is very preliminary; suggestions are welcome on improving the prompt. What other Lean features would you want the LLMs to make more use of? </li>\n<li>OpenAI API-compatible proxy server: this can be plugged into any application that takes an OpenAI-compatible endpoint with a custom base URL. I have tested it to work with <a href=\"https://openwebui.com/\">OpenWebUI</a>, a fully featured chat interface, and <a href=\"https://www.continue.dev/\">Continue</a>, a VS Code plugin coding assistant.</li>\n</ul>",
        "id": 490283858,
        "sender_full_name": "GasStationManager",
        "timestamp": 1734773944
    },
    {
        "content": "<p>I really need to play around with your project sometime.  (Also, have you seen: <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/What.20do.20you.20hope.20AI.20can.20help.20you.20achieve.20with.20Lean.20in.202025.3F\">#general &gt; What do you hope AI can help you achieve with Lean in 2025?</a>?)</p>",
        "id": 490311350,
        "sender_full_name": "Jason Rute",
        "timestamp": 1734798308
    },
    {
        "content": "<p>I just posted a <a href=\"https://gasstationmanager.github.io/ai/2025/01/22/hallucination.html\">report</a> on some initial experiments using Lean and its features to detect and recover from hallucinations in coding AI models. To summarize:</p>\n<ul>\n<li>Start with Lean 4 coding problem instances where some AI model hallucinates. In this post we focus on a problem where DeepSeek v3 made a logical error.</li>\n<li>Explore approaches that utilize Lean features to allow the AI to detect and recover from the hallucination. Here, I explored applying property-based testing: generate random inputs and verify the input-output pair against the formal specification.</li>\n<li>In this instance, I was not able to directly apply Plausible because the problem's formal specification is not Decidable. Instead wrote a script that calls Plausible's #sample command to generate random input values, then calls a custom heuristic proof script to attempt proving the property or its negation.</li>\n<li>the script was able to find counterexamples that violated the specification. Prompted with the counterexamples, DeepSeek was able to fix its code and output a corrected solution.  </li>\n</ul>\n<p><a href=\"https://github.com/GasStationManager/WakingUp\">code</a> is available.<br>\nComments are welcome! I personally think this is where Lean and other ITPs can be extremely useful.</p>",
        "id": 495386574,
        "sender_full_name": "GasStationManager",
        "timestamp": 1737585610
    },
    {
        "content": "<p>In my <a href=\"https://gasstationmanager.github.io/ai/2025/03/28/alphabeta.html\">most recent report</a>, I explored a style of development that, once adopted, will help detect and reduce hallucinations in coding AIs. It is a combination of programming with dependent types and (property-based) test-driven development.</p>\n<ul>\n<li>Encode the specification as type information in the input &amp; return types, e.g. via subtyping.</li>\n<li>This naturally results in proof goals inside of the implementation. Initially, we can put <code>sorry</code>s in place of the required proofs.</li>\n<li>Before attempting the proofs, we want to first ensure that the implementation is correct. We apply property-based testing (PBT) to test whether there are counterexamples to these proof goals.</li>\n</ul>\n<p>Comments welcome!</p>",
        "id": 511855459,
        "sender_full_name": "GasStationManager",
        "timestamp": 1744502662
    },
    {
        "content": "<p>The reason why I think this is promising for AI coding: this provides frequent and immediate feedback from Lean to detect hallucinations. When an AI (or human) makes a bug in code, finding the bug is essentially a credit-assignment problem. Doing property-based testing inside code makes it easier to locate the bug.</p>\n<p>One issue I encountered: a natural way to do property-based testing is to put <code>plausible</code> in place of <code>sorry</code>s inside the implementation. However I was often not able to make <code>plausible</code> work, even after making sure the input data types are SampleableExt and Shrinkable, and the proof goal is Decidable. E.g. plausible seems to get confused inside of recursive functions:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">f</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">:</span><span class=\"n\">Int</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:{</span><span class=\"n\">y</span><span class=\"o\">:</span><span class=\"n\">Int</span><span class=\"w\"> </span><span class=\"bp\">//</span><span class=\"n\">y</span><span class=\"bp\">&gt;</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">∧</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"bp\">≠</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">}</span>\n<span class=\"o\">:=</span>\n<span class=\"k\">if</span><span class=\"w\"> </span><span class=\"n\">hcomp</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">&gt;=</span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"k\">then</span>\n<span class=\"w\">  </span><span class=\"bp\">⟨</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">+</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">omega</span><span class=\"o\">)</span><span class=\"bp\">⟩</span>\n<span class=\"k\">else</span>\n<span class=\"w\">  </span><span class=\"bp\">⟨</span><span class=\"w\"> </span><span class=\"n\">f</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">plausible</span><span class=\"bp\">⟩</span><span class=\"w\"> </span><span class=\"c1\">--error: (kernel) declaration has free variables '_tmp✝'</span>\n<span class=\"n\">termination_by</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"bp\">-</span><span class=\"n\">x</span><span class=\"o\">)</span><span class=\"bp\">.</span><span class=\"n\">toNat</span>\n<span class=\"n\">decreasing_by</span><span class=\"o\">{</span>\n<span class=\"w\">  </span><span class=\"n\">simp_wf</span>\n<span class=\"w\">  </span><span class=\"n\">omega</span>\n<span class=\"o\">}</span>\n</code></pre></div>\n<p>Right now my workaround is to check the conditions manually and output error messages using IO.println. </p>\n<p>Anyone else encountered similar issues? I would prefer to resolve this inside Lean, e.g. with a modified <code>plausible</code> command, rather than doing it in python; but I am not good in Lean metaprogramming (yet)...</p>",
        "id": 511856677,
        "sender_full_name": "GasStationManager",
        "timestamp": 1744503745
    },
    {
        "content": "<p>I think such issues with plausible should be posted in #general or on the plausible issue tracker, rather than in a stream that not all plausible users are subscribed to</p>",
        "id": 511856832,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1744503892
    },
    {
        "content": "<p>Thanks for the suggestion! I'll make a topic in #general that goes into details. Meanwhile I'm gonna make a wild prediction: soon Plausible and similar testing tools will become more relevant to AI, just like how these days work on REPLs are often closely associated with AI use cases. As REPLs lie on a critical path connecting LLMs to direct formal reasoning with Lean, Plausible and test tools will lie on a critical path connecting informal reasoning output from LLMs (proof sketches, code) to formal feed back from the ITP.</p>",
        "id": 511964454,
        "sender_full_name": "GasStationManager",
        "timestamp": 1744595466
    }
]