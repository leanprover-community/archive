[
    {
        "content": "<p>I know that in textbook MCTS the rollout stage involves playing the game randomly and checking whether it results in a win, or loss or a draw. I don't quite understand how this can be applied in a theorem proving context because it's more likely to lose than to win, and also the proof can be lengthened indefinitely with no end in sight.</p>",
        "id": 539685472,
        "sender_full_name": "(deleted)",
        "timestamp": 1758000435
    },
    {
        "content": "<p>My imagination tells me that in the context of Lean, moves can't easily be enumerated so an LLM is responsible for generating possible moves up to some limit. And because intermediate game states have to be preserved, virtual machine snapshotting is used.</p>",
        "id": 539685677,
        "sender_full_name": "(deleted)",
        "timestamp": 1758000602
    },
    {
        "content": "<p>This is why there is Infinibranch</p>",
        "id": 539685914,
        "sender_full_name": "(deleted)",
        "timestamp": 1758000761
    },
    {
        "content": "<p>Don't worry Morph what I say is based on publicly available information I'm not divulging your secrets</p>",
        "id": 539685983,
        "sender_full_name": "(deleted)",
        "timestamp": 1758000803
    },
    {
        "content": "<p>A paper on automated theorem proving tells me that in the rollout stage as random play is not possible we need to evaluate the move in a different way</p>",
        "id": 539686062,
        "sender_full_name": "(deleted)",
        "timestamp": 1758000850
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"511228\">Huỳnh Trần Khanh</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Monte.20Carlo.20tree.20search/near/539686062\">said</a>:</p>\n<blockquote>\n<p>A paper on automated theorem proving tells me that in the rollout stage as random play is not possible we need to evaluate the move in a different way</p>\n</blockquote>\n<p>A RL policy does this. The policy selects \"good\" moves in the case of games and \"good\" next steps in a proof  in the case of Lean. </p>\n<p>The literature has many articles that take this path. See, for example, Section 3.4 in <a href=\"https://arxiv.org/abs/2404.09939\">A Survey on Deep Learning for Theorem Proving</a> for a starting point on the literature.</p>\n<p>PS: I don't think you mean \"evaluate\" as traditionally that's the role of a value function or multiple runs in the case of GRPO, Dr GRPO, GSPO, or any other GRPO variant.</p>",
        "id": 539687331,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1758001685
    },
    {
        "content": "<p>Most applications of MCTS replace the random rollouts with another heuristic. Typically some form of neural network is trained to predict the value of each state. The alphazero paper explains the idea quite well.</p>",
        "id": 539688036,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1758002116
    },
    {
        "content": "<p>Oh I see I misunderstood your question. You really did mean \"evaluate\". In that case, my PS gives the answer, i.e. value function or multiple runs in the case of GRPO, Dr GRPO, GSPO, or any other GRPO variant.</p>",
        "id": 539693030,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1758004873
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"511228\">Huỳnh Trần Khanh</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Monte.20Carlo.20tree.20search/near/539685472\">said</a>:</p>\n<blockquote>\n<p>I know that in textbook MCTS the rollout stage involves playing the game randomly and checking whether it results in a win, or loss or a draw. I don't quite understand how this can be applied in a theorem proving context because it's more likely to lose than to win, and also the proof can be lengthened indefinitely with no end in sight.</p>\n</blockquote>\n<p>Maybe that's why you want some version of curriculum learning? The model might be able to get some of the easiest problems by luck, gradually knowing how to solve them during training, and then proceeds to some harder ones.</p>",
        "id": 539916318,
        "sender_full_name": "Wang Jingting",
        "timestamp": 1758073982
    }
]