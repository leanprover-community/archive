[
    {
        "content": "<p>Hello! I've seen many discussions about results of <em>big_name_like_Deepseek</em>-Prover, but, for them, the formalizations of problems are given (and fixed) in the specific benchmark. I've been looking at the <em>big_name_...</em>-Formalizers and noticed that they give out slightly different formalizations for the same problem, which obviously is not unexpected, but brings the following possibly philosophical question: </p>\n<p>Can such autoformalization be objectively judged at least somewhat automatically in the sense that, given two different attempts, one is obviously better and should be used instead of the other, or they are the same in quality? If such methods are unattainable at the moment, what role do formalizers even play in the whole 'AI for TP' race?</p>\n<p>For example, Kimina Autoformalizer, when choosing how to formalize \"Find the largest integer number smaller than sqrt(4)+sqrt(5). The answer is 4\", provides the following formalizations, both of which are able to be proven and are pretty much correct (the second one is probably a little bit worse, but I feel like, if a student rephrased the problem in such a way, they would still get good marks):</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span>\n\n<span class=\"kn\">open</span><span class=\"w\"> </span><span class=\"n\">Real</span>\n\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">my_favorite_theorem_1</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">IsGreatest</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℤ</span><span class=\"w\"> </span><span class=\"bp\">|</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&lt;</span><span class=\"w\"> </span><span class=\"bp\">√</span><span class=\"mi\">4</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"bp\">√</span><span class=\"mi\">5</span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"gr\">sorry</span>\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">my_favorite_theorem_2</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">⌊</span><span class=\"n\">Real</span><span class=\"bp\">.</span><span class=\"n\">sqrt</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">Real</span><span class=\"bp\">.</span><span class=\"n\">sqrt</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"bp\">⌋</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"gr\">sorry</span>\n</code></pre></div>",
        "id": 531398817,
        "sender_full_name": "V S",
        "timestamp": 1753716480
    },
    {
        "content": "<p>If you want something automated, I am sure you can just use an LLM as a judge after making a rubric. These have been shown to be effective in other contexts, so I see no reason they cannot be used here.</p>",
        "id": 531412655,
        "sender_full_name": "Justin Asher",
        "timestamp": 1753720623
    },
    {
        "content": "<p>(The point is that autoformalization, outside of type checking and handwritten tests, is not a verifiable domain.)</p>",
        "id": 531412786,
        "sender_full_name": "Justin Asher",
        "timestamp": 1753720679
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"374083\">@Scott Kovach</span> wrote an interesting <a href=\"https://cutfree.net/notes/autoformalization.html\">blog post</a> discussing the underlying philosophical question.</p>",
        "id": 531422369,
        "sender_full_name": "Jibiana Jakpor",
        "timestamp": 1753723788
    },
    {
        "content": "<p>I have been thinking off extending CodeBleu to Proofs I think it is a non-trivial yet relevant metric will be happy to hear thoughts on it.</p>",
        "id": 533600690,
        "sender_full_name": "Shashank Kirtania",
        "timestamp": 1754772360
    },
    {
        "content": "<p>Why proofs?   Also would you use it for RL training or just as a metric?</p>",
        "id": 533602170,
        "sender_full_name": "Jason Rute",
        "timestamp": 1754774287
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> RL training does sound relevant, but I am not super sure how to go about it with help of the metric, right now I am trying to use it as a metric and understand if it representative enough to be used for training purposes. (mostly analysis)</p>",
        "id": 535391776,
        "sender_full_name": "Shashank Kirtania",
        "timestamp": 1755725078
    },
    {
        "content": "<p>Also some relevant work to compare two formalizations:<br>\n<a href=\"https://arxiv.org/abs/2507.07399\">https://arxiv.org/abs/2507.07399</a></p>",
        "id": 535392533,
        "sender_full_name": "Shashank Kirtania",
        "timestamp": 1755725456
    },
    {
        "content": "<p>A possible additional evaluation metric could also be the cosine similarity between the text embeddings of the natural-language statement and its corresponding Lean code. Or does this approach have some potential disadvantages that I am missing?  <br>\n(I understand that for concepts which are outside of the training data distribution of the chosen embedding model the results will mostly likely be bad, but for more common statements/proofs etc this might be a viable approach..)</p>",
        "id": 545779378,
        "sender_full_name": "Paul Dietze",
        "timestamp": 1760817900
    },
    {
        "content": "<p>You're right to be skeptical, embedding models struggle with these nuances for a fundamental reason: natural language and Lean code don't actually exist in the same semantic space, even when describing identical mathematical concepts.<br>\nIf standard embeddings could meaningfully bridge NL and formal mathematics, we'd see much better performance in retrieval tasks over Mathlib (as projects like Lean Copilot have discovered).</p>",
        "id": 545785607,
        "sender_full_name": "Shashank Kirtania",
        "timestamp": 1760825462
    },
    {
        "content": "<p>I wasn't necessarily referring to existing off-the-shelf embedding models, but rather to potential future natural-language - Lean embedding models which would enforce closeness in the semantic space. My point was more long-term: such models could provide a complementary semantic signal given enough training data and even turn out to be better at capturing nuances than type and/or tree-based similarity measures. But this is just a hypothesis  and I would be very interested in counter-arguments.</p>\n<p>Apparently, a recently published paper is directly relevant to this post: <a href=\"https://arxiv.org/pdf/2510.15681\">https://arxiv.org/pdf/2510.15681</a> . Unfortunately, it seems that the embedding model weights were not released.</p>",
        "id": 545787723,
        "sender_full_name": "Paul Dietze",
        "timestamp": 1760828928
    }
]