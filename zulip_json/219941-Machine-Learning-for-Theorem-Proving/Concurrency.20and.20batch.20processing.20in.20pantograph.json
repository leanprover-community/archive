[
    {
        "content": "<p>Here is a brief conversation over email with <span class=\"user-mention\" data-user-id=\"599027\">@Leni Aniva</span> that I'm moving here:</p>\n<p><span class=\"user-mention\" data-user-id=\"901685\">@gambpang</span> I am a fan of your Pantograph software.Â  Have you considered either a concurrent or batch oriented interface?Â  I would like to make use of something like this and am happy to do some development.Â  If there is already an effort in progress, I can pitch in with development or code review. If not, I could submit a patch to your repository.</p>\n<p><span class=\"user-mention\" data-user-id=\"599027\">@Leni Aniva</span> Thanks for the interest! Python seems to favour multiprocessing compared to multithreading, so the current support for parallelism we have in PyPantograph is that you can pickle a goal and send it to another process as a file. Unifying goals after a proof finishes, however, is a  difficult algorithmic problem and I plan to add this feature in the summer.</p>\n<p>... continuing:</p>\n<p>I had in mind to add concurrency in the Lean executable by exposing a TCP socket to handle commands concurrently across a tree of shared states.  The specific functionality that I would like to support is to share a complex environment in memory among many threads and avoid start-up costs.  For a first cut, there would be branching but not merging (at least not as an explicit feature).</p>",
        "id": 513984879,
        "sender_full_name": "gambpang",
        "timestamp": 1745453940
    },
    {
        "content": "<p>so you want the compiled Lean executable (pantograph repl or smth) itself to act like a server?</p>",
        "id": 513985417,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1745454332
    },
    {
        "content": "<p>One of Dr. Avigad's students at CMU tried that. It was really hard</p>",
        "id": 513985434,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1745454346
    },
    {
        "content": "<p>Also I don't see what benefit this could provide over the current solution of having one instance per process. If a buggy tactic crashes Pantograph it would be hard to recover</p>",
        "id": 513985557,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1745454423
    },
    {
        "content": "<p>My solution is to have multiple instances of whatever proof finding program you're running. If one instance crashes, use industry standard methods of error recovery and generate plenty of checkpoints. This is also the solution used by Amazon and Harmonic afaik</p>",
        "id": 513986056,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1745454658
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/513985557\">said</a>:</p>\n<blockquote>\n<p>Also I don't see what benefit this could provide over the current solution of having one instance per process. If a buggy tactic crashes Pantograph it would be hard to recover</p>\n</blockquote>\n<p>Do you think it would be worth trying a load-balanced cluster where each node is a Lean server instance (using the TCP approach internally for efficiency within that node), coupled with frequent checkpointing? This would provide instance-level fault isolation, while still gaining <em>some</em> resource efficiency. I am mainly concerned about starting many server instances, like <span class=\"user-mention\" data-user-id=\"901685\">@gambpang</span>  mentioned, and the resulting high RAM and CPU usage.</p>",
        "id": 513995520,
        "sender_full_name": "Justin Asher",
        "timestamp": 1745460893
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"599027\">@Leni Aniva</span> Thanks for the feedback!  I'm curious what sort of workloads you've run in the one-instance per process setting.  I'm thinking of trying to saturate my hardware by batching examples as much as possible.  Also, I'm fairly new to this scene and building intuition.  How common is it for a buggy tactic to crash Pantograph?  Are there some especially buggy tactics to treat with more care?</p>",
        "id": 513997537,
        "sender_full_name": "gambpang",
        "timestamp": 1745462261
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/513995520\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/513985557\">said</a>:</p>\n<blockquote>\n<p>Also I don't see what benefit this could provide over the current solution of having one instance per process. If a buggy tactic crashes Pantograph it would be hard to recover</p>\n</blockquote>\n<p>Do you think it would be worth trying a load-balanced cluster where each node is a Lean server instance (using the TCP approach internally for efficiency within that node), coupled with frequent checkpointing? This would provide instance-level fault isolation, while still gaining <em>some</em> resource efficiency. I am mainly concerned about starting many server instances, like <span class=\"user-mention silent\" data-user-id=\"901685\">gambpang</span>  mentioned, and the resulting high RAM and CPU usage.</p>\n</blockquote>\n<p>By server do you mean LSP server or just the pantograph repl? the repl should have a much smaller footprint.</p>\n<p>We can have load balancing but it should be done outside of Lean</p>",
        "id": 514011637,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1745470249
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"901685\">gambpang</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/513997537\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span> Thanks for the feedback!  I'm curious what sort of workloads you've run in the one-instance per process setting.  I'm thinking of trying to saturate my hardware by batching examples as much as possible.  Also, I'm fairly new to this scene and building intuition.  How common is it for a buggy tactic to crash Pantograph?  Are there some especially buggy tactics to treat with more care?</p>\n</blockquote>\n<p>aesop used to be pretty buggy. I have the timeout feature as a safeguard in Pantograph, but a tactic can just ignore that. Ive had aesop crash my machine and max out the ram. simp could go into an infinite loop. My go-to is to avoid these tactics for now.</p>",
        "id": 514011776,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1745470335
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/514011637\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"780541\">Justin Asher</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/513995520\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Concurrency.20and.20batch.20processing.20in.20pantograph/near/513985557\">said</a>:</p>\n<blockquote>\n<p>Also I don't see what benefit this could provide over the current solution of having one instance per process. If a buggy tactic crashes Pantograph it would be hard to recover</p>\n</blockquote>\n<p>Do you think it would be worth trying a load-balanced cluster where each node is a Lean server instance (using the TCP approach internally for efficiency within that node), coupled with frequent checkpointing? This would provide instance-level fault isolation, while still gaining <em>some</em> resource efficiency. I am mainly concerned about starting many server instances, like <span class=\"user-mention silent\" data-user-id=\"901685\">gambpang</span>  mentioned, and the resulting high RAM and CPU usage.</p>\n</blockquote>\n<p>By server do you mean LSP server or just the pantograph repl? the repl should have a much smaller footprint.</p>\n<p>We can have load balancing but it should be done outside of Lean</p>\n</blockquote>\n<p>That makes sense. I will have to play around with the REPL more. The Kimina Lean Server seems to implement this idea well (using pre-started Lean REPL workers). Thanks for the information.</p>",
        "id": 514184868,
        "sender_full_name": "Justin Asher",
        "timestamp": 1745516132
    },
    {
        "content": "<p>hey <span class=\"user-mention\" data-user-id=\"780541\">@Justin Asher</span> you might find this helpful: <a class=\"message-link\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Serverless.20Pantograph.20on.20Morph.20Cloud/near/514487185\">#Machine Learning for Theorem Proving &gt; Serverless Pantograph on Morph Cloud @ ðŸ’¬</a>  - using the open-source template in that thread, it's quite easy to put a load balancer / orchestrator in front of our Pantograph wrapper and start from snapshots to handle incoming requests.</p>",
        "id": 514487463,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1745632382
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> This is super helpful! Thanks for letting me know. Using snapshots seems like the right approach here, given that you can quickly start VMs.</p>",
        "id": 514490987,
        "sender_full_name": "Justin Asher",
        "timestamp": 1745635174
    }
]