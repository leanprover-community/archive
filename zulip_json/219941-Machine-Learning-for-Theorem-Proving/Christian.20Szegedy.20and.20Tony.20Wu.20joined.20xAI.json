[
    {
        "content": "<p><a href=\"https://x.ai/\">https://x.ai/</a></p>",
        "id": 374699075,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1689184009
    },
    {
        "content": "<p>Is Markus Rabe still at Google?  Do you know if anyone is left at Google working on AI for mathematics/formalization/theorem proving?</p>",
        "id": 374746745,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689196496
    },
    {
        "content": "<p>And are Szegedy or Tony working on the same topics still at xAI?</p>",
        "id": 374747160,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689196632
    },
    {
        "content": "<p><a href=\"https://www.linkedin.com/in/markusnrabe/\">https://www.linkedin.com/in/markusnrabe/</a> suggests Markus is not</p>",
        "id": 374747369,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1689196728
    },
    {
        "content": "<p>It might also be worth mentioning that some of the HTPS authors at Meta (many also authors on DSP) first switched to working on LLMs, namely LLaMA and then left to start a company about a month ago: <a href=\"https://twitter.com/aimistral/status/1670133250130427905\">https://twitter.com/aimistral/status/1670133250130427905</a></p>",
        "id": 374747500,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689196790
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/374746745\">said</a>:</p>\n<blockquote>\n<p>Is Marcus Rabe still at Google?  Do you know if anyone is left at Google working on AI for mathematics/formalization/theorem proving?</p>\n</blockquote>\n<p>My understanding is that the N2Formal group at Google no longer works on automated theorem proving; I don't know for sure though. Like the HTPS authors you mentioned, Markus left to join a startup. Seems like large industry companies are turning to focus on the emergent chatbot tech over other lines of research. I wonder if the same thing is happening at DeepMind after Google's announcement merging it with Google Brain.</p>",
        "id": 374754442,
        "sender_full_name": "Andreas Gittis",
        "timestamp": 1689199520
    },
    {
        "content": "<blockquote>\n<p>And are Szegedy or Tony working on the same topics still at xAI?</p>\n</blockquote>\n<p>Seems they're still working on math and reasoning according to Tony's <a href=\"https://twitter.com/Yuhu_ai_/status/1679194328672133120\">tweet</a> and Twitter bio. I was aware that Christian left <a href=\"https://twitter.com/ChrSzegedy/status/1643463787721424897\">since April</a>, but heard about Tony's departure only from the NY Times article. Interestingly that Tony's <a href=\"https://yuhuaiwu.github.io/\">webpage</a> and Markus's LinkedIn both say \"stealth startup\" but they turn out to be different ...</p>",
        "id": 374759626,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1689201917
    },
    {
        "content": "<p>Also see <a href=\"https://twitter.com/TheGregYang/status/1679168317897211910\">announcement</a> from Greg Yang:</p>\n<blockquote>\n<p>Finally launched <a href=\"http://x.ai\">http://x.ai</a>!<br>\nThe mathematics of deep learning is profound, beautiful, and unreasonably effective. Developing the \"theory of everything\" for large neural networks will be central to taking AI to the next level. Conversely, this AI will enable everyone to understand our mathematical universe in ways unimaginable before.<br>\nMath for AI and AI for math!<br>\nAny mathematician/theorist excited about this needs to DM me!</p>\n</blockquote>",
        "id": 374760453,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1689202325
    },
    {
        "content": "<blockquote>\n<p>Do you know if anyone is left at Google working on AI for mathematics/formalization/theorem proving?</p>\n</blockquote>\n<p>Also, don't forget DeepMind is now part of Google, and given <a href=\"#narrow/stream/113486-announce/topic/Paid.20studies.20to.20formalize.20math.20problems.20in.20Lean\">what data they're collecting</a> it's conceivable they've taken up the IMO grand challenge.</p>",
        "id": 374767606,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1689206549
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224323\">Junyan Xu</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/374759626\">said</a>:</p>\n<blockquote>\n<p>Interestingly that Tony's <a href=\"https://yuhuaiwu.github.io/\">webpage</a> and Markus's LinkedIn both say \"stealth startup\" but they turn out to be different ...</p>\n</blockquote>\n<p>Ah okay, I'll remove the name from my previous post.</p>",
        "id": 374769191,
        "sender_full_name": "Andreas Gittis",
        "timestamp": 1689207453
    },
    {
        "content": "<p>Is it public that DeepMind is collecting that data?</p>",
        "id": 374789968,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689216651
    },
    {
        "content": "<p>One can see the name of the researcher who posted the study before taking it, so I think it's public to anyone who registered a Prolific account via the link.</p>\n<p>On the other hand, Geoffrey Irving (coauthor of the paper <em><a href=\"https://arxiv.org/abs/1701.06972\">Deep Network Guided Proof Search</a></em> (2017) with Szegedy and Kaliszyk, now a safety researcher at DeepMind) quite impressively <a href=\"#narrow/stream/116395-maths/topic/Hartogs's.20theorem\">formalized Hartogs's theorem</a> a while ago. I'd guess some DeepMind people are still pursuing <a href=\"https://www.deepmind.com/blog/identifying-and-eliminating-bugs-in-learned-predictive-models\">this line of research</a> (<a href=\"http://web.archive.org/web/20190328181618/https://deepmind.com/blog/robust-and-verified-ai/\">earlier version</a>); also interesting to see <a href=\"https://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html\">AutoBound</a> coming out of Google lately.</p>",
        "id": 374802197,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1689224202
    },
    {
        "content": "<p>On twitter now, there is a <a href=\"https://twitter.com/xai\">stream from x.ai</a> discussing the new venture.  Talia Ringer is <a href=\"https://twitter.com/TaliaRinger/status/1679946547532816384\">livetweeting</a> it for anyone who wants the short version.</p>",
        "id": 375379131,
        "sender_full_name": "Ryan McCorvie",
        "timestamp": 1689365531
    },
    {
        "content": "<p>It's quite surprising to see that the majority of individuals from leading organizations (OpenAI, Meta, Google, and the like) who were previously involved in automatic theorem proving seem to have shifted their focus. As a novice in this area, I'm genuinely interested to gather your thoughts on the possible reasons for this trend. Could it be:</p>\n<p>1  These major companies are primarily focused on advancing Large Language Models and may perceive theorem proving as less immediately applicable to their operational needs?<br>\nOr 2  They are pioneers in this field. And the impressive work these individuals have done in this field has opened doors to exciting startup opportunities they couldn't pass up?</p>",
        "id": 376824940,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1689816826
    },
    {
        "content": "<p><a href=\"https://twitter.com/AlbertQJiang/status/1679598310045220866?s=20\">https://twitter.com/AlbertQJiang/status/1679598310045220866?s=20</a></p>",
        "id": 376879649,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689840033
    },
    {
        "content": "<p><a href=\"https://twitter.com/jessemhan/status/1679602978339057669?s=20\">https://twitter.com/jessemhan/status/1679602978339057669?s=20</a></p>",
        "id": 376879851,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689840076
    },
    {
        "content": "<p>(That was Albert expressing the same thought and Jesse giving his personal experience.)</p>",
        "id": 376880132,
        "sender_full_name": "Jason Rute",
        "timestamp": 1689840131
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Thanks for the info! I didn't know about that conversation. It looks more like the second point. Thanks again <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 376905589,
        "sender_full_name": "Dongwei Jiang",
        "timestamp": 1689845209
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"616750\">Dongwei Jiang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/376824940\">said</a>:</p>\n<blockquote>\n<p>It's quite surprising to see that the majority of individuals from leading organizations (OpenAI, Meta, Google, and the like) who were previously involved in automatic theorem proving seem to have shifted their focus. As a novice in this area, I'm genuinely interested to gather your thoughts on the possible reasons for this trend. Could it be:</p>\n<p>1  These major companies are primarily focused on advancing Large Language Models and may perceive theorem proving as less immediately applicable to their operational needs?<br>\nOr 2  They are pioneers in this field. And the impressive work these individuals have done in this field has opened doors to exciting startup opportunities they couldn't pass up?</p>\n</blockquote>\n<p>I think an important reasoning for people leaving theorem proving is that their AGI timelines have drastically shortened over the past few years. I came into theorem proving thinking that a superhuman theorem prover would be a major milestone on the long journey towards AGI, like AlphaZero was. Now it looks like we will solve AGI and get theorem proving as a special case.</p>",
        "id": 377035458,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1689868866
    },
    {
        "content": "<p>The definition of AGI here could use a bit clarification. If it's about computers doing all normal human's cognitive work, satisfying both \"G\" and \"I\", it might still not generalize \"up\" to the stronger reasoning skills mathematicians have.</p>",
        "id": 377037434,
        "sender_full_name": "Jonathan Yan",
        "timestamp": 1689869194
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"443213\">Jonathan Yan</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/377037434\">said</a>:</p>\n<blockquote>\n<p>The definition of AGI here could use a bit clarification. If it's about computers doing all normal human's cognitive work, satisfying both \"G\" and \"I\", it might still not generalize \"up\" to the stronger reasoning skills mathematicians have.</p>\n</blockquote>\n<p>If we're at median human level but not yet at academic mathematician level, we can put in more compute and more data and get to academic mathematician level. </p>\n<p>What I said above is not quite precise, because models can be strongly superhuman along some axes while far behind humans on others. This is already the case: gpt-4 is superhuman in knowing facts but far worse than humans at executing long term plans. The more precise statement is \"once models are median human level along all axes of intelligence then getting them to academic mathematician level along all axes is a matter of compute and data\".</p>",
        "id": 377040511,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1689869783
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"284997\">Zhangir Azerbayev</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/377040511\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"443213\">Jonathan Yan</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/377037434\">said</a>:</p>\n<blockquote>\n<p>The definition of AGI here could use a bit clarification. If it's about computers doing all normal human's cognitive work, satisfying both \"G\" and \"I\", it might still not generalize \"up\" to the stronger reasoning skills mathematicians have.</p>\n</blockquote>\n<p>If we're at median human level but not yet at academic mathematician level, we can put in more compute and more data and get to academic mathematician level. </p>\n<p>What I said above is not quite precise, because models can be strongly superhuman along some axes while far behind humans on others. This is already the case: gpt-4 is superhuman in knowing facts but far worse than humans at executing long term plans. The more precise statement is \"once models are median human level along all axes of intelligence then getting them to academic mathematician level along all axes is a matter of compute and data\".</p>\n</blockquote>\n<p>This is not obvious to me, because it's cheap to produce the data for lots of median level human tasks. What if a large model learns to solve all of these relatively easy tasks in somewhat wrong ways, for example by overfitting. Then more compute and data will not improve the system's capability for more difficult tasks.</p>",
        "id": 377044965,
        "sender_full_name": "Jonathan Yan",
        "timestamp": 1689870716
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"443213\">Jonathan Yan</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/377044965\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"284997\">Zhangir Azerbayev</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/377040511\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"443213\">Jonathan Yan</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Christian.20Szegedy.20and.20Tony.20Wu.20joined.20xAI/near/377037434\">said</a>:</p>\n<blockquote>\n<p>The definition of AGI here could use a bit clarification. If it's about computers doing all normal human's cognitive work, satisfying both \"G\" and \"I\", it might still not generalize \"up\" to the stronger reasoning skills mathematicians have.</p>\n</blockquote>\n<p>If we're at median human level but not yet at academic mathematician level, we can put in more compute and more data and get to academic mathematician level. </p>\n<p>What I said above is not quite precise, because models can be strongly superhuman along some axes while far behind humans on others. This is already the case: gpt-4 is superhuman in knowing facts but far worse than humans at executing long term plans. The more precise statement is \"once models are median human level along all axes of intelligence then getting them to academic mathematician level along all axes is a matter of compute and data\".</p>\n</blockquote>\n<p>This is not obvious to me, because it's cheap to produce the data for lots of median level human tasks. What if a large model learns to solve all of these relatively easy tasks in somewhat wrong ways, for example by overfitting. Then more compute and data will not improve the system's capability for more difficult tasks.</p>\n</blockquote>\n<p>A few points. </p>\n<ol>\n<li>\n<p>Learning the entire joint distribution of human performance is different from simulating median human performance. In particular, if we have the entire joint distribution of human performance we can condition on the desired slice of the distribution. In deep learning, this idea is called <a href=\"https://evjang.com/2021/10/23/generalization.html\">\"just ask for generalization\"</a>. </p>\n</li>\n<li>\n<p>If by \"overfit\" you meant overfit to the pre-training data, I don't think we will have the problem of overfitting to 10 trillion tokens any time soon.</p>\n</li>\n<li>\n<p>When doing RLHF/RLAIF for something like chat models, the reward model is very brittle and easy to overfit to. However, mathematics is a particular domain where it's much easier to create robust reward models. For example, you can think of Lean as a very good GOFAI reward model for mathematical correctness. Of course, mathematical correctness is not all we care about, and we also want a reward model that encodes mathematical beauty.</p>\n</li>\n</ol>",
        "id": 377048826,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1689871593
    },
    {
        "content": "<p>Re 2: by \"overfit\" I mean not learning the appropriate algorithm. For example, a model that solves the IMO challenge by memorizing all patterns of competitive math problems probably cannot generalize to Fermat's Last Theorem? A human IMO expert, however, with limited memory and inference budget, cannot memorize then try all these problem patterns. The expert must generate many such patterns in the process of solving a problem, ie an algorithm that \"rediscovers\" many such math patterns on the fly. This algo is more \"general\" so gets a greater chance when attempting harder math like FLT.</p>\n<p>This is consistent with your point 1. Today's large models with huge compute budgets are under less pressure than humans to \"think properly\", so for example they can memorize many simple logos, some incorrect heuristics and brute force to solve common tasks. This learned skill plausibly generalizes to many unseen tasks, even to the point of doing all human cognitive work that has industry applications, but fails on qualitatively harder tasks. This is the scenario I imagined earlier where we create AGI in a practical sense but still cannot solve math.</p>\n<p>Strongly agree that it'll be a huge step forward if \"mathematical beauty\" can be encoded!</p>",
        "id": 377133410,
        "sender_full_name": "Jonathan Yan",
        "timestamp": 1689896080
    },
    {
        "content": "<p>Contrasting Olympiad problems and \"Fermat's last theorem\" is misleading in terms of what it takes. Mathematics advances in a series of steps, each of which can come from (among other things):</p>\n<ul>\n<li>By analogy from previous mathematics</li>\n<li>Combining ideas from different places. </li>\n<li>Learning from examples: for instance deeply understanding an example of a construction of Jorgensen (of a hyperbolic structure) was a key part of Thurston's geometrrization.</li>\n<li>Abstraction and generalization.</li>\n<li>Cleverness (a la Olympiad problems).</li>\n<li>Refactoring clever ideas to understand (to quote Dennis Sullivan: \"why be clever when we can be intelligent\").</li>\n<li>Trying a lot of stuff and retaining what works. Also understanding why something fails after making progress and modifying.</li>\n</ul>\n<p>A great piece of mathematics involves a lot of such steps, building on different pieces of mathematics and using different strengths in different parts. </p>\n<p>If each step can be accomplished by AI then putting them together may just be engineering. Till recently the missing capability was reasoning by analogy (including generalization - which is analogy with an example/special case).</p>\n<p>Also while aesthetic appeal has a role in mathematics (personally I prefer though Poincare's description of this as \"Joy of comprehension\") it is not necessary for guiding mathematics (at least in areas I know). At any time there are clear ultimate goals, specific test cases which are beyond the limits of present techniques, explicit open problems, newly developed techniques that can be extended, and various other guides to what is desirable worthwhile progress. </p>\n<p>I am not claiming that AI will reach the level of say Gromov - I have no basis for judging whether this will happen one way or the other. All I am saying is that looking at the end result of research mathematics may mislead people into thinking this is a mysterious activity of an entirely different nature from other reasoning.</p>",
        "id": 377165990,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1689911275
    },
    {
        "content": "<p>If mathematics is not qualitatively harder, as suggested by the above comment, more precisely, math progress can come from the culmination of lots of standard reasoning steps, but not necessarily involving a mysterious step like the \"Joy of comprehension\", then I agree with the feeling expressed earlier that we will \"get theorem proving as a special case\" of AGI.</p>",
        "id": 377168045,
        "sender_full_name": "Jonathan Yan",
        "timestamp": 1689912362
    },
    {
        "content": "<p>Language modelling is very useful for ATP. A lot of companies close doors to their models so I can't mess around and hack stuff. Some people saw the benefits in ACTUALLY opening up the best models to the entire community and decided to build their own.</p>",
        "id": 377401327,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1689965972
    }
]