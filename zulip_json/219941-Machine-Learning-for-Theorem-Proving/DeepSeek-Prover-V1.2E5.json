[
    {
        "content": "<p>We have submitted a new technical report on <a href=\"https://arxiv.org/abs/2408.08152\">arXiv</a> introducing DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4. This model demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the miniF2F benchmark (63.5%) and the ProofNet benchmark (25.3%).</p>\n<p>DeepSeek-Prover-V1.5 enhances its predecessor by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with a specialization in formal mathematical languages, it undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths.</p>\n<p>The base, SFT, and RL models have been open-sourced on <a href=\"https://huggingface.co/collections/deepseek-ai/deepseek-prover-v15-66beb212ae70890c90f24176\">Hugging Face</a>.</p>",
        "id": 462694813,
        "sender_full_name": "Huajian Xin",
        "timestamp": 1723781004
    },
    {
        "content": "<p>A message was moved from this topic to <a class=\"stream-topic\" data-stream-id=\"180721\" href=\"/#narrow/stream/180721-mathlib-maintainers/topic/spam\">#mathlib maintainers &gt; spam</a> by <span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span>.</p>",
        "id": 462700513,
        "sender_full_name": "Notification Bot",
        "timestamp": 1723783461
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"481527\">Huajian Xin</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/DeepSeek-Prover-V1.2E5/near/462694813\">said</a>:</p>\n<blockquote>\n<p>We have submitted a new technical report on <a href=\"https://arxiv.org/abs/2408.08152\">arXiv</a> introducing DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4. This model demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the miniF2F benchmark (63.5%) and the ProofNet benchmark (25.3%).</p>\n<p>DeepSeek-Prover-V1.5 enhances its predecessor by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with a specialization in formal mathematical languages, it undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths.</p>\n<p>The base, SFT, and RL models have been open-sourced on <a href=\"https://huggingface.co/collections/deepseek-ai/deepseek-prover-v15-66beb212ae70890c90f24176\">Hugging Face</a>.</p>\n</blockquote>\n<p>Hi, I just tried running the model, but I'm getting errors regarding \"DeepseekConfig\" and \"DeepseekV2Config\". </p>\n<p>More specifically, running the huggingface code directly gives the error:</p>\n<blockquote>\n<p>File \"./.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Prover-V1.5-RL/944fec2f7a2006a2a22ded13b187feb4304b48c2/modeling_deepseek.py\", line 57, in &lt;module&gt;<br>\n    from .configuration_deepseek import DeepseekV2Config<br>\nImportError: cannot import name 'DeepseekV2Config' from 'transformers_modules.deepseek-ai.DeepSeek-Prover-V1.5 RL.944fec2f7a2006a2a22ded13b187feb4304b48c2.configuration_deepseek' (./.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Prover-V1.5-RL/944fec2f7a2006a2a22ded13b187feb4304b48c2/configuration_deepseek.py)</p>\n</blockquote>\n<p>Modifying the class name in the configuration_deepseek.py file (from DeepseekConfig to DeepseekV2Config) gives another error essentially saying that \"somewhere else is trying to import DeepseekConfig and can't find this attribute\".</p>\n<p>Are the DeepseekConfig and DeepseekV2Config classes supposed to be the same thing? Would renaming them in the code be a safe fix for these errors?</p>",
        "id": 462731969,
        "sender_full_name": "Yufan Zhao",
        "timestamp": 1723793711
    },
    {
        "content": "<p>Based on the weight dictionary (particularly the <code>self_attn</code> keys), this seems to be using the <a href=\"https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat\">DeepSeek MoE architecture</a> which is not the same as the <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\">DeepSeek V2 architecture</a>.</p>\n<p>These are both implemented in vLLM:<br>\n<a href=\"https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/deepseek.py\">https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/deepseek.py</a><br>\n<a href=\"https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/deepseek_v2.py\">https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/deepseek_v2.py</a></p>\n<p>but only V2 has a WIP PR to huggingface/transformers. Maybe people probably forgot about DeepSeek MoE now that DeepSeek V2 is out. If I have time I'll send a PR, otherwise you can raise an issue with DeepSeek team.</p>",
        "id": 462778862,
        "sender_full_name": "llllvvuu",
        "timestamp": 1723812093
    },
    {
        "content": "<p>For MacOS, I have PR'd: <a href=\"https://github.com/ml-explore/mlx-examples/pull/942\">https://github.com/ml-explore/mlx-examples/pull/942</a></p>\n<p>I will make a similar PR for huggingface/transformers when I get a chance</p>",
        "id": 462825095,
        "sender_full_name": "llllvvuu",
        "timestamp": 1723827271
    },
    {
        "content": "<p>This should work for running with huggingface/transformers:</p>\n<ol>\n<li>Look in the following directories for modeling_deepseek.py</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-Prover-V1.5-RL/snapshots\n~/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Prover-V1.5-RL\n</code></pre></div>\n<ol start=\"2\">\n<li>Replace with the modeling_deepseek.py from DeepSeek MoE V1: <a href=\"https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py\">https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py</a></li>\n<li>Try again:</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForCausalLM</span><span class=\"p\">,</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">,</span> <span class=\"n\">TextStreamer</span><span class=\"p\">,</span> <span class=\"n\">pipeline</span>\n\n<span class=\"n\">model_id</span> <span class=\"o\">=</span> <span class=\"s2\">\"deepseek-ai/DeepSeek-Prover-V1.5-RL\"</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForCausalLM</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_id</span><span class=\"p\">,</span> <span class=\"n\">device_map</span><span class=\"o\">=</span><span class=\"s2\">\"auto\"</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_id</span><span class=\"p\">)</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">TextStreamer</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">)</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"text-generation\"</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">streamer</span><span class=\"o\">=</span><span class=\"n\">streamer</span>\n<span class=\"p\">)</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"import Mathlib</span>\n<span class=\"s2\">import Aesop</span>\n\n<span class=\"s2\">set_option maxHeartbeats 0</span>\n\n<span class=\"s2\">open BigOperators Real Nat Topology Rat</span>\n\n<span class=\"s2\">/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?</span>\n<span class=\"s2\">Show that it is $</span><span class=\"se\">\\\\</span><span class=\"s2\">frac{2</span><span class=\"se\">\\\\</span><span class=\"s2\">sqrt</span><span class=\"si\">{3}</span><span class=\"s2\">}</span><span class=\"si\">{3}</span><span class=\"s2\">$.-/</span>\n<span class=\"s2\">theorem amc12b_2003_p6 (a r : ℝ) (u : ℕ → ℝ) (h₀ : ∀ k, u k = a * r ^ k) (h₁ : u 1 = 2)</span>\n<span class=\"s2\">  (h₂ : u 3 = 6) : u 0 = 2 / Real.sqrt 3 ∨ u 0 = -(2 / Real.sqrt 3) := by\"\"\"</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">pipe</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">)</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">\"generated_text\"</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Currently working on integration into the main repo: <a href=\"https://github.com/huggingface/transformers/pull/32862\">https://github.com/huggingface/transformers/pull/32862</a><br>\nAs well as as to fix the HF custom code: <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/discussions/1\">https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/discussions/1</a></p>\n<p>It is extremely slow on MacBook since bitsandbytes quantization only supports CUDA. So, probably those with CUDA will get better performance with vLLM, while those with MacBook will get better performance with mlx_lm</p>",
        "id": 462891997,
        "sender_full_name": "llllvvuu",
        "timestamp": 1723864072
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"607118\">llllvvuu</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/DeepSeek-Prover-V1.2E5/near/462891997\">said</a>:</p>\n<blockquote>\n<p>This should work for running with huggingface/transformers:</p>\n<ol>\n<li>Look in the following directories for modeling_deepseek.py</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-Prover-V1.5-RL/snapshots\n~/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Prover-V1.5-RL\n</code></pre></div>\n<ol start=\"2\">\n<li>Replace with the modeling_deepseek.py from DeepSeek MoE V1: <a href=\"https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py\">https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py</a></li>\n<li>Try again:</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForCausalLM</span><span class=\"p\">,</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">,</span> <span class=\"n\">TextStreamer</span><span class=\"p\">,</span> <span class=\"n\">pipeline</span>\n\n<span class=\"n\">model_id</span> <span class=\"o\">=</span> <span class=\"s2\">\"deepseek-ai/DeepSeek-Prover-V1.5-RL\"</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForCausalLM</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_id</span><span class=\"p\">,</span> <span class=\"n\">device_map</span><span class=\"o\">=</span><span class=\"s2\">\"auto\"</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_id</span><span class=\"p\">)</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">TextStreamer</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">)</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"text-generation\"</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">streamer</span><span class=\"o\">=</span><span class=\"n\">streamer</span>\n<span class=\"p\">)</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"import Mathlib</span>\n<span class=\"s2\">import Aesop</span>\n\n<span class=\"s2\">set_option maxHeartbeats 0</span>\n\n<span class=\"s2\">open BigOperators Real Nat Topology Rat</span>\n\n<span class=\"s2\">/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?</span>\n<span class=\"s2\">Show that it is $</span><span class=\"se\">\\\\</span><span class=\"s2\">frac{2</span><span class=\"se\">\\\\</span><span class=\"s2\">sqrt</span><span class=\"si\">{3}</span><span class=\"s2\">}</span><span class=\"si\">{3}</span><span class=\"s2\">$.-/</span>\n<span class=\"s2\">theorem amc12b_2003_p6 (a r : ℝ) (u : ℕ → ℝ) (h₀ : ∀ k, u k = a * r ^ k) (h₁ : u 1 = 2)</span>\n<span class=\"s2\">  (h₂ : u 3 = 6) : u 0 = 2 / Real.sqrt 3 ∨ u 0 = -(2 / Real.sqrt 3) := by\"\"\"</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">pipe</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">)</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">\"generated_text\"</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Currently working on integration into the main repo: <a href=\"https://github.com/huggingface/transformers/pull/32862\">https://github.com/huggingface/transformers/pull/32862</a><br>\nAs well as as to fix the HF custom code: <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/discussions/1\">https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/discussions/1</a></p>\n<p>It is extremely slow on MacBook since bitsandbytes quantization only supports CUDA. So, probably those with CUDA will get better performance with vLLM, while those with MacBook will get better performance with mlx_lm</p>\n</blockquote>\n<p>i tried using the hugging face transformer like in the example above, but I'm getting </p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"bp\">$</span><span class=\"w\"> </span><span class=\"n\">python3</span><span class=\"w\"> </span><span class=\"n\">run_model</span><span class=\"bp\">.</span><span class=\"n\">py</span>\n<span class=\"n\">The</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">for</span><span class=\"w\"> </span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"w\"> </span><span class=\"n\">contains</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code</span><span class=\"w\"> </span><span class=\"n\">which</span><span class=\"w\"> </span><span class=\"n\">must</span><span class=\"w\"> </span><span class=\"n\">be</span><span class=\"w\"> </span><span class=\"n\">executed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">correctly</span><span class=\"w\"> </span><span class=\"n\">load</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">model</span><span class=\"bp\">.</span><span class=\"w\"> </span><span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">inspect</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"w\"> </span><span class=\"k\">at</span><span class=\"w\"> </span><span class=\"n\">https</span><span class=\"o\">:</span><span class=\"bp\">//</span><span class=\"n\">hf</span><span class=\"bp\">.</span><span class=\"n\">co</span><span class=\"bp\">/</span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"bp\">.</span>\n<span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">avoid</span><span class=\"w\"> </span><span class=\"n\">this</span><span class=\"w\"> </span><span class=\"n\">prompt</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">future</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">passing</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">argument</span><span class=\"w\"> </span><span class=\"ss\">`trust_remote_code</span><span class=\"bp\">=</span><span class=\"n\">True</span><span class=\"bp\">`.</span>\n\n<span class=\"n\">Do</span><span class=\"w\"> </span><span class=\"n\">you</span><span class=\"w\"> </span><span class=\"n\">wish</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code?</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"n\">y</span><span class=\"bp\">/</span><span class=\"n\">N</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"n\">y</span>\n<span class=\"n\">The</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">for</span><span class=\"w\"> </span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"w\"> </span><span class=\"n\">contains</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code</span><span class=\"w\"> </span><span class=\"n\">which</span><span class=\"w\"> </span><span class=\"n\">must</span><span class=\"w\"> </span><span class=\"n\">be</span><span class=\"w\"> </span><span class=\"n\">executed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">correctly</span><span class=\"w\"> </span><span class=\"n\">load</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">model</span><span class=\"bp\">.</span><span class=\"w\"> </span><span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">inspect</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"w\"> </span><span class=\"k\">at</span><span class=\"w\"> </span><span class=\"n\">https</span><span class=\"o\">:</span><span class=\"bp\">//</span><span class=\"n\">hf</span><span class=\"bp\">.</span><span class=\"n\">co</span><span class=\"bp\">/</span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"bp\">.</span>\n<span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">avoid</span><span class=\"w\"> </span><span class=\"n\">this</span><span class=\"w\"> </span><span class=\"n\">prompt</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">future</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">passing</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">argument</span><span class=\"w\"> </span><span class=\"ss\">`trust_remote_code</span><span class=\"bp\">=</span><span class=\"n\">True</span><span class=\"bp\">`.</span>\n\n<span class=\"n\">Do</span><span class=\"w\"> </span><span class=\"n\">you</span><span class=\"w\"> </span><span class=\"n\">wish</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code?</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"n\">y</span><span class=\"bp\">/</span><span class=\"n\">N</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"n\">y</span>\n</code></pre></div>\n<p>this gives </p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">ImportError</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">This</span><span class=\"w\"> </span><span class=\"n\">modeling</span><span class=\"w\"> </span><span class=\"n\">file</span><span class=\"w\"> </span><span class=\"n\">requires</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">following</span><span class=\"w\"> </span><span class=\"n\">packages</span><span class=\"w\"> </span><span class=\"n\">that</span><span class=\"w\"> </span><span class=\"n\">were</span><span class=\"w\"> </span><span class=\"n\">not</span><span class=\"w\"> </span><span class=\"n\">found</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">your</span><span class=\"w\"> </span><span class=\"n\">environment</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">flash_attn</span><span class=\"bp\">.</span><span class=\"w\"> </span><span class=\"n\">Run</span><span class=\"w\"> </span><span class=\"ss\">`pip</span><span class=\"w\"> </span><span class=\"n\">install</span><span class=\"w\"> </span><span class=\"n\">flash_attn</span>\n</code></pre></div>\n<p>I looked into this and it says flash_attn requires CUDA to run so it is not available for Mac.<br>\nany work around here! even mlx_lm is not working</p>",
        "id": 463444252,
        "sender_full_name": "Kevin Ishimwe",
        "timestamp": 1724083215
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"707408\">Kevin Ishimwe</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/DeepSeek-Prover-V1.2E5/near/463444252\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"607118\">llllvvuu</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/DeepSeek-Prover-V1.2E5/near/462891997\">said</a>:</p>\n<blockquote>\n<p>This should work for running with huggingface/transformers:</p>\n<ol>\n<li>Look in the following directories for modeling_deepseek.py</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-Prover-V1.5-RL/snapshots\n~/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Prover-V1.5-RL\n</code></pre></div>\n<ol start=\"2\">\n<li>Replace with the modeling_deepseek.py from DeepSeek MoE V1: <a href=\"https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py\">https://huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py</a></li>\n<li>Try again:</li>\n</ol>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">AutoModelForCausalLM</span><span class=\"p\">,</span> <span class=\"n\">AutoTokenizer</span><span class=\"p\">,</span> <span class=\"n\">TextStreamer</span><span class=\"p\">,</span> <span class=\"n\">pipeline</span>\n\n<span class=\"n\">model_id</span> <span class=\"o\">=</span> <span class=\"s2\">\"deepseek-ai/DeepSeek-Prover-V1.5-RL\"</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AutoModelForCausalLM</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_id</span><span class=\"p\">,</span> <span class=\"n\">device_map</span><span class=\"o\">=</span><span class=\"s2\">\"auto\"</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">AutoTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">model_id</span><span class=\"p\">)</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">TextStreamer</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">)</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"text-generation\"</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">streamer</span><span class=\"o\">=</span><span class=\"n\">streamer</span>\n<span class=\"p\">)</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"import Mathlib</span>\n<span class=\"s2\">import Aesop</span>\n\n<span class=\"s2\">set_option maxHeartbeats 0</span>\n\n<span class=\"s2\">open BigOperators Real Nat Topology Rat</span>\n\n<span class=\"s2\">/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?</span>\n<span class=\"s2\">Show that it is $</span><span class=\"se\">\\\\</span><span class=\"s2\">frac{2</span><span class=\"se\">\\\\</span><span class=\"s2\">sqrt</span><span class=\"si\">{3}</span><span class=\"s2\">}</span><span class=\"si\">{3}</span><span class=\"s2\">$.-/</span>\n<span class=\"s2\">theorem amc12b_2003_p6 (a r : ℝ) (u : ℕ → ℝ) (h₀ : ∀ k, u k = a * r ^ k) (h₁ : u 1 = 2)</span>\n<span class=\"s2\">  (h₂ : u 3 = 6) : u 0 = 2 / Real.sqrt 3 ∨ u 0 = -(2 / Real.sqrt 3) := by\"\"\"</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">pipe</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">)</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">\"generated_text\"</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Currently working on integration into the main repo: <a href=\"https://github.com/huggingface/transformers/pull/32862\">https://github.com/huggingface/transformers/pull/32862</a><br>\nAs well as as to fix the HF custom code: <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/discussions/1\">https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/discussions/1</a></p>\n<p>It is extremely slow on MacBook since bitsandbytes quantization only supports CUDA. So, probably those with CUDA will get better performance with vLLM, while those with MacBook will get better performance with mlx_lm</p>\n</blockquote>\n<p>i tried using the hugging face transformer like in the example above, but I'm getting </p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"bp\">$</span><span class=\"w\"> </span><span class=\"n\">python3</span><span class=\"w\"> </span><span class=\"n\">run_model</span><span class=\"bp\">.</span><span class=\"n\">py</span>\n<span class=\"n\">The</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">for</span><span class=\"w\"> </span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"w\"> </span><span class=\"n\">contains</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code</span><span class=\"w\"> </span><span class=\"n\">which</span><span class=\"w\"> </span><span class=\"n\">must</span><span class=\"w\"> </span><span class=\"n\">be</span><span class=\"w\"> </span><span class=\"n\">executed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">correctly</span><span class=\"w\"> </span><span class=\"n\">load</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">model</span><span class=\"bp\">.</span><span class=\"w\"> </span><span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">inspect</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"w\"> </span><span class=\"k\">at</span><span class=\"w\"> </span><span class=\"n\">https</span><span class=\"o\">:</span><span class=\"bp\">//</span><span class=\"n\">hf</span><span class=\"bp\">.</span><span class=\"n\">co</span><span class=\"bp\">/</span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"bp\">.</span>\n<span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">avoid</span><span class=\"w\"> </span><span class=\"n\">this</span><span class=\"w\"> </span><span class=\"n\">prompt</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">future</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">passing</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">argument</span><span class=\"w\"> </span><span class=\"ss\">`trust_remote_code</span><span class=\"bp\">=</span><span class=\"n\">True</span><span class=\"bp\">`.</span>\n\n<span class=\"n\">Do</span><span class=\"w\"> </span><span class=\"n\">you</span><span class=\"w\"> </span><span class=\"n\">wish</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code?</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"n\">y</span><span class=\"bp\">/</span><span class=\"n\">N</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"n\">y</span>\n<span class=\"n\">The</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">for</span><span class=\"w\"> </span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"w\"> </span><span class=\"n\">contains</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code</span><span class=\"w\"> </span><span class=\"n\">which</span><span class=\"w\"> </span><span class=\"n\">must</span><span class=\"w\"> </span><span class=\"n\">be</span><span class=\"w\"> </span><span class=\"n\">executed</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">correctly</span><span class=\"w\"> </span><span class=\"n\">load</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">model</span><span class=\"bp\">.</span><span class=\"w\"> </span><span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">inspect</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">repository</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"w\"> </span><span class=\"k\">at</span><span class=\"w\"> </span><span class=\"n\">https</span><span class=\"o\">:</span><span class=\"bp\">//</span><span class=\"n\">hf</span><span class=\"bp\">.</span><span class=\"n\">co</span><span class=\"bp\">/</span><span class=\"n\">deepseek</span><span class=\"bp\">-</span><span class=\"n\">ai</span><span class=\"bp\">/</span><span class=\"n\">DeepSeek</span><span class=\"bp\">-</span><span class=\"n\">Prover</span><span class=\"bp\">-</span><span class=\"n\">V1</span><span class=\"bp\">.</span><span class=\"m\">5</span><span class=\"bp\">-</span><span class=\"n\">RL</span><span class=\"bp\">.</span>\n<span class=\"n\">You</span><span class=\"w\"> </span><span class=\"n\">can</span><span class=\"w\"> </span><span class=\"n\">avoid</span><span class=\"w\"> </span><span class=\"n\">this</span><span class=\"w\"> </span><span class=\"n\">prompt</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">future</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">passing</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">argument</span><span class=\"w\"> </span><span class=\"ss\">`trust_remote_code</span><span class=\"bp\">=</span><span class=\"n\">True</span><span class=\"bp\">`.</span>\n\n<span class=\"n\">Do</span><span class=\"w\"> </span><span class=\"n\">you</span><span class=\"w\"> </span><span class=\"n\">wish</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">run</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">custom</span><span class=\"w\"> </span><span class=\"n\">code?</span><span class=\"w\"> </span><span class=\"o\">[</span><span class=\"n\">y</span><span class=\"bp\">/</span><span class=\"n\">N</span><span class=\"o\">]</span><span class=\"w\"> </span><span class=\"n\">y</span>\n</code></pre></div>\n<p>this gives </p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">ImportError</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">This</span><span class=\"w\"> </span><span class=\"n\">modeling</span><span class=\"w\"> </span><span class=\"n\">file</span><span class=\"w\"> </span><span class=\"n\">requires</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">following</span><span class=\"w\"> </span><span class=\"n\">packages</span><span class=\"w\"> </span><span class=\"n\">that</span><span class=\"w\"> </span><span class=\"n\">were</span><span class=\"w\"> </span><span class=\"n\">not</span><span class=\"w\"> </span><span class=\"n\">found</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">your</span><span class=\"w\"> </span><span class=\"n\">environment</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">flash_attn</span><span class=\"bp\">.</span><span class=\"w\"> </span><span class=\"n\">Run</span><span class=\"w\"> </span><span class=\"ss\">`pip</span><span class=\"w\"> </span><span class=\"n\">install</span><span class=\"w\"> </span><span class=\"n\">flash_attn</span>\n</code></pre></div>\n<p>I looked into this and it says flash_attn requires CUDA to run so it is not available for Mac.<br>\nany work around here! even mlx_lm is not working</p>\n</blockquote>\n<p>I have an <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-Base/discussions/3/files\">improvement</a> to step 1: in both locations, delete configuration_deepseek.py and modeling_deepseek.py (after backing them up somewhere), and replace config.json with:</p>\n<div class=\"codehilite\" data-code-language=\"JSON\"><pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">\"architectures\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"s2\">\"LlamaForCausalLM\"</span>\n<span class=\"w\">    </span><span class=\"p\">],</span>\n<span class=\"w\">    </span><span class=\"nt\">\"attention_bias\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"attention_dropout\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.0</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"bos_token_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">100000</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"eos_token_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">100001</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"hidden_act\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"silu\"</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"hidden_size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">4096</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"initializer_range\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">0.02</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"intermediate_size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">11008</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"max_position_embeddings\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">4096</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"model_type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"llama\"</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"num_attention_heads\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">32</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"num_hidden_layers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">30</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"num_key_value_heads\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">32</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"pretraining_tp\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"quantization\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">\"group_size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">64</span><span class=\"p\">,</span>\n<span class=\"w\">        </span><span class=\"nt\">\"bits\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">8</span>\n<span class=\"w\">    </span><span class=\"p\">},</span>\n<span class=\"w\">    </span><span class=\"nt\">\"rms_norm_eps\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1e-06</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"rope_scaling\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">null</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"rope_theta\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">10000</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"tie_word_embeddings\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"torch_dtype\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bfloat16\"</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"transformers_version\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"4.33.1\"</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"use_cache\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">\"vocab_size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">102400</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<p>Then try step 2 again. The \"trust_remote_code\" message should be gone (if not, let me know). mlx_lm.convert should also work at this point.</p>",
        "id": 464067620,
        "sender_full_name": "llllvvuu",
        "timestamp": 1724249093
    },
    {
        "content": "<p>How do we have the model hosted as a Hugging Face Inference Endpoint?</p>",
        "id": 464150978,
        "sender_full_name": "Seth Ahrenbach",
        "timestamp": 1724269648
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 496472311,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1738127362
    }
]