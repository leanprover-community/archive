[
    {
        "content": "<p>Hi Zulip Community,</p>\n<p>I am looking forward to your input in what is the best way to programmatically interact with Lean 4 - I want to get compilation feedback and tactic state information to try out some ML stuff.</p>\n<p>I am currently using Lean REPL (<a href=\"https://github.com/leanprover-community/repl\">https://github.com/leanprover-community/repl</a>) and it takes roughly half a second to get the compilation feedback for a machine generated autoformalization of theorem statement (looks like theorem theorem_statement := by sorry). Is this the best speed one can get? </p>\n<p>Thanks!</p>",
        "id": 476545144,
        "sender_full_name": "Mert Ünsal",
        "timestamp": 1728759463
    },
    {
        "content": "<p>I feel the design space is so large here that it doesn't make sense to ask for the \"best way\", since each way comes with its own advantages and disadvantages.  Also, some ways of running lean just don't let you do stuff (like interact with the environment, or see what code is in the current file).  In short I know of four general paradigms for interacting with Lean:</p>\n<ol>\n<li>Running a Lean file from the command line.  Simplest, but probably most limited and slowest.</li>\n<li>Running from inside Lean's metaprogramming using a tactic or command.</li>\n<li>Using Lean's language server</li>\n<li>Using that Lean is written in Lean, so you can reuse parts of Lean's parser/kernel/interpreter.</li>\n</ol>",
        "id": 476547501,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728761779
    },
    {
        "content": "<p>I know since LLMs are often the bottleneck, one doesn't usually optimize the interaction (and you should ask yourself if this is slowing you down), but I think you could get things to go a lot faster.</p>",
        "id": 476547506,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728761789
    },
    {
        "content": "<p>Also, researchers aren't very open about what they do and the Lean organization hasn't put a lot of effort into standardizing this.  You could also try <a href=\"https://leandojo.readthedocs.io/en/latest/\">LeanDojo</a> or <a href=\"https://github.com/siddhartha-gadgil/LeanAide\">LeanAide</a>.  Or just try directly calling Lean from the command line or from the language server.  (Note the language server is hard to use and it has a hard-coded 100ms delay, at least it did in Lean 3.)</p>",
        "id": 476547511,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728761802
    },
    {
        "content": "<p>Also, you could probably get a good upper bound on the time for autoformalization by just generating 500, 1000, 2000 statements in the same file (duplicates might be okay) and timing how long it takes to run that file in Lean from the command line.</p>",
        "id": 476547520,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728761815
    },
    {
        "content": "<p>Let us know what eventually works for you.  I wish more people shared what they practically do here, and I wish more of this was upstreamed to common tools that other people use.</p>",
        "id": 476547525,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728761824
    },
    {
        "content": "<p>I have experimented a bit with LeanDojo but it seems to be extremely slow in initializing a theorem for me - somehow it needs to trace everything and really doesn't work as fast as the standard Infoview environment. </p>\n<p>I think this is a very important aspect to enable machine-assistance to Lean 4 - the model interacting a little bit with the environment, at the very least, can assure that the recommendations of the model doesn't result in compilation errors. I guess it could be very valuable if Lean FRO worked on a standardized interaction with an 'environment'.</p>",
        "id": 476548204,
        "sender_full_name": "Mert Ünsal",
        "timestamp": 1728762497
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/476547520\">said</a>:</p>\n<blockquote>\n<p>Also, you could probably get a good upper bound on the time for autoformalization by just generating 500, 1000, 2000 statements in the same file (duplicates might be okay) and timing how long it takes to run that file in Lean from the command line.</p>\n</blockquote>\n<p>Thanks - I will try this. I am wondering how they did in AlphaProof but I am assuming they just had parallelized across many CPUs with Google compute.</p>",
        "id": 476548360,
        "sender_full_name": "Mert Ünsal",
        "timestamp": 1728762668
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/476547506\">said</a>:</p>\n<blockquote>\n<p>I know since LLMs are often the bottleneck, one doesn't usually optimize the interaction (and you should ask yourself if this is slowing you down), but I think you could get things to go a lot faster.</p>\n</blockquote>\n<p>When I am running a 7B model I can get 2000 tokens/sec so to generate a next Lean proof step can take 10ms - so in my case I believe it's actually the compiler feedback that is the bottleneck.</p>",
        "id": 476548441,
        "sender_full_name": "Mert Ünsal",
        "timestamp": 1728762768
    },
    {
        "content": "<p>As for AlphaProof, they had enough Lean experts on that team that I wouldn't be surprised if they forked Lean. If not, they certainly had the metaprogramming knowledge to do whatever they needed (and enough compute, that running Lean in parallel wasn't likely a huge bottleneck).  (Meta similarly had <span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span> on their team and my understanding is that for the HyperTree Proof Search paper, he wrote a custom Lean parser for running tactics.  I could be mistaken on the details.)</p>",
        "id": 476548601,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728762925
    },
    {
        "content": "<p>There is also <a href=\"https://github.com/openai/lean-gym\">LeanGym</a> but it only works in Lean 3 (from what I understand).</p>",
        "id": 476548721,
        "sender_full_name": "Mert Ünsal",
        "timestamp": 1728763069
    },
    {
        "content": "<p>I generally recommend that machine learning researchers partner with Lean metaprogramming experts, but this is moving so fast, that advice might be more and more unrealistic.</p>",
        "id": 476548783,
        "sender_full_name": "Jason Rute",
        "timestamp": 1728763114
    },
    {
        "content": "<p>If there's anyone willing to help with this, I am more than happy to collaborate</p>",
        "id": 476549038,
        "sender_full_name": "Mert Ünsal",
        "timestamp": 1728763357
    },
    {
        "content": "<p>8 messages were moved from this topic to <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ABEL\">#Machine Learning for Theorem Proving &gt; ABEL</a> by <span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span>.</p>",
        "id": 477191091,
        "sender_full_name": "Notification Bot",
        "timestamp": 1729078091
    },
    {
        "content": "<p>2 messages were moved from this topic to <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Pantograph\">#Machine Learning for Theorem Proving &gt; Pantograph</a> by <span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span>.</p>",
        "id": 485528933,
        "sender_full_name": "Notification Bot",
        "timestamp": 1733101349
    },
    {
        "content": "<p>I made a separate topic <span aria-label=\"up\" class=\"emoji emoji-2b06\" role=\"img\" title=\"up\">:up:</span> for <a href=\"https://arxiv.org/abs/2410.16429\">Pantograph</a> but the one sentence summary is that it seems to be another way to communicate with Lean (it seems through the language server) for research (and maybe practical tools?).</p>",
        "id": 485529035,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733101451
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/485529035\">said</a>:</p>\n<blockquote>\n<p>I made a separate topic <span aria-label=\"up\" class=\"emoji emoji-2b06\" role=\"img\" title=\"up\">:up:</span> for <a href=\"https://arxiv.org/abs/2410.16429\">Pantograph</a> but the one sentence summary is that it seems to be another way to communicate with Lean (it seems through the language server) for research (and maybe practical tools?).</p>\n</blockquote>\n<p>Pantograph doesn't invoke Lean through the LSP. It bypasses it.</p>",
        "id": 485922032,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733250225
    },
    {
        "content": "<p>So in my list above is the interaction model closest to option 2?</p>\n<blockquote>\n<p>Running from inside Lean's metaprogramming using a tactic or command.</p>\n</blockquote>\n<p>In other words, your AI won't be able to see the rest of the file or library (except the symbolic representations stored the Lean environment)?  But maybe I'm mistaken and it is more closer to option 4:</p>\n<blockquote>\n<p>Using that Lean is written in Lean, so you can reuse parts of Lean's parser/kernel/interpreter.</p>\n</blockquote>\n<p>I see you are doing something with Lean's parser.  I'm not sure if that is just for parsing an LLM written proof of the current theorem, or also parsing other parts of the current Lean file and project.</p>",
        "id": 485980520,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733271054
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/485980520\">said</a>:</p>\n<blockquote>\n<p>So in my list above is the interaction model closest to option 2?</p>\n<blockquote>\n<p>Running from inside Lean's metaprogramming using a tactic or command.</p>\n</blockquote>\n<p>In other words, your AI won't be able to see the rest of the file or library (except the symbolic representations stored the Lean environment)?  But maybe I'm mistaken and it is more closer to option 4:</p>\n<blockquote>\n<p>Using that Lean is written in Lean, so you can reuse parts of Lean's parser/kernel/interpreter.</p>\n</blockquote>\n<p>I see you are doing something with Lean's parser.  I'm not sure if that is just for parsing an LLM written proof of the current theorem, or also parsing other parts of the current Lean file and project.</p>\n</blockquote>\n<p>Its closer to option 2 with some features implemented in option 4</p>",
        "id": 485995961,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733278664
    },
    {
        "content": "<blockquote>\n<p>Note the language server is hard to use and it has a hard-coded 100ms delay, at least it did in Lean 3.</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I'm wondering if that's still the case for Lean 4.</p>",
        "id": 490895283,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1735231136
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> I assume it is. The two best ways to check this are either (1) to ask someone who made it, or (2) test it by repeatedly calling the server.  The later could be done by borrowing some code from <a href=\"https://github.com/leanprover-community/lean-client-python\">https://github.com/leanprover-community/lean-client-python</a>.</p>",
        "id": 490904483,
        "sender_full_name": "Jason Rute",
        "timestamp": 1735237911
    },
    {
        "content": "<p>That shouldn't be the case but I don't see any reason to use it for ML purposes over specialized interfaces like Pantograph</p>",
        "id": 490904624,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1735238003
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span> How easy would it to make a tool (like a plugin) which can see both the current goal state so it can suggest tactics or whole proofs, but also the rest of the file (in text) or project so that it can suggest proofs based on other tactics proofs in the file or project (not the term proofs, but the tactic proofs)?</p>",
        "id": 490905096,
        "sender_full_name": "Jason Rute",
        "timestamp": 1735238354
    },
    {
        "content": "<p>Tactics can’t see the tactic proofs, even the proof you are currently in.</p>",
        "id": 490905200,
        "sender_full_name": "Jason Rute",
        "timestamp": 1735238434
    },
    {
        "content": "<p>General plugins like GitHub Copilot, Continue, Cursor can’t see proof states.</p>",
        "id": 490905352,
        "sender_full_name": "Jason Rute",
        "timestamp": 1735238559
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/490905352\">said</a>:</p>\n<blockquote>\n<p>General plugins like GitHub Copilot, Continue, Cursor can’t see proof states.</p>\n</blockquote>\n<p>Can these systems not see error messages?</p>",
        "id": 490906055,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1735239103
    },
    {
        "content": "<p>Maybe, I’d have to double check, but where is the error message coming from?  The language server?</p>",
        "id": 490906284,
        "sender_full_name": "Jason Rute",
        "timestamp": 1735239249
    },
    {
        "content": "<p>I guess Cursor (which I haven’t used yet) has a “fix with ai” feature for fixing errors.  It isn’t the use case I envisioned. (I envisioned something more like Tactician for Coq or what I think <span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span> envisioned with minicxt.) But if that feature already exists in current tools, how does that apply to Lean?  What data does it need to be trained on?  Does it already work for Lean in Cursor + Sonnet 3.5?</p>",
        "id": 490907437,
        "sender_full_name": "Jason Rute",
        "timestamp": 1735240043
    },
    {
        "content": "<p>I see, for an editor plugin, interaction with the language server makes more sense. There shouldn't be any artificial delays as I said but if any existing request is insufficient, installing a new RPC handler on the server side could be a valid option</p>",
        "id": 490908648,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1735240900
    },
    {
        "content": "<p>I've been working on a thin Python wrapper around the language server.<br>\nSo I can provide some benchmarks on the interaction:</p>\n<p>First a file needs to be opened in the language server.<br>\nLoading random .lean files from /mathlib/Mathlib/:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.18</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">72.28</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">2817.45</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.20</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">57.63</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">2239.09</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.13</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">39.00</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">1660.48</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.21</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">56.99</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">2341.68</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n</code></pre></div>\n<p>Thankfully, the lsp allows multiple files to be opened in parallel:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">1.34</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">219.91</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">8685.79</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">1.27</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">183.02</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">7649.99</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.32</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">95.10</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">3629.08</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.23</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">72.02</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">3173.30</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">1.07</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">308.99</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">13099.81</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">Loaded</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">0.63</span><span class=\"w\"> </span><span class=\"n\">files</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">197.90</span><span class=\"w\"> </span><span class=\"n\">lines</span><span class=\"bp\">/</span><span class=\"n\">s</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mf\">7633.16</span><span class=\"w\"> </span><span class=\"n\">chars</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n</code></pre></div>\n<p>The imports seem to influence the time it takes to load a file.<br>\nArtificial delay on the file opening in the lsp seems unlikely?</p>\n<p>Querying an opened file is quite fast:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">Running</span><span class=\"w\"> </span><span class=\"mi\">50</span><span class=\"w\"> </span><span class=\"n\">repeats</span><span class=\"w\"> </span><span class=\"n\">of</span><span class=\"w\"> </span><span class=\"n\">each</span><span class=\"w\"> </span><span class=\"n\">request</span><span class=\"o\">:</span>\n<span class=\"n\">plain_goal</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">179.82</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">plain_term_goal</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">159.10</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">completion</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">18.67</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">definition</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">334.63</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">hover</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">881.99</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">declaration</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">250.87</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">type_definition</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">1071.57</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">document_highlight</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">26.48</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">document_symbol</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">261.13</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">semantic_tokens_full</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">42.73</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">semantic_tokens_range</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">55.14</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n<span class=\"n\">folding_range</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"mf\">551.44</span><span class=\"w\"> </span><span class=\"n\">requests</span><span class=\"bp\">/</span><span class=\"n\">s</span>\n</code></pre></div>\n<p>Again, individual request times vary a lot, depending on the file and position.</p>\n<p>The wrapper is not ready for use yet but the benchmarking code is here:<br>\n<a href=\"https://github.com/oOo0oOo/leanclient/blob/main/tests/test_client_benchmark.py\">https://github.com/oOo0oOo/leanclient/blob/main/tests/test_client_benchmark.py</a><br>\nEDIT: Fixed url</p>\n<p>I use the lsp for parallelizable and query-heavy tasks like data extraction.<br>\nIt worked very well so far!</p>\n<p>To use the lsp as a REPL (like Pantograph or LeanDojo) would require some speedup on the file opening:</p>\n<ul>\n<li>Use <code>textDocument/didChange</code> for incremental updates.</li>\n<li>Some interactions might be possible before <code>waitForDiagnostics</code> returns?</li>\n</ul>",
        "id": 491624356,
        "sender_full_name": "Oliver Dressler",
        "timestamp": 1735830959
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/476547501\">said</a>:</p>\n<blockquote>\n<p>I feel the design space is so large here that it doesn't make sense to ask for the \"best way\", since each way comes with its own advantages and disadvantages.  Also, some ways of running lean just don't let you do stuff (like interact with the environment, or see what code is in the current file).  In short I know of four general paradigms for interacting with Lean:</p>\n<ol>\n<li>Running a Lean file from the command line.  Simplest, but probably most limited and slowest.</li>\n<li>Running from inside Lean's metaprogramming using a tactic or command.</li>\n<li>Using Lean's language server</li>\n<li>Using that Lean is written in Lean, so you can reuse parts of Lean's parser/kernel/interpreter.</li>\n</ol>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> <br>\nThe \"four general paradigms for interacting with Lean\" you mentioned are very interesting, but they are quite abstract. Where can I find some examples or cases about these four paradigms? </p>\n<p>Additionally, to which paradigm does the method of \"writing code in VSCode and seeing information in the InfoView\" belong? <span aria-label=\"melting face\" class=\"emoji emoji-1fae0\" role=\"img\" title=\"melting face\">:melting_face:</span></p>",
        "id": 493535942,
        "sender_full_name": "John Smith",
        "timestamp": 1736844086
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"819526\">dialectics</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Interacting.20with.20Lean.204/near/493535942\">said</a>:</p>\n<blockquote>\n<p>Additionally, to which paradigm does the method of \"writing code in VSCode and seeing information in the InfoView\" belong? <span aria-label=\"melting face\" class=\"emoji emoji-1fae0\" role=\"img\" title=\"melting face\">:melting_face:</span></p>\n</blockquote>\n<p>To be clear, I was talking about <em>machine</em>-interaction with Lean, specifically in AI applications.  But nonetheless, the Lean VSCode plugin works through the language server (approach 3).</p>",
        "id": 493574310,
        "sender_full_name": "Jason Rute",
        "timestamp": 1736857651
    },
    {
        "content": "<p>As for examples:</p>\n<ul>\n<li>I think a lot of AI papers using very large LLMs run lean on a file (approach 1) for running their benchmarks.  For example I thought the CORPRA evals ran this way.  If the LLM generation time is much more than the compile time, it isn’t an issue for benchmarks.</li>\n<li>All the tactic-based tools like Lean Copilot, LLMLean, and Aesop use metaprogramming (approach 2) to write their tactics, but the ones connected to an LLM at least use the parser as well (which is sort of approach 4).</li>\n<li>Maybe approaches 2 and 4 are blurry and could be combined.  It is all a form of metaprogramming.  I was just trying express 4 as something more drastic where you say make your own language server or forked Lean version.  I don’t know if anyone does this (maybe DeepMind?).</li>\n<li>There are a number of applications which you can run with <code>lean exe _</code> like the Lean REPL, Improver, and various proof recording libraries.  I dont know the details.  They are clearly metaprogramming so in the 2 or 4 category, but again maybe those categories should be fleshed out more.   I think to build such apps you likely need to know Lean metaprogramming as in the metaprogramming book, but not the inner workings of Lean.</li>\n<li>Last, the editor plugins communicate via LSP (this includes Lean’s widgets).  Also <span class=\"user-mention\" data-user-id=\"802311\">@Oliver Dressler</span> just released a Python library wrapping the LSP which might be useful for those not wanting to learn Lean metaprogramming.</li>\n</ul>",
        "id": 493579947,
        "sender_full_name": "Jason Rute",
        "timestamp": 1736859479
    },
    {
        "content": "<p>A simple example using approach 1 :)</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">subprocess</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">sp</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">tempfile</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">State</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">goals</span><span class=\"p\">:</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">code</span><span class=\"p\">:</span><span class=\"nb\">str</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">goals</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">code</span> <span class=\"o\">=</span> <span class=\"n\">goals</span><span class=\"p\">,</span> <span class=\"n\">code</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__repr__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">goals</span> <span class=\"ow\">or</span> <span class=\"s2\">\"No goals\"</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_state</span><span class=\"p\">(</span><span class=\"n\">code</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">tempfile</span><span class=\"o\">.</span><span class=\"n\">NamedTemporaryFile</span><span class=\"p\">(</span><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">'w'</span><span class=\"p\">,</span> <span class=\"n\">suffix</span><span class=\"o\">=</span><span class=\"s1\">'.lean'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">temp</span><span class=\"p\">:</span>\n        <span class=\"n\">temp</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">code</span><span class=\"p\">)</span>\n        <span class=\"n\">temp</span><span class=\"o\">.</span><span class=\"n\">flush</span><span class=\"p\">()</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">sp</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s2\">\"lake\"</span><span class=\"p\">,</span> <span class=\"s2\">\"env\"</span><span class=\"p\">,</span> <span class=\"s2\">\"lean\"</span><span class=\"p\">,</span> <span class=\"s2\">\"--json\"</span><span class=\"p\">,</span> <span class=\"n\">temp</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">]</span> <span class=\"p\">,</span> <span class=\"n\">capture_output</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">states</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">stdout</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">'</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">):</span>\n            <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">():</span> <span class=\"k\">continue</span>\n            <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n            <span class=\"n\">goals</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'data'</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">goals</span><span class=\"o\">.</span><span class=\"n\">startswith</span><span class=\"p\">(</span><span class=\"s1\">'unsolved goals'</span><span class=\"p\">):</span>\n                <span class=\"n\">states</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">State</span><span class=\"p\">(</span><span class=\"n\">goals</span><span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">:],</span> <span class=\"n\">code</span><span class=\"p\">))</span>\n        <span class=\"k\">return</span> <span class=\"n\">states</span> <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">states</span><span class=\"p\">)</span> <span class=\"k\">else</span> <span class=\"p\">[</span><span class=\"n\">State</span><span class=\"p\">(</span><span class=\"s2\">\"\"</span><span class=\"p\">,</span> <span class=\"n\">code</span><span class=\"p\">)]</span>\n\n<span class=\"n\">thm</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"theorem demo (P Q R : Prop) : (P → Q) → (Q → R) → (P → R) := by\"\"\"</span>\n<span class=\"n\">tactics</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'intro h1 h2 p'</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"sd\">'''have q : Q := by</span>\n<span class=\"sd\">      apply h1</span>\n<span class=\"sd\">      exact p'''</span><span class=\"p\">,</span>\n    <span class=\"s1\">'apply h2'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'exact q'</span>\n<span class=\"p\">]</span>\n<span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">get_state</span><span class=\"p\">(</span><span class=\"n\">thm</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n<span class=\"n\">states</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">tac</span> <span class=\"ow\">in</span> <span class=\"n\">tactics</span><span class=\"p\">:</span>\n    <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">get_state</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">code</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"n\">tac</span><span class=\"si\">}</span><span class=\"s2\">\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s1\">'</span><span class=\"se\">\\n\\n</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n    <span class=\"n\">states</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n</code></pre></div>\n<p><a href=\"/user_uploads/3121/QD5pDKQ3lsuALYKBuz8t6MdO/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/QD5pDKQ3lsuALYKBuz8t6MdO/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"414x716\" src=\"/user_uploads/thumbnail/3121/QD5pDKQ3lsuALYKBuz8t6MdO/image.png/840x560.webp\"></a></div>",
        "id": 503049113,
        "sender_full_name": "RexWang",
        "timestamp": 1741014242
    },
    {
        "content": "<p>Hello everyone!<br>\nI'm excited to share <a href=\"https://github.com/augustepoiroux/LeanInteract\">LeanInteract</a>, a Python package built on top of the Lean REPL. (Yay, yet another tool to interact with Lean 4 ^^)<br>\nOverall, it aims at being simple to set up and use for both new and experienced users. I developed it while working on autoformalization research, particularly needing version compatibility and efficient type-checking through state management.</p>\n<p>Some of the features I find most useful:</p>\n<ul>\n<li>Compatibility with <strong><em>all</em></strong> Lean versions between v4.7.0-rc1 and v4.19.0-rc2, with all the latest features and bug fixes of Lean REPL made available for all Lean versions.</li>\n<li>Simple interaction with Lean projects: start from existing projects, or instantiate temporary ones with custom dependencies. I find the latter option especially helpful for AI benchmarks stored in datasets (JSON, ...).</li>\n<li>Based on Lean REPL, so you can interleave executing Lean commands with tactics-based proof search.</li>\n</ul>\n<p>Let me know what you think :)</p>",
        "id": 511636349,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1744373725
    },
    {
        "content": "<p>Bonus: I personally like the recent addition of the <code>rootGoals</code> option in the Lean REPL (thus available in LeanInteract). It extracts initial goals from declarations in a Lean file, allowing you to work with theorems in existing projects without tracing the whole project.</p>",
        "id": 511636401,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1744373739
    },
    {
        "content": "<p>Do I read correctly that you have backported the <code>rootGoals</code> feature to all 12 intermediate lean releases?</p>",
        "id": 511636767,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1744373832
    },
    {
        "content": "<p>Yes indeed, it is tested <a href=\"https://github.com/augustepoiroux/repl/blob/v1.0.6/test/root_goals.in\">here</a> and <a href=\"https://github.com/augustepoiroux/repl/blob/v1.0.6/test/self_proof_check.in\">here</a> for Lean v4.7.0-rc1 for example. Although there are only two tests for it, so please tell me if you find a bug ;)<br>\nThe backport is also done for the intermediate rc versions</p>",
        "id": 511638043,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1744374174
    },
    {
        "content": "<p>Most importantly I think, the proof validation in tactic mode that has been added today to the REPL has also been backported</p>",
        "id": 511638410,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1744374264
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"321854\">@Auguste Poiroux</span> I'm a bit late on this, but it looks cool! I've been working a similar tool supporting some autoformalization research called <a href=\"http://github.com/jsimonrichard/lean-sdk/\">lean-sdk</a>... it's very similar, but I wanted the flexibility of a REPL fork rather than the original project, and I wanted to use LSP-style JSON RPC instead of REPL's current format to improve reliability... but maybe you've solved that problem without that change.</p>\n<p>My project is pre-pre-alpha, so I don't expect it to really be useful to anyone (except other developers) yet, but I wonder if we should have a chat to figure out if we should combine forces or keep our projects separate. Let me know what you think</p>",
        "id": 511936062,
        "sender_full_name": "J. Simon Richard",
        "timestamp": 1744573080
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"876642\">@J. Simon Richard</span>! I am happy to hear that you like my project ^^<br>\nWith LeanInteract, my initial goal was to make it flexible. There are several options making it possible to switch the repl backend: in LeanREPLConfig you have <a href=\"https://github.com/augustepoiroux/LeanInteract/blob/c7e9052c0c8d6d46971a0d09213e6723f87c461a/src/lean_interact/config.py#L198-L199\"><code>repl_git</code> and <code>repl_rev</code></a> parameters, and in LeanServer, there is a <a href=\"https://github.com/augustepoiroux/LeanInteract/blob/c7e9052c0c8d6d46971a0d09213e6723f87c461a/src/lean_interact/server.py#L112-L141\"><code>run_dict</code></a> method which communicate with the repl in raw JSON format. However, I must recognize that it is not very flexible anymore, especially because of the constraints regarding Lean versioning LeanInteract now imposes on the repl backend.<br>\nBut I would be very happy to make some changes to make this less strict. This can be particularly helpful for people developing new features for the Lean REPL like you. I also welcome all kinds of contributions to LeanInteract, so feel free to make a PR ;)</p>",
        "id": 512098203,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1744641793
    },
    {
        "content": "<p>I was reading the Kimina report (great work, congrats to everyone who worked on this project), and found their description of the Numina Lean Server explaining why it is efficient: using Lean REPL, reusing preloaded environments based on import headers, and parallelization across CPUs.<br>\nThere is an example <a href=\"https://github.com/augustepoiroux/LeanInteract/blob/main/examples/proof_generation_and_autoformalization.py#L98-L171\">script</a> doing exactly that in LeanInteract.<br>\nOne difference with the described Numina server is the use of LRU-based caching mechanism of the headers. Our example script is not tailored to be used during training, but for evaluation, hence no need for LRU-based caching (I believe, please tell me if I misunderstood the role of the LRU-based caching).<br>\nIf you want to <em>efficiently</em> evaluate the <a href=\"https://huggingface.co/AI-MO/Kimina-Prover-Preview-Distill-7B\">Kimina</a> models, the example script should work almost out-of-the-box, minus adding the model config and updating the prompting <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 512146458,
        "sender_full_name": "Auguste Poiroux",
        "timestamp": 1744654895
    }
]