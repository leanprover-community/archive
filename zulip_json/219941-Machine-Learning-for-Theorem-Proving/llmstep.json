[
    {
        "content": "<p>Hi all, check out <code>llmstep</code>: a tactic for suggesting proof steps using a language model: </p>\n<ul>\n<li><a href=\"https://github.com/wellecks/llmstep\">https://github.com/wellecks/llmstep</a></li>\n</ul>\n<p>It uses a model running on your own device, and the suggestions usually take around a second or less with a GPU. </p>\n<p>By default, <code>llmstep</code> uses an open-source language model fine-tuned on LeanDojo’s training set. <span class=\"user-mention\" data-user-id=\"565199\">@Rahul Saha</span>  and I are working on improving it, please let us know if you have any feedback!</p>\n<p><a href=\"/user_uploads/3121/IJ4e0mOFRqiOEYW4KfG2yfJX/llmstep.gif\">llmstep.gif</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/IJ4e0mOFRqiOEYW4KfG2yfJX/llmstep.gif\" title=\"llmstep.gif\"><img src=\"/user_uploads/3121/IJ4e0mOFRqiOEYW4KfG2yfJX/llmstep.gif\"></a></div>",
        "id": 385775343,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1692318685
    },
    {
        "content": "<p>Cool! Excited to try it out.</p>",
        "id": 385784010,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692320846
    },
    {
        "content": "<p><del>Is there a mode which doesn't require the tactic type suggestion?</del></p>",
        "id": 385784064,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692320866
    },
    {
        "content": "<p>Could you write a wrapper that explores the proof tree itself?</p>",
        "id": 385784182,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692320896
    },
    {
        "content": "<p>If you can give me a \"no hands\" version, I would love to <a href=\"#narrow/stream/287929-mathlib4/topic/.22library.20scale.22.20testing.20of.20tactics/near/385585562\">run it against the whole library</a>.</p>",
        "id": 385784419,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692320978
    },
    {
        "content": "<p>It seems like lines 24-141 of <a href=\"https://github.com/wellecks/llmstep/blob/master/LLMstep/LLMstep.lean\"><code>LLMStep.lean</code></a> could be immediately upstreamed to Std! (Someone was independently asking for this functionality recently, maybe Yury?)</p>",
        "id": 385787165,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692321864
    },
    {
        "content": "<p>It would be great to have the <code>elab_rules : tactic  | `(tactic | llmstep%$tac $pfx:str) =&gt; ...</code> declaration that actually does the work be separated out into a <code>MetaM</code> function (that pretty prints the goal, calls the model, and returns the parsed <code>Syntax</code> suggestions) and an <code>elab_rules</code> that pipes that to <code>addSuggestions</code>.</p>\n<p>This would make it relatively easy to add a full self-driving mode. :-)</p>",
        "id": 385787846,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692322086
    },
    {
        "content": "<p>Are PRs welcome? :-)</p>",
        "id": 385788279,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692322235
    },
    {
        "content": "<p>Bug report: <code>llmstep</code> creates a widget, but does not put a blue squiggle under the <code>llmstep</code> when it finishes. I think this is essential so you can see that something happened! (See how all the other suggestions functions work.)</p>",
        "id": 385791302,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692323261
    },
    {
        "content": "<p>Unfortunately on my (soon to be replaced) old machine, it takes 30-90s to get suggestions.</p>",
        "id": 385792015,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692323520
    },
    {
        "content": "<p>That aside, it works. <span aria-label=\"octopus\" class=\"emoji emoji-1f419\" role=\"img\" title=\"octopus\">:octopus:</span>.</p>",
        "id": 385792108,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692323551
    },
    {
        "content": "<ul>\n<li><a href=\"https://github.com/wellecks/llmstep/pull/2\">https://github.com/wellecks/llmstep/pull/2</a> factors out a MetaM tactic</li>\n<li><a href=\"https://github.com/wellecks/llmstep/pull/3\">https://github.com/wellecks/llmstep/pull/3</a> bumps Mathlib and thereby reduces imports (tactic only depends on Std now)</li>\n<li><a href=\"https://github.com/wellecks/llmstep/pull/4\">https://github.com/wellecks/llmstep/pull/4</a> refactors <code>addSuggestions</code>, making it easier to upstream</li>\n</ul>",
        "id": 385801844,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692326958
    },
    {
        "content": "<p>Awesome, thanks!! PR's are definitely welcome, I'll take a look!</p>",
        "id": 385805742,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1692328532
    },
    {
        "content": "<p>This is a really exciting release. I've had a lot of fun playing with it.</p>",
        "id": 385904545,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692362649
    },
    {
        "content": "<p>I created a <a href=\"https://github.com/wellecks/llmstep/pull/5\">PR</a> that adds support for a llama.cpp inference server. This means that, in principle, anyone with a decent laptop should be able to run tactic prediction locally without gpus.</p>",
        "id": 385904991,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692362821
    },
    {
        "content": "<p>Unfortunately, the default llmstep model has an architecture that isn't compatible with llama.cpp. I quickly finetuned a <a href=\"https://huggingface.co/zhangirazerbayev/open-llama-3b_next-tactic_dev0.2\">next tactic prediction model</a> that is compatible, but it's very undertrained (&lt;1 gpu hour) and should only really be used as a proof of concept that the latency is ok.</p>",
        "id": 385905747,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692363088
    },
    {
        "content": "<p>Note Lean 3 has (or had at least) a built in interface to give machine learning suggestions automatically in the info view.   <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/api.20suggestion.20vscode.20integration\">#general &gt; api suggestion vscode integration</a>   I have a number of questions, partly based on my experience with that</p>\n<ul>\n<li>can someone easily rig up a similar thing for Lean 3?</li>\n<li>can this tactic (or better an automatic suggestion giver) filter out tactics which fail like lean gptf did?</li>\n</ul>",
        "id": 385915378,
        "sender_full_name": "Jason Rute",
        "timestamp": 1692365784
    },
    {
        "content": "<p>Also why this model over Reprover?  Is it just that there is no Reprover for Lean 4?</p>",
        "id": 385915508,
        "sender_full_name": "Jason Rute",
        "timestamp": 1692365814
    },
    {
        "content": "<p>This is awesome! We are also working on something that can be used to build tools for tactic suggestions  (hopefully will be able to share more later). </p>\n<blockquote>\n<p>Also why this model over Reprover? Is it just that there is no Reprover for Lean 4?</p>\n</blockquote>\n<p>ReProver (w/ retrieval) requires access to a corpus of all premises in the environment and their pre-computed embeddings, which LLMStepp does not currently provide. </p>\n<p>It should be easy to use <a href=\"https://huggingface.co/kaiyuy/leandojo-lean4-tacgen-byt5-small\">ReProver (w/o retrieval)</a>, though the input format has some minor differences. Note that Pythia is ~10x larger than ReProver (w/o retrieval). I'm curious about whether it performs much better. We're trying to replace the ByT5 backbone in ReProver with larger LLMs such as StarCoder but no luck yet. Maybe we should try Pythia?</p>",
        "id": 385934437,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1692370298
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep/near/385934437\">said</a>:</p>\n<blockquote>\n<p>Maybe we should try Pythia?</p>\n</blockquote>\n<p>Pythia was designed with interpretability research rather than performance in mind. By today's standards, the larger Pythia models are very undertrained and you should expect much stronger theorem proving ability from something like starcoder.</p>",
        "id": 385938340,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692371253
    },
    {
        "content": "<p>Will the math-lm project produce models potentially useful for this purpose anytime soon?</p>",
        "id": 385939309,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1692371553
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep/near/385939309\">said</a>:</p>\n<blockquote>\n<p>Will the math-lm project produce models potentially useful for this purpose anytime soon?</p>\n</blockquote>\n<p>Yes. The current timeline is early September.</p>",
        "id": 385939449,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692371597
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep/near/385915378\">said</a>:</p>\n<blockquote>\n<ul>\n<li>can this tactic (or better an automatic suggestion giver) filter out tactics which fail like lean gptf did?</li>\n</ul>\n</blockquote>\n<p>Yes it checks if the suggestion succeeds (and if so, highlights it in blue), and additionally if it completes the proof (and highlights it in green) </p>\n<p>Regarding the model, we finetuned a standard decoder-only model with the idea that we can always drop in a better one (e.g. see Zhangir's comments). As Kaiyu mentioned, it shouldn't be too hard to use the non-retrieval version of ReProver too. Adding support for a retrieval-augmented model is a great idea!</p>",
        "id": 385960316,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1692380053
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep/near/385784419\">said</a>:</p>\n<blockquote>\n<p>If you can give me a \"no hands\" version, I would love to <a href=\"#narrow/stream/287929-mathlib4/topic/.22library.20scale.22.20testing.20of.20tactics/near/385585562\">run it against the whole library</a>.</p>\n</blockquote>\n<p>Just so it is clear, while there would be some value to these numbers, it would be a <strong>vast</strong> overestimate of the capability of this model.  In particular, this model was trained on theorems from the library, and likely has memorized most of the proofs, especially the short proofs.  A better evaluation would be on theorems not used for training (assuming <span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span> / <span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> left out some testing theorems from the training).  Also, even in this case there are two realistic concerns.  (1) if the test theorems are randomly mixed in the library, it would be very different from much of mathlib development where one is working on a new file with new definitions and lemmas., so proving 50% of test theorems might not translate to anywhere near 50% of real world theorems.  This is worse for models like this which don't take the environment into account.  (2) It is still not clear how much of mathlib already occurs in the pre-training data and  contaminates any meaningful evaluation on test theorems.  I know some people who think this is a minimal concern and some who feel it is a huge problem in this area.</p>",
        "id": 385998170,
        "sender_full_name": "Jason Rute",
        "timestamp": 1692398635
    },
    {
        "content": "<p>Here's an idea for testing: benchmark against every theorem in Mathlib <em>after</em> running (non-terminal) aesop on the initial goal. This</p>\n<ul>\n<li>removes all of the \"it's easy anyway\" goals out of the benchmark</li>\n<li>hopefully mostly preserves the provability of the goals!</li>\n<li>may shuffle things up just enough to prevent memorization<br>\n(How true the 3rd point is is unclear!)</li>\n</ul>",
        "id": 386018957,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692407554
    },
    {
        "content": "<p>I want to say something about my llmstep PR <a href=\"https://github.com/wellecks/llmstep/pull/2\">https://github.com/wellecks/llmstep/pull/2</a> above, which refactors the code (it's a tiny change) so rather than just providing a syntactical tactic, it provides a <code>MetaM</code> level tactic that can be run on any goal.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"sd\">/--</span>\n<span class=\"sd\">Call the LLM on a goal, asking for suggestions beginning with a prefix.</span>\n<span class=\"sd\">-/</span>\n<span class=\"kd\">def</span> <span class=\"n\">llmStep</span> <span class=\"o\">(</span><span class=\"n\">pre</span> <span class=\"o\">:</span> <span class=\"n\">String</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">g</span> <span class=\"o\">:</span> <span class=\"n\">MVarId</span><span class=\"o\">)</span> <span class=\"o\">:</span> <span class=\"n\">MetaM</span> <span class=\"o\">(</span><span class=\"n\">List</span> <span class=\"n\">String</span><span class=\"o\">)</span> <span class=\"o\">:=</span> <span class=\"k\">do</span>\n   <span class=\"bp\">...</span>\n</code></pre></div>\n<p>I hope that everyone releasing Lean tools can make sure to do this as well --- it allows us to easily share benchmarking tools, use common frontends for displaying suggestions, layer best-first search on top in a generic way,  etc.</p>\n<p>Perhaps it's even worth agreeing what an API should look like!</p>\n<p>The obvious thing missing from the signature above is that probably we should return a <code>List (String \\times Float)</code>, to allow the LLM to say something about its confidence in each answer, to help rank answers and steer search.</p>",
        "id": 386020001,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692407958
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep/near/386018957\">said</a>:</p>\n<blockquote>\n<p>Here's an idea for testing: benchmark against every theorem in Mathlib <em>after</em> running (non-terminal) aesop on the initial goal. This</p>\n<ul>\n<li>removes all of the \"it's easy anyway\" goals out of the benchmark</li>\n<li>hopefully mostly preserves the provability of the goals!</li>\n<li>may shuffle things up just enough to prevent memorization<br>\n(How true the 3rd point is is unclear!)</li>\n</ul>\n</blockquote>\n<p>I think that a past/future split is the best way to evaluate neural theorem provers. The <a href=\"https://huggingface.co/openlm-research/open_llama_3b\">open llama 3b</a> model I finetuned was released on June 7th, so rolling back the finetuning dataset to a mathlib commit from that date would mean all mathlib theorems merged between June 7th and now are fair game for testing. </p>\n<p>This methodology also has the advantage of directly measuring a system's utility to mathlib developers.</p>",
        "id": 386078448,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692436609
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span>, I like the past/future split, but this doesn't really allow for comparisons between different models, does it? (As they'll all have different cut-offs.)</p>",
        "id": 386290361,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692585352
    },
    {
        "content": "<p>I wonder if it's worth having a tool that lists the dates on which every declaration in a library landed in the git repository.</p>",
        "id": 386290480,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692585411
    },
    {
        "content": "<p>Does <a href=\"https://mathlib-changelog.org/\">https://mathlib-changelog.org/</a> not do that?</p>",
        "id": 386291415,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1692585867
    },
    {
        "content": "<p>As far as I'm aware that was never updated from Lean 3 to Lean 4. We'd also want machine-readable data.</p>",
        "id": 386294526,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692587733
    },
    {
        "content": "<p>LeanDojo has a <a href=\"https://leandojo.readthedocs.io/en/latest/traced_data.html#lean_dojo.data_extraction.traced_data.TracedTheorem.get_creation_date\">function</a> intended for getting the creation date of a theorem.  It may not be very reliable since it just uses the file name and line number to query <code>git log</code>. I'm not sure what will happen if the theorem has undergone many modifications. Also, for mathlib4, the git log only reflects the porting progress instead of mathlib's development process?</p>",
        "id": 386295102,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1692588101
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/llmstep/near/386290361\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"284997\">Zhangir Azerbayev</span>, I like the past/future split, but this doesn't really allow for comparisons between different models, does it? (As they'll all have different cut-offs.)</p>\n</blockquote>\n<p>It would require retraining all baselines on the new finetuning dataset. Right now, that's not a problem, since the only strong baseline is ReProver and it's a very small language model. It may become an issue in the future.</p>",
        "id": 386324976,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1692604230
    },
    {
        "content": "<p>Hi all,  we posted a preprint about <code>llmstep v1.0.0</code>: <a href=\"https://arxiv.org/pdf/2310.18457.pdf\">https://arxiv.org/pdf/2310.18457.pdf</a></p>\n<p>Notable additions:</p>\n<ul>\n<li>Reprover (e.g. for CPU)</li>\n<li>Free GPU via Google Colab</li>\n<li>Runtime and theorem proving benchmarks</li>\n</ul>\n<p><a href=\"https://github.com/wellecks/llmstep\">https://github.com/wellecks/llmstep</a></p>",
        "id": 399813732,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1698878402
    },
    {
        "content": "<p>We also added experimental <a href=\"https://arxiv.org/abs/2310.10631\">llemma</a> suggestions that use the preceding file content (or other language models via <code>--hf-model</code>). Currently it requires a GPU that supports vLLM. Here is an example from <code>Examples.lean</code>:<br>\n<a href=\"/user_uploads/3121/9FUA4M7sw0Ou_MZLJ5qZdO5d/Screen-Shot-2023-11-01-at-3.25.54-PM.png\">Screen-Shot-2023-11-01-at-3.25.54-PM.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/9FUA4M7sw0Ou_MZLJ5qZdO5d/Screen-Shot-2023-11-01-at-3.25.54-PM.png\" title=\"Screen-Shot-2023-11-01-at-3.25.54-PM.png\"><img src=\"/user_uploads/3121/9FUA4M7sw0Ou_MZLJ5qZdO5d/Screen-Shot-2023-11-01-at-3.25.54-PM.png\"></a></div>",
        "id": 399814240,
        "sender_full_name": "Sean Welleck",
        "timestamp": 1698878598
    }
]