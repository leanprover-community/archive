[
    {
        "content": "<p>Are there any recent surveys on autoformalisation that include a description of the techniques used and a link to the relevant code? Honest accounts of both successes and failures would be particularly appreciated.</p>",
        "id": 459859686,
        "sender_full_name": "Cris Salvi",
        "timestamp": 1723365592
    },
    {
        "content": "<p>I don’t think there is such a survey. Autoformalization is pretty recent and I don’t think we have hit a stable point where a survey wouldn’t be very much out-of-date.</p>",
        "id": 459944805,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410193
    },
    {
        "content": "<p>Here are some key points that might help:</p>\n<ul>\n<li>Qingxiang Wang has a <a href=\"https://arxiv.org/abs/1912.02636\">few papers</a> trying out auto-formalization using machine translation</li>\n<li>Christian Szegedy wrote <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-53518-6_1\">A Promising Path Towards Autoformalization and General Artificial Intelligence</a> which is a manifesto of sort on the topic.  The idea is that we can get into a virtuous loop where autoformalization and automated theorem proving strengthen each other in a reinforcement learning loop.</li>\n<li>Christian Szegedy gave a talk at <a href=\"https://aitp-conference.org/2020/\">AITP 2020</a> showcasing how hard this topic is and the brick walls they were running into.</li>\n</ul>",
        "id": 459944832,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410222
    },
    {
        "content": "<p>Then everything changed in 2022 with the paper <a href=\"https://arxiv.org/abs/2205.12615\">Autoformalization with Large Language Models</a>.  The main idea was that LLMs for Code (codex at the time) could do autoformalization at a sufficiently useful level (about 25% correct for theorem statements of competition problems IIRC).  That was good enough for generating additional data for reinforcement learning of automated theorem provers.</p>",
        "id": 459944838,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410239
    },
    {
        "content": "<p>Shortly after that paper, a number of works came out (many talked about on this Zulip at length):</p>\n<ul>\n<li><a href=\"https://github.com/zhangir-azerbayev/lean-chat\">LeanChat</a> (a pre-chat-gpt chat tool for autoformalization into Lean)</li>\n<li><a href=\"https://github.com/siddhartha-gadgil/LeanAide\">LeanAide</a>: Autoformalization connected to Lean metaprogramming so can better find and fix translation mistakes</li>\n<li><a href=\"https://arxiv.org/abs/2210.12283\">Draft-Sketch-Prove</a>: Formalize theorem statements into proof sketchs and fill in details with Hammer.  Many papers in automated theorem proving for Isabelle extend Draft-Sketch-Prove:<ul>\n<li><a href=\"https://arxiv.org/abs/2305.16366\">Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving</a></li>\n<li><a href=\"https://arxiv.org/abs/2310.00656\">LEGO-Prover: Neural Theorem Proving with Growing Libraries</a></li>\n<li><a href=\"https://arxiv.org/abs/2309.15806\">Lyra: Orchestrating Dual Correction in Automated Theorem Proving</a></li>\n<li><a href=\"https://arxiv.org/abs/2405.14414\">Proving Theorems Recursively</a></li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/2302.12433\">ProofNet</a>: A benchmark for Lean made by autoformalizing textbook exercises</li>\n<li><a href=\"https://arxiv.org/abs/2403.18120\">Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization</a>: I think the idea is to check informal LLM generated answers by using auto-formalization.</li>\n<li><a href=\"https://arxiv.org/abs/2311.03755\">Multilingual Mathematical Autoformalization</a>: Using autoformalization in reverse (formal to informal) to generate training data for LLMs.</li>\n<li><a href=\"https://arxiv.org/abs/2405.14333\">DeepSeek-Prover</a> and <a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\">AlphaProof</a> both use autoformalized competition problems at a large scale (millions of problems) to train automated theorem provers at scale.</li>\n</ul>\n<p>(I’m sure I’m missing a lot of works.)</p>",
        "id": 459944892,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410259
    },
    {
        "content": "<p>Also, many people have been converting Lean theorems into informal statements for the purpose of either better documentation (automatic documentation), for semantic search (Moogle, LeanSearch), or for making training data for automated theorem proving.</p>",
        "id": 459944935,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410308
    },
    {
        "content": "<p>I’m less aware of work pursuing autoformalization for larger projects (e.g. formalizing a book, or a paper), but I think that would be the next big goal.  Another big goal would be to do more systematic autoformalization, like formalizing a whole database, or a whole encyclopedic reference of some sort.  Or trying to scrap and formalize all theorems of a particular form in some area of mathematics.</p>",
        "id": 459944977,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410342
    },
    {
        "content": "<p>Also just found this benchmark on autoformalization in Lean: <a href=\"https://arxiv.org/html/2406.06555v1\">An Evaluation Benchmark for Autoformalization in Lean4</a></p>",
        "id": 459945459,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410759
    },
    {
        "content": "<p>And this: <a href=\"https://arxiv.org/html/2406.01940v1\">Process-Driven Autoformalization in Lean 4</a></p>",
        "id": 459945553,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410844
    },
    {
        "content": "<p>And: <a href=\"https://arxiv.org/html/2406.03847v2\">Lean Workbook: A large-scale Lean problem set formalized from natural language math problems</a>  (discussed in <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.20Workbook.3A.20A.20large-scale.20Lean.20formalized.20problem.20set\">#Machine Learning for Theorem Proving &gt; Lean Workbook: A large-scale Lean formalized problem set</a>)</p>",
        "id": 459945708,
        "sender_full_name": "Jason Rute",
        "timestamp": 1723410988
    },
    {
        "content": "<p>This is great <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> thanks a lot for the response.</p>",
        "id": 460027995,
        "sender_full_name": "Cris Salvi",
        "timestamp": 1723450999
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> — Would you say that approaches based entirely on LLMs are currently the default methods for autoformalisation? Also, as far as you know, has the combination of MCTS + LLMs + RL with Lean feedback been considered/explored?</p>",
        "id": 529000778,
        "sender_full_name": "Cris Salvi",
        "timestamp": 1752652634
    },
    {
        "content": "<p>I see a lot of efforts on (like some of the papers mentioned in the thread) on informal theorems to formal theorems, however not so much for informal proof to formal proof? <br>\nWhy is that? Is it because formal &amp; informal proof writing are quite different or it’s much harder problem to solve or something else?</p>",
        "id": 536156631,
        "sender_full_name": "Shashank Kirtania",
        "timestamp": 1756184628
    },
    {
        "content": "<p>I think it's just a lot harder than merely translating a theorem statement, but there's a fair number of people working on this.</p>",
        "id": 536299490,
        "sender_full_name": "Phillip Harris",
        "timestamp": 1756236458
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"477600\">Phillip Harris</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Autoformalisation/near/536299490\">said</a>:</p>\n<blockquote>\n<p>there's a fair number of people working on this.</p>\n</blockquote>\n<p>Who are you referring to?  I think the Trinity stuff by Morph is the only thing of that sort released, especially focusing on the harder case where the proof has intermediate definitions and lemmas.</p>",
        "id": 536588926,
        "sender_full_name": "Jason Rute",
        "timestamp": 1756381985
    },
    {
        "content": "<p>(Although as theorem proving gets more advanced, there is a fair amount of proof formalization going on already in automated theorem proving, since usually the model first comes up with a separate natural language proof, either in its chain of thought or through another model.  It seems easier for a model to do complex math reasoning in natural language than in formal language.)</p>",
        "id": 536589054,
        "sender_full_name": "Jason Rute",
        "timestamp": 1756382028
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Only one proof has been published hasn't it? Not the tool itself. At least, I could not find it here:<br>\n<a href=\"https://github.com/morph-labs\">https://github.com/morph-labs</a></p>",
        "id": 536606893,
        "sender_full_name": "Bas Spitters",
        "timestamp": 1756387754
    },
    {
        "content": "<p>That's right. The tool is not publicly available</p>",
        "id": 536616384,
        "sender_full_name": "(deleted)",
        "timestamp": 1756390416
    },
    {
        "content": "<p>Yeah, by “release” I mean releasing results, not tools.</p>",
        "id": 536617087,
        "sender_full_name": "Jason Rute",
        "timestamp": 1756390609
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Autoformalisation/near/536588926\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"477600\">Phillip Harris</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Autoformalisation/near/536299490\">said</a>:</p>\n<blockquote>\n<p>there's a fair number of people working on this.</p>\n</blockquote>\n<p>Who are you referring to?  I think the Trinity stuff by Morph is the only thing of that sort released, especially focusing on the harder case where the proof has intermediate definitions and lemmas.</p>\n</blockquote>\n<p>Yeah, Morph and some other people who haven't released anything yet.</p>",
        "id": 537312749,
        "sender_full_name": "Phillip Harris",
        "timestamp": 1756827138
    },
    {
        "content": "<p>We evaluated common language models on the autoformalization task (just statement translation) and provided additional training focused on two key capabilities required for autoformalization models: formal domain knowledge and informal-to-formal reasoning.<br>\nOur paper is available at: <a href=\"https://arxiv.org/abs/2508.04440\">https://arxiv.org/abs/2508.04440</a>, and the model can be found at: <a href=\"https://huggingface.co/stepfun-ai/StepFun-Formalizer-32B\">https://huggingface.co/stepfun-ai/StepFun-Formalizer-32B</a><br>\nAlthough this work is still in its early stages, we hope it will be helpful to everyone.</p>",
        "id": 537404541,
        "sender_full_name": "Yutong Wu",
        "timestamp": 1756878198
    },
    {
        "content": "<p>Thanks for the evaluation. It is of help. Any plans on adding Goedel-Formalizer-V2 to the comparison?</p>\n<p>PS: I know Goedel-Formalizer-V2 was released <em>one day</em> before your article, but maybe in 2508.04440v2?</p>",
        "id": 537406306,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1756879477
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"900292\">Kelly Davis</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Autoformalisation/near/537406306\">said</a>：</p>\n<blockquote>\n<p>Thanks for the evaluation. It is of help. Any plans on adding Goedel-Formalizer-V2 to the comparison?</p>\n<p>PS: I know Goedel-Formalizer-V2 was released <em>one day</em> before your article, but maybe in 2508.04440v2?</p>\n</blockquote>\n<p>Thank you for the reminder. We will add a comparison with Goedel-Formalizer-V2 in the revised version of the paper.</p>",
        "id": 537407039,
        "sender_full_name": "Yutong Wu",
        "timestamp": 1756879963
    }
]