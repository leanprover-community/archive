[
    {
        "content": "<p>There is a new paper on arXiv called <a href=\"https://arxiv.org/abs/2308.02540\">Top-down Automated Theorem Proving (Notes for Sir Timothy)\n</a> by C. E. Larson, N. Van Cleemput.</p>",
        "id": 383698819,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691686786
    },
    {
        "content": "<p>I haven't read it in detail but it seems very critical of Lean and current ATP methods.  While I do agree that ATP is hitting a wall and we need to find methods which are more big picture, I also find myself disagreeing with a lot of the stuff in this paper.</p>",
        "id": 383698860,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691686795
    },
    {
        "content": "<p>One quick example:</p>\n<blockquote>\n<p>Furthermore “human-style” ATP must also include the design principle of working directly in the domains of the various mathematical sub-fields, with the objects (integers, matrices, graphs, etc) of those sub-fields, without translation to any more fundamental sub-field (for instance, set theory), just as human mathematicians do.</p>\n</blockquote>\n<p>I can't tell if they are talking about traditional FOL ATP systems like Vampire, E, etc.  If so, I think this has a lot of merit.  But if one is talking about Lean, Coq or Isabelle, then while those are not yet used as a general purpose ATPs, I would say the users of Lean/Coq/Isabelle are focused on working directly with integers, matrices, and graphs even if one first defines these objects via some logical foundation.  Indeed, ITPs provide a natural and principled way to represent mathematic objects on a computer so one can work with them.</p>",
        "id": 383698901,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691686802
    },
    {
        "content": "<p>I'll try to read the paper in more depth and summarize it if I have time.</p>",
        "id": 383698975,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691686808
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Top.20Down.20Automated.20Theorem.20Proving/near/383698901\">said</a>:</p>\n<blockquote>\n<p>One quick example:</p>\n<blockquote>\n<p>Furthermore “human-style” ATP must also include the design principle of working directly in the domains of the various mathematical sub-fields, with the objects (integers, matrices, graphs, etc) of those sub-fields, without translation to any more fundamental sub-field (for instance, set theory), just as human mathematicians do.</p>\n</blockquote>\n<p>I can't tell if they are talking about traditional FOL ATP systems like Vampire, E, etc.  If so, I think this has a lot of merit.  But if one is talking about Lean, Coq or Isabelle, then while those are not yet used as a general purpose ATPs, I would say the users of Lean/Coq/Isabelle are focused on working directly with integers, matrices, and graphs even if one first defines these objects via some logical foundation.  Indeed, ITPs provide a natural and principled way to represent mathematic objects on a computer so one can work with them.</p>\n</blockquote>\n<p>I totally agree with you. <br>\nWhen I used Lean, I used it not only as a tool but a library to help me construct proofs and theorems. This is because it has many useful definitions, such as natural numbers, that can be used. If we use a top-down approach, then we have to define everything ourselves. That  would be very tedious.</p>",
        "id": 383793865,
        "sender_full_name": "Min-Hsien Weng",
        "timestamp": 1691716275
    },
    {
        "content": "<p>Ok, I've read up to page 8 and I have a much better picture.  Overall, I think they have some good points, but they continue to use ITPs as a strawman.</p>",
        "id": 384324370,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862669
    },
    {
        "content": "<p>To illustrate their main idea this give this example.  Suppose G is a graph with n vertices.  Assume every vertex of G has degree at least n/2.   Now show that G has a Hamiltonian cycle.  They propose that the way to solve it is not to reason from basic logical rules (as they claim automated theorem provers do), but to pass through high level steps (intermediate lemmas if you well), such as:</p>\n<ul>\n<li>the vertices in every longest path in graph G induce a subgraph of G which is Hamiltonian</li>\n<li>every longest path in graph G contains all of the vertices of the graph</li>\n</ul>\n<p>From these it is then much easier to solve the goal.</p>",
        "id": 384324394,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862676
    },
    {
        "content": "<p>I <span aria-label=\"100\" class=\"emoji emoji-1f4af\" role=\"img\" title=\"100\">:100:</span> percent agree.  Much of classical ATP is built around basically cut-free proof systems where it is impossible to create lemmas.  More modern neural ATP systems also are typically based around finding small next proof steps.  While those next steps can in theory be lemmas (through things like <code>have</code> tactics or Isabelle's Isar style proof), this doesn't seem to be used much or done well yet.  Language models like ChatGPT and Minerva actually seem better at producing high level proofs that anything I've seen in the formal math word, and that is certainly one approach.  A lot of other approaches I've seen, and papers like Draft-Sketch-Prove and Decoding the Enigma start to exploit this by first having a language model give a high level proof in natural language and then translating it to a proof sketch in Isabelle (again progressively breaking the main goal into intermediate lemmas).  In a different direction, papers like <a href=\"https://arxiv.org/abs/2108.11204\">Subgoal search for complex reasoning tasks</a> shows that there is a big advantage to predicting future states instead of next actions.  But still, I think we have a long way to go still.</p>",
        "id": 384324416,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862685
    },
    {
        "content": "<p>What I don't agree with is that the authors seem to think that in ITPs like Lean that one writes a proof in a fundamentally different way.  I'm sure a typical proof of the above theorem in Lean (or even Metamath) would again proceed in a series of high-level lemmas just like they gave.  And further, as papers like Draft-Sketch-Prove has shown, ITPs can actually be a great component inside an automated theorem prover when combined with other high level tools like language models or conjecturing bots.</p>",
        "id": 384324425,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862691
    },
    {
        "content": "<p>Now, what I finally figured out is that the authors have their only research program which has been going on for half a decade of developing a conjecturing tool that is supposed to find conjectures (or intermediate lemmas) just like the ones in the Hamiltonian graph example above.  It is called <a href=\"http://nvcleemp.github.io/conjecturing/\">Conjecturing</a>.  I don't know much about it yet.  It seems to be related to Sage.</p>",
        "id": 384324439,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862697
    },
    {
        "content": "<p>If it actually works well, then I could see it being a good addition to current efforts in automated theorem proving.  It seems clear to me that the most powerful tools in the end will combine neural AI (for interfacing with natural language mathematics and human mathematicians), automated conjecturing (for automated ideation and discovery), computer algebra systems (for computation), and theorem provers (for checking details and building a common library of mathematical facts).  I don't see much need for gate keeping here.  (It is for example sad that there is still so much incompatibility between today's ITPs and CAS systems today.)</p>",
        "id": 384324451,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862703
    },
    {
        "content": "<p>I still haven't read the rest.  I think the rest of the paper is mostly a systematic response to arguments to their \"top-down\" automated theorem proving program.</p>",
        "id": 384324460,
        "sender_full_name": "Jason Rute",
        "timestamp": 1691862707
    },
    {
        "content": "<p>THank <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> For good summary. <br>\nI like the paper describing their approach as top-down programming. Top-down programming is more like how we actually program: start with a function and update it as we learn and find out more problems. This lets us learn the details of the function and make design decisions. Bottom-up programming may hide such information because we are just calling it from a library. </p>\n<p>The author suggested that this top-down approach will help us reproduce the proofs written in the publication. I think the authors would make a good point here, as translating proofs into Lean requires a bit of manual work, and the translation may lose some information, due to abstraction. With the help of LLM, the precision of translation may be improved?!</p>\n<p>I am trying to understand the Conjecturing part of the paper.</p>",
        "id": 384605463,
        "sender_full_name": "Min-Hsien Weng",
        "timestamp": 1691971922
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Are you familiar with \"proof planning\"? It's a research program for automated theorem proving based around the idea of defining high level goal blocks with input preconditions and output postconditions, without necessarily having a proof of the goal block. The idea is to find a high level proof plan and then recursively decompose those goals into smaller goals until they can be solved by ATP. The proof plan is found by coding the plan in STRIPS or PDDP language and feeding it to an automated planner.</p>",
        "id": 386993505,
        "sender_full_name": "Patrick Nicodemus",
        "timestamp": 1692851220
    },
    {
        "content": "<p>it seems related to this.</p>",
        "id": 386993698,
        "sender_full_name": "Patrick Nicodemus",
        "timestamp": 1692851296
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"460251\">@Patrick Nicodemus</span> I'm not.  Does it work?  Are there any good benchmarks on this topic?</p>",
        "id": 387137571,
        "sender_full_name": "Jason Rute",
        "timestamp": 1692900315
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Top.20Down.20Automated.20Theorem.20Proving/near/387137571\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"460251\">Patrick Nicodemus</span> I'm not.  Does it work?  Are there any good benchmarks on this topic?</p>\n</blockquote>\n<p>It was pretty heavily studied for a while, but Afaik it's not an active research area right now. It's not clear if there's a good reason it died out, maybe people just shifted their focus? It sounds sensible to me, AI planning has a solid history and it makes sense to draw on it here. You'd have to talk to somebody who was actually part of one of those groups to get a good appraisal. Which you can do if you want. many of them are still active. </p>\n<p>These research groups built pieces of software called \"proof planners\" where you would write \"methods\" which are like declarative versions of tactics but \"coarser\", the details of how it is to be filled in and implemented are left up to the search engine which combines them. Methods also differ from tactics in that you have to write clear pre and post conditions that the proof planner can use to figure out when they can be chained together to achieve valid proof.</p>\n<p>Here is a paper where three of them are compared and contrasted.<br>\n\"On the comparison of proof planning systems Lambda Clam, Omega and IsaPlanner.\"<br>\n<a href=\"https://www.cl.cam.ac.uk/~mj201/publications/comp_pp_final.pdf\">https://www.cl.cam.ac.uk/~mj201/publications/comp_pp_final.pdf</a></p>\n<p>The technique was first applied in a theorem prover called Oyster (Alan Bundy and the DREAM group at Edinburgh) based on extensional Martin-Lof dependent type theory, to plan proofs based on a technique called \"rippling\" for proving things by induction, it is a rewriting technique to prove the induction step by migrating the variable from the inside out until you reduce it to the inductive hypothesis. This first proof planner was called Clam, there is also Lambda Clam for higher order logic (this is getting off topic, but the point of Lambda Clam is that it is written in a higher order logic programming language called Lambda Prolog which is supposed to be particularly adequate for theorem proving, programming language implementation and similar tasks because it gives you very elegant ways of dealing with free and bound variables. For this reason Lambda Prolog was added as a semi-official tactic language for Coq, the Embeddable Lambda Prolog Interpreter or ELPI for short).</p>\n<p>Another planner called Omega (different research group, Research Center for Artificial Intelligence at Saarbrücken) was integrated with traditional ATP's, knowledge-bases, and CAS's that it could query. Their papers are interesting. <br>\n<a href=\"https://www.sciencedirect.com/science/article/pii/S0004370299000764\">https://www.sciencedirect.com/science/article/pii/S0004370299000764</a><br>\n<a href=\"https://www.sciencedirect.com/science/article/pii/S0004370207001968\">https://www.sciencedirect.com/science/article/pii/S0004370207001968</a></p>\n<p>Lucas Dixon, who is now at Google, wrote a planner called IsaPlanner for Isabelle.<br>\n<a href=\"https://era.ed.ac.uk/handle/1842/1250\">https://era.ed.ac.uk/handle/1842/1250</a><br>\nA sample paper if you want to skim the abstract<br>\n<a href=\"https://ieeexplore.ieee.org/document/8131278\">https://ieeexplore.ieee.org/document/8131278</a><br>\nThis is a philosophical argument that we should pay more attention to high level planning as an integral part of mathematical problem solving<br>\n<a href=\"https://www.cambridge.org/core/journals/review-of-symbolic-logic/article/abs/plans-and-planning-in-mathematical-proofs/6F84A9D231721E7B1B60056F5493D795\">https://www.cambridge.org/core/journals/review-of-symbolic-logic/article/abs/plans-and-planning-in-mathematical-proofs/6F84A9D231721E7B1B60056F5493D795</a></p>",
        "id": 387175260,
        "sender_full_name": "Patrick Nicodemus",
        "timestamp": 1692919116
    }
]