[
    {
        "content": "<p>TLDR: what are the existing Python &lt;--&gt; Lean interfaces and their competitive advantages over each other? </p>\n<p>What are the tools to interface with Lean from Python? I know of three: PyPantograph (I have been bettling with its installation in the past 24 hours, with some success but some issues remain), LeanDojo (I heard it's worse than Pantograph but I don't remember in what way), and LeanClient (very new so I guess there will be many rough edges). My use-case is that our Lean lab at UW is starting an autoformalization project, in which we explore how different LLMs without any fine-tune can close goals. We want to just write Python code without any Lean meta-programming. The main feature we need is to take the current proof state and apply a tactic to it, and report the compilation error or the new proof state.</p>",
        "id": 496436723,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738104215
    },
    {
        "content": "<p>It seems that LeanClient does not allow for a tactic string to be passed but only a whole lean file, so that won't work. But I came across <a href=\"https://github.com/sorgfresser/lean-repl-py\">lean-repl-py</a>. Has anyone experimented with it?</p>",
        "id": 496437822,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738104895
    },
    {
        "content": "<p>For that use case you describe, <a href=\"https://github.com/leanprover-community/repl\">repl</a> should be the best for interacting with Lean during a proof, other than the tools you mentioned.</p>",
        "id": 496437832,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1738104901
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"631691\">@Thomas Zhu</span> , what is the difference between lean-repl and Pantograph and LeanDojo? I am sure there are some differences but I don't see them at first glance</p>",
        "id": 496438035,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738105053
    },
    {
        "content": "<p>repl is lightweight and written only in Lean. I don't know the lean-repl-py you mentioned but on first glance it is a wrapper around repl. We have been using repl for interacting between Python and Lean and the experience is pretty good. LeanDojo (and probably Pantograph) additionally support extracting training data for fine tuning and not just interacting with Lean in real time, but I am not familiar with them enough to give a detailed comparison. I think Pantograph has better support for some tactics and sorry-filling.</p>",
        "id": 496438808,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1738105471
    },
    {
        "content": "<p>In their <a href=\"https://arxiv.org/pdf/2410.16429\">paper</a> they mention support for have, conv, and calc tactics, which I am not sure repl fully supports. They also allow a model to submit a proof draft with sorry as holes, and then work on the holes. (cc <span class=\"user-mention\" data-user-id=\"599027\">@Leni Aniva</span>)</p>",
        "id": 496439353,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1738105774
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"631691\">Thomas Zhu</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496439353\">said</a>:</p>\n<blockquote>\n<p>In their <a href=\"https://arxiv.org/pdf/2410.16429\">paper</a> they mention support for have, conv, and calc tactics, which I am not sure repl fully supports. They also allow a model to submit a proof draft with sorry as holes, and then work on the holes. (cc <span class=\"user-mention silent\" data-user-id=\"599027\">Leni Aniva</span>)</p>\n</blockquote>\n<p>yeah pantograph supports have, conv, and calc. It is also written only in Lean</p>",
        "id": 496439480,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1738105839
    },
    {
        "content": "<p>The problem with asking for Python &lt;--&gt; Lean interfaces is that there are a myriad of ways that one could \"talk to Lean\" from within Python.  Are you interested in:</p>\n<ul>\n<li>Just proving a single theorem with a language model (you could use some sort of REPL or just print the proof to a file and run lean as sub-process on the file)</li>\n<li>interacting with a whole file similar to what you do in a Lean editor (For that, it would be good to interact with Lean LSP language server.  See <span class=\"user-mention\" data-user-id=\"802311\">@Oliver Dressler</span>'s <a href=\"https://github.com/oOo0oOo/leanclient\">https://github.com/oOo0oOo/leanclient</a>  )</li>\n<li>Guiding a tree search from within Python.  (For that you need a tool which has the ability to jump from one proof state in a proof to another one.  I think maybe LeanDojo, Pantograph, or maybe one of the REPLs could help.)</li>\n<li>Extract data.  For that it depends on what sort of data.  Any of the above tools might help.</li>\n</ul>",
        "id": 496449786,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738111892
    },
    {
        "content": "<p>Honestly, it sounds like you are just in the first case and could get by with no special application.  Use Python to edit your file and to run Lean from the command line as a process.  Parse the messages which aren't too hard to parse.  Easy-peasy. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 496449806,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738111910
    },
    {
        "content": "<p>I wrote more about the many options for interacting with Lean here: <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/leanclient.3A.20Interact.20with.20Lean.20from.20Python.20via.20the.20LSP\">#Machine Learning for Theorem Proving &gt; leanclient: Interact with Lean from Python via the LSP</a> But that was also more about the low-level methods.  It is true there are a lot of high-level tools now available, some in Lean, some in Python.  Don't discount some of the tools written in Lean.  Some are reasonably user-facing and don't involve metaprogramming.</p>",
        "id": 496449810,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738111917
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"414458\">@Vasily Ilin</span>  Finally, I think it is best if you are <em>very specific</em> about what features you need.</p>\n<ul>\n<li>Do you need to see goal states?</li>\n<li>Do you need to interact with existing mathlib files or will all your files be new files with a rigid structure and just a single theorem per file?</li>\n<li>Do you need any special Lean features (like tooltips, goal states for expressions), or just the ability to run Lean on a proof?</li>\n<li>Will your proofs be only tactic proofs or also term proofs?</li>\n<li>Will your AI write separate lemmas (which go before the main theorem)?</li>\n<li>Do you need the ability to interface with other theorem provers?  If so how do you envision this happening?</li>\n</ul>",
        "id": 496449899,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738111941
    },
    {
        "content": "<p>Also, you say you are doing auto-formalization, but then you mention proving theorems.  Will you be auto-formalizing statements, definitions, or proofs?  Or all three?</p>",
        "id": 496449936,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738111975
    },
    {
        "content": "<p>And if what you are doing is a bit complicated, don't hesitate to at least consult if not partner with others who know metaprogramming.</p>",
        "id": 496450176,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738112155
    },
    {
        "content": "<p>Sorry, I missed the part where you said you need to apply a single tactic to the current proof state and see the result.  Then you need a REPL-like tool.  But the follow-up question is if you plan to do a tree search (best-first search, Monte Carlo tree search, or iterative deepening)?   You need a REPL that supports this.  Also note, if you already have a proof-state-to-tactic predictor, you could just plug it into Aesop and have a full proof-search tool.  That is what Lean Copilot does for example.  (And I think that is what ABEL may also do when released if I understand.)  It might require a bit more Lean programming, but not too much if your tactic predicting LLM is hosted over a network connection (from the internet or a Python server).</p>",
        "id": 496451508,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738112885
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496449936\">said</a>:</p>\n<blockquote>\n<p>Also, you say you are doing auto-formalization, but then you mention proving theorems.  Will you be auto-formalizing statements, definitions, or proofs?  Or all three?</p>\n</blockquote>\n<p>I might be mis-using the term. For me \"autoformalization\" is \"automatic formalization/proof of an already stated theorem\", not \"first translating from natural language to formal language and then formalizing\".</p>",
        "id": 496452941,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738113913
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496451508\">said</a>:</p>\n<blockquote>\n<p>Sorry, I missed the part where you said you need to apply a single tactic to the current proof state and see the result.  Then you need a REPL-like tool.  But the follow-up question is if you plan to do a tree search (best-first search, Monte Carlo tree search, or iterative deepening)?   You need a REPL that supports this.  Also note, if you already have a proof-state-to-tactic predictor, you could just plug it into Aesop and have a full proof-search tool.  That is what Lean Copilot does for example.  (And I think that is what ABEL may also do when released if I understand.)  It might require a bit more Lean programming, but not too much if your tactic predicting LLM is hosted over a network connection (from the internet or a Python server).</p>\n</blockquote>\n<p>Woah! I don't know anything about aesop but I love the idea of \"just plugging it into aesop\". What's a good resource to learn more?</p>",
        "id": 496453082,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738114001
    },
    {
        "content": "<p>Autoformalization usually implies translation from informal to formal, not automated theorem proving.  So if you are not translating something I would say it isnâ€™t autoformalization.</p>",
        "id": 496453118,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738114029
    },
    {
        "content": "<p>I guess I am doing autoproving then. Does not roll off the tongue the same way as autoformalization</p>",
        "id": 496453425,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738114260
    },
    {
        "content": "<p>Ok, I've probably oversold Aesop.  But the example is here: <a href=\"https://github.com/leanprover-community/aesop/blob/caa2968c7b51ea680605e7d500c6739912aa0446/AesopTest/TacGen.lean#L10\">https://github.com/leanprover-community/aesop/blob/caa2968c7b51ea680605e7d500c6739912aa0446/AesopTest/TacGen.lean#L10</a></p>\n<p>What you would need to supply is a server which takes as input a goal state string, and outputs a list of tactic suggestions with a floating point number between 0 and 1 for each specifying how good that tactic is (usually you use the model's reported probability, i.e. 2^log_prob, of the LLM prediction, unless you have a value function).  It is okay to use JSON for your server communication.  Then a person who knows a bit of Lean programming could write you a little function which replaces <code>myTacGen</code> in this example and interfaces with your server.  Running <code>aesop</code> will call your model at each step of its tree search. But there are lots of little paper cuts involved.  Maybe LeanDojo or Pantograph would be better.</p>",
        "id": 496453882,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738114599
    },
    {
        "content": "<p>I think a question that remains is whether you want to benchmark the proving ability of different language models, or do you want to develop a tool someone can use to prove theorems. For the former you can do the proof search logic (if any) in Python. For the latter you can look into Aesop + tacGen, and/or LeanCopilot/LLMLean/etc.</p>",
        "id": 496454383,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1738114936
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"631691\">Thomas Zhu</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496454383\">said</a>:</p>\n<blockquote>\n<p>I think a question that remains is whether you want to benchmark the proving ability of different language models, or do you want to develop a tool someone can use to prove theorems. For the former you can do the proof search logic (if any) in Python. For the latter you can look into Aesop + tacGen, and/or LeanCopilot/LLMLean/etc.</p>\n</blockquote>\n<p>That's a good way to put it. I am very far from developing any kind of useful tool, I just want to learn how to do these sort of things. Since I like LLMs, I thought I'd combine the two. So yea, essentially my goal is to benchmark the ability of existing LLMs to perform auto-proving (sounds much worse that auto-formalization...) with a naive scaling-test-time-compute strategy</p>",
        "id": 496454718,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738115166
    },
    {
        "content": "<p>For benchmarking, I strongly recommend our work miniCTX as a modern benchmark for LLMs. In fact, we are currently working on evaluating theorem proving abilities of LLMs and will release a type of benchmark/leaderboard soon!</p>",
        "id": 496455482,
        "sender_full_name": "Thomas Zhu",
        "timestamp": 1738115641
    },
    {
        "content": "<blockquote>\n<p>naive scaling-test-time-compute strategy</p>\n</blockquote>\n<p>Note, that tree search (which I assume you will have if you are doing individual tactic generation) is one of the original scaling-test-time-compute strategies.  Almost all of the original neural theorem provers used it.  (And the newer ones like DeepSeek-Prover got better when they added  tree search.) You see the same sort of log-linear scaling as you extend the search time (or search tree size) as you do in modern test-time-scaling plots.  There are even folk observations (not well tested) that running smaller models in a tree search are just as good as running larger models as long as you let them search for the same amount of overall compute. (The only two methods that I know of which likely scale better than tree search are multiple independent tree searches as in the HTPS paper, and reinforcement learning plus tree search, like in AlphaProof.)</p>",
        "id": 496456765,
        "sender_full_name": "Jason Rute",
        "timestamp": 1738116454
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496456765\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>naive scaling-test-time-compute strategy</p>\n</blockquote>\n<p>Note, that tree search (which I assume you will have if you are doing individual tactic generation) is one of the original scaling-test-time-compute strategies.  Almost all of the original neural theorem provers used it.  (And the newer ones like DeepSeek-Prover got better when they added  tree search.) You see the same sort of log-linear scaling as you extend the search time (or search tree size) as you do in modern test-time-scaling plots.  There are even folk observations (not well tested) that running smaller models in a tree search are just as good as running larger models as long as you let them search for the same amount of overall compute. (The only two methods that I know of which likely scale better than tree search are multiple independent tree searches as in the HTPS paper, and reinforcement learning plus tree search, like in AlphaProof.)</p>\n</blockquote>\n<p>Yes, I am not trying to get a new SotA with this project, just playing around with the ideas :). Maybe once we learn how to do this naive strategy we'll branch out into some RL too. But not now</p>",
        "id": 496457657,
        "sender_full_name": "Vasily Ilin",
        "timestamp": 1738117123
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"414458\">Vasily Ilin</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496437822\">said</a>:</p>\n<blockquote>\n<p>It seems that LeanClient does not allow for a tactic string to be passed but only a whole lean file, so that won't work. But I came across <a href=\"https://github.com/sorgfresser/lean-repl-py\">lean-repl-py</a>. Has anyone experimented with it?</p>\n</blockquote>\n<p>Just to clarify, maybe I missunderstood: The LSP does allow for incremental updates (see <a href=\"https://github.com/oOo0oOo/leanclient/blob/main/examples/simple_repl.ipynb\">example</a>). But you are going to have to keep track of coords (line, char), indentation etc yourself.</p>\n<p>See <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/leanclient.3A.20Interact.20with.20Lean.20from.20Python.20via.20the.20LSP\">limitations</a> of the LSP...</p>",
        "id": 496489187,
        "sender_full_name": "Oliver Dressler",
        "timestamp": 1738137620
    },
    {
        "content": "<p>Before diving into more complex tree search strategies, I would recommend to first implement the simple inference-time-compute strategy of \"LLM-Lean feedback loop\", where LLM-written code is passed to Lean and any error messages passed back. Especially for general-purpose LLMs without any fine tunes, this can help smooth over the LLM's imperfect grasp of Lean 4 syntax by allowing the LLM to fix it.  My implementation <a href=\"https://github.com/GasStationManager/LeanTool\">here</a>.</p>",
        "id": 496853459,
        "sender_full_name": "GasStationManager",
        "timestamp": 1738265627
    },
    {
        "content": "<p>There's also my ancient <a href=\"https://www.youtube.com/watch?v=CEwRMT0GpKo\">Sagredo</a> implementation of this same loop.</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"CEwRMT0GpKo\" href=\"https://www.youtube.com/watch?v=CEwRMT0GpKo\"><img src=\"https://uploads.zulipusercontent.net/334dd3177945ef6955a1e621087a23617632651b/68747470733a2f2f692e7974696d672e636f6d2f76692f434577524d543047704b6f2f64656661756c742e6a7067\"></a></div>",
        "id": 496878764,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1738275399
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"414458\">Vasily Ilin</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/496437822\">said</a>:</p>\n<blockquote>\n<p>It seems that LeanClient does not allow for a tactic string to be passed but only a whole lean file, so that won't work. But I came across <a href=\"https://github.com/sorgfresser/lean-repl-py\">lean-repl-py</a>. Has anyone experimented with it?</p>\n</blockquote>\n<p>I am maintaining lean-repl-py as part of my thesis, but it is not really much more than a convenient wrapper around the Lean repl and the python subprocess module. I just found myself re-writing the same 150 lines of code every time I started something new, so I separated this. Let me know if you end up using it and if there's anything that would make it more convenient though!<br>\nPersonally, I think Pantograph is most likely the best that's out there.</p>",
        "id": 498200701,
        "sender_full_name": "Simon Sorg",
        "timestamp": 1738872041
    },
    {
        "content": "<p>If anyone is still looking for a gym environment for theorem proving, our environment <code>itp-interface</code> (<a href=\"https://github.com/trishullab/itp-interface\">https://github.com/trishullab/itp-interface</a>) might come in handy. It is implemented in Python and can support both Lean 4 and Coq.</p>\n<p>So what's different?</p>\n<ul>\n<li>Supports both Lean 4 and Coq. Yes, you just have to write code only once (for proof automation) and it will work for both the ITPs!</li>\n<li>Supports parallel tactic execution, which can help a lot while automatically searching for proofs through LLMs. It can maintain a proof environment pool which can help in running multiple tactics seamlessly for the same state.</li>\n<li>Manages memory more efficiently. If you use Lean 4 REPL, then after a couple of backtracks the JRPC bloats and can take a large amount of memory. We avoid such pitfalls.</li>\n<li>Lean 4 REPL proof mode has some issues regarding accepting incorrect proofs, we avoid that. (See <a href=\"https://github.com/leanprover-community/repl/issues/44\">https://github.com/leanprover-community/repl/issues/44</a>)</li>\n<li>One can in parallel collect proof step data across multiple repositories.</li>\n<li>Support for multiple versions of Coq and Lean 4</li>\n<li>The library is designed to scale and run on <code>ray cluster</code> while doing a highly parallel proof search.</li>\n</ul>\n<p>Setup is super easy:</p>\n<ol>\n<li>pip install itp-interface</li>\n<li>install-itp-interface</li>\n<li>We are done setting up for Lean 4. No more installation steps (including the need to install Lean 4) and other hassles. <br>\n(See our python package: <a href=\"https://pypi.org/project/itp-interface/\">https://pypi.org/project/itp-interface/</a> for more details)</li>\n</ol>\n<p>We build our work on top of the libraries Lean 4 REPL(<a href=\"https://github.com/leanprover-community/repl\">https://github.com/leanprover-community/repl</a>) and Coq Serapy (<a href=\"https://github.com/HazardousPeach/coq_serapy\">https://github.com/HazardousPeach/coq_serapy</a>). </p>\n<p>I would like to thank my co-contributer: <span class=\"user-mention\" data-user-id=\"644040\">@George Tsoukalas</span> , and the community at Zulip for giving us valuable feedback while building this tool. </p>\n<p>This tool can be further supplemented by our proof search tool (<a href=\"https://pypi.org/project/proof-wala/\">https://pypi.org/project/proof-wala/</a>) which can run multilingual proof searches for Coq and Lean. The two can be used for end-to-end proof generation given a theorem in Lean 4 or Coq. The <code>proof-wala</code> tool also supports visualization of tree searched and annotation of proof trees which can be further used for some form of RL style training.</p>\n<p>Looking forward to any feedback you might have.</p>",
        "id": 498675876,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1739166944
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657996\">Amitayush Thakur</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/498675876\">said</a>:</p>\n<blockquote>\n<p>If anyone is still looking for a gym environment for theorem proving, our environment <code>itp-interface</code> (<a href=\"https://github.com/trishullab/itp-interface\">https://github.com/trishullab/itp-interface</a>) might come in handy. It is implemented in Python and can support both Lean 4 and Coq.</p>\n<p>So what's different?</p>\n<ul>\n<li>Supports both Lean 4 and Coq. Yes, you just have to write code only once (for proof automation) and it will work for both the ITPs!</li>\n<li>Supports parallel tactic execution, which can help a lot while automatically searching for proofs through LLMs. It can maintain a proof environment pool which can help in running multiple tactics seamlessly for the same state.</li>\n<li>Manages memory more efficiently. If you use Lean 4 REPL, then after a couple of backtracks the JRPC bloats and can take a large amount of memory. We avoid such pitfalls.</li>\n<li>Lean 4 REPL proof mode has some issues regarding accepting incorrect proofs, we avoid that. (See <a href=\"https://github.com/leanprover-community/repl/issues/44\">https://github.com/leanprover-community/repl/issues/44</a>)</li>\n<li>One can in parallel collect proof step data across multiple repositories.</li>\n<li>Support for multiple versions of Coq and Lean 4</li>\n<li>The library is designed to scale and run on <code>ray cluster</code> while doing a highly parallel proof search.</li>\n</ul>\n<p>Setup is super easy:</p>\n<ol>\n<li>pip install itp-interface</li>\n<li>install-itp-interface</li>\n<li>We are done setting up for Lean 4. No more installation steps (including the need to install Lean 4) and other hassles. <br>\n(See our python package: <a href=\"https://pypi.org/project/itp-interface/\">https://pypi.org/project/itp-interface/</a> for more details)</li>\n</ol>\n<p>We build our work on top of the libraries Lean 4 REPL(<a href=\"https://github.com/leanprover-community/repl\">https://github.com/leanprover-community/repl</a>) and Coq Serapy (<a href=\"https://github.com/HazardousPeach/coq_serapy\">https://github.com/HazardousPeach/coq_serapy</a>). </p>\n<p>I would like to thank my co-contributer: <span class=\"user-mention silent\" data-user-id=\"644040\">George Tsoukalas</span> , and the community at Zulip for giving us valuable feedback while building this tool. </p>\n<p>This tool can be further supplemented by our proof search tool (<a href=\"https://pypi.org/project/proof-wala/\">https://pypi.org/project/proof-wala/</a>) which can run multilingual proof searches for Coq and Lean. The two can be used for end-to-end proof generation given a theorem in Lean 4 or Coq. The <code>proof-wala</code> tool also supports visualization of tree searched and annotation of proof trees which can be further used for some form of RL style training.</p>\n<p>Looking forward to any feedback you might have.</p>\n</blockquote>\n<p>Check out more details here:  <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/itp-interface.20and.20proof-wala\">#Machine Learning for Theorem Proving &gt; itp-interface and proof-wala</a></p>",
        "id": 498893722,
        "sender_full_name": "Amitayush Thakur",
        "timestamp": 1739232967
    },
    {
        "content": "<p>A couple of updates to <a href=\"https://github.com/GasStationManager/LeanTool\">LeanTool</a>, my LLM-Lean interface, that may be of interest to folks here:</p>\n<ul>\n<li>\n<p>Now supports reasoning models including o3-mini, Deepseek r1, and gemini-flash-thinking. Now you have the reasoning power of o3-mini / r1, <br>\n that also outputs valid Lean 4. (Some of these models like r1 and gemini-flash-thinking do not support function calling, the new LeanTool update allows them to pass code in plain text instead. )</p>\n</li>\n<li>\n<p>OpenAI-compatible API proxy server: queries get routed to the LLM, which talks to Lean directly, fix errors if any, and returns valid Lean 4 code.  Works with coding assistants including Cline and Continue, and chat interfaces like OpenWebUI. I have set up a demo for the proxy server at <a href=\"http://www.codeproofarena.com:8800/v1\">http://www.codeproofarena.com:8800/v1</a>  . This is bring-your-own-api-key; if you have an API key for Anthropic / OpenAI / Google / Deepseek, you can point your app to the above API base address and pass the API key. You can of course run your own server if you install LeanTool locally; I'm taking <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> 's advice from his recent Lean Together talk to make tools more easily available for users to try.</p>\n</li>\n</ul>",
        "id": 499809277,
        "sender_full_name": "GasStationManager",
        "timestamp": 1739559941
    },
    {
        "content": "<p>What do we need to have this usable from within VSCode?</p>",
        "id": 499855294,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1739578126
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/499809277\">said</a>:</p>\n<blockquote>\n<p>the proxy server at <a href=\"http://www.codeproofarena.com:8800/v1\">http://www.codeproofarena.com:8800/v1</a></p>\n</blockquote>\n<p>I get a 404.</p>",
        "id": 499855387,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1739578179
    },
    {
        "content": "<p>Both <a href=\"https://www.continue.dev/\">Continue</a> and  <a href=\"https://cline.bot/\">Cline</a> are VSCode extensions. Once installed, for Cline you'll be prompted to set up a model; put the above URL as the API base URL and model type as \"openAI-compatible\", model name \"sonnet\" for sonnet 3.5, or \"gpt\" for gpt 4o, \"deepseek\" for Deepseek v3. And fill in your API key for the model. <br>\nFor Continue it will be similar; there will  be a link to the config file you can click on, and then add a custom model as in the instructions here <a href=\"https://docs.continue.dev/customize/model-providers/openai\">https://docs.continue.dev/customize/model-providers/openai</a></p>",
        "id": 499860395,
        "sender_full_name": "GasStationManager",
        "timestamp": 1739581137
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/499855387\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Which.20Python.20.3C--.3E.20Lean.20interfaces.20exist.3F/near/499809277\">said</a>:</p>\n<blockquote>\n<p>the proxy server at <a href=\"http://www.codeproofarena.com:8800/v1\">http://www.codeproofarena.com:8800/v1</a></p>\n</blockquote>\n<p>I get a 404.</p>\n</blockquote>\n<p>That URL should be fine as the API base URL; the actual request goes to something like <code>.../v1/chat/completions</code></p>",
        "id": 499861361,
        "sender_full_name": "GasStationManager",
        "timestamp": 1739581713
    },
    {
        "content": "<p>I also get a 404</p>",
        "id": 499894166,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1739607513
    },
    {
        "content": "<p>Getting 404 from the browser is okay / normal as <a href=\"http://www.codeproofarena.com:8800/v1\">http://www.codeproofarena.com:8800/v1</a> is the \"API base URL\" you would put in the config of your app. To send a raw POST request  you'd send to <a href=\"http://www.codeproofarena.com:8800/v1/chat/completions\">http://www.codeproofarena.com:8800/v1/chat/completions</a></p>",
        "id": 499932157,
        "sender_full_name": "GasStationManager",
        "timestamp": 1739640022
    }
]