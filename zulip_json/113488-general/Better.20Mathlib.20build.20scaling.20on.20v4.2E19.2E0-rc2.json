[
    {
        "content": "<p>PSA: <span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span>'s work on theorem-level parallelism speeds up Mathlib builds substantially on systems with many cores. Here are Mathlib build times on AWS <code>c8g</code> instances with different numbers of cores:</p>\n<table>\n<thead>\n<tr>\n<th>Instance type</th>\n<th>cores</th>\n<th>v4.19.0-rc2</th>\n<th>v4.18.0</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>c8g</td>\n<td>16</td>\n<td>30m17s</td>\n<td>30m15s</td>\n</tr>\n<tr>\n<td>c8g</td>\n<td>32</td>\n<td>17m58s</td>\n<td>19m36s</td>\n</tr>\n<tr>\n<td>c8g</td>\n<td>64</td>\n<td>12m49s</td>\n<td>16m1s</td>\n</tr>\n<tr>\n<td>c8g</td>\n<td>96</td>\n<td>11m38s</td>\n<td></td>\n</tr>\n<tr>\n<td>c8g</td>\n<td>192</td>\n<td>12m51s</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>So we have no difference on 16 cores, but 32 got somewhat faster and on 64 there's a big difference. 96 and 192 are still not worth it.</p>\n<p>I also tested different AWS instance types. <code>c8g</code> (Gravitron, ARM64) seems to be consistently the fastest. For x86, <code>c7a</code> (AMD) is much faster than <code>c7i</code> (Intel) or any of the other <code>c*</code> instances.</p>\n<p>(Disclosure: I work for AWS now.)</p>",
        "id": 510863723,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1744099050
    }
]