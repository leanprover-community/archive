[
    {
        "content": "<p>This thread is for discussion of the <a class=\"stream-topic\" data-stream-id=\"113486\" href=\"/#narrow/channel/113486-announce/topic/AI.20for.20math.20fund\">#announce &gt; AI for math fund</a> announcement.</p>",
        "id": 486407232,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1733439043
    },
    {
        "content": "<p>I really hope that both applications and funding decisions will focus on building practical tools, and will try to ensure that the funded projects produce widely used tools. (i.e. not just research notes about tools that aren't usable in practice...)</p>",
        "id": 486409838,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1733440377
    },
    {
        "content": "<p>There is so much basic infrastructure that we are missing here. (I know that people are working on some of these already.)</p>",
        "id": 486409852,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1733440384
    },
    {
        "content": "<ul>\n<li>A generic Lean framework for talking to LLMs. (e.g. with a pluggable model backend, and a uniform frontend for running chat sessions (in a monad!) and handling the task of extracting code blocks, running them in Lean, and extracting the messages/goal states ready for the agent to hand back to the model).</li>\n<li>A <code>#formalize</code> command built on top of that.</li>\n<li>A general purpose library for Monte Carlo tree search for proofs.</li>\n<li>A replacement for the past-its-use-by-date Lean REPL.</li>\n<li>Python bindings for a REPL.</li>\n<li>Data extraction tools. We have <code>lean-training-data</code>, <code>LeanDojo</code>, and <code>Pantograph</code>, but we could do better!</li>\n<li>A tool for \"indexing sorries\", e.g. that can go through branches of github repositories, and record the locations (along with git SHAs and dependency/toolchain information) of <code>sorry</code>s, in a way that can be reproducibly \"replayed\" (preferably with direct integration with a good REPL, so you can just \"load the state of a sorry\" with a single command).</li>\n<li>General purpose premise selection tools</li>\n<li>Hammers</li>\n<li>Improving <code>Plausible</code> (Lean's counterexample generating library) from a proof of concept to a real tool.</li>\n<li>Designing an open, extensible, automatically generated \"library\". (Containing statements both generated by autoformalization and \"fuzzing\" of existing open statements, and containing proofs and disproofs of statements, along with provenance information for statements and proofs.)</li>\n<li>Good data sets for training/testing/benchmarking specialized models that try to answer \"how likely is this statement to be true\" and \"how long is the proof of this statement likely to be\".</li>\n</ul>",
        "id": 486409865,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1733440389
    },
    {
        "content": "<p>If I had infinite time this is what I would work on, and if I had infinite money this is what I would hire for. :-)</p>",
        "id": 486409868,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1733440390
    },
    {
        "content": "<p>I second usable tools.  Ideally, such projects, if for Lean, should have a good mix of folks with skills across AI, Lean metaprogramming, and Lean practitioners with usable tools in mind.  Take premise selection for example from Kim's list above.  Let's say you want to make a useful premise selection tool for Lean users.  On the AI side, there are questions about how to design a good premise selection tool that is fast enough, doesn't break the bank, and is robust to new definitions and lemmas being added or changed.  On the metaprogramming side, there are different questions: If you are doing a key-query system, where are the keys stored?  What do you do if the premises change?  Can your system work across recent versions?  Is there a mechanism for loading previously computed keys?  How about computing keys for new definitions?  How about computing keys for definitions in the current file?  How about computing the keys during CI?  (This is all starting to look like .olean files, by the way.  Can one just piggyback on that system?)  If instead, it is more practical for the premise selector to live on a server (say because they are too big or because it can be integrated with LeanSearch), how does that work?  Is there a LeanSearch-like API, but one that can be prompted with the current theorem?  Can this be integrated with the tactic framework or with the vscode plugin or language server?  Can the Lean/Mathlib version be communicated automatically?  On the user experience side, there are questions about what premise selection is needed for, if it needs to be a tactic or plugin, if it would interfere with privacy to have it hosted on a server, and if one needs to only pick premises in the current environment or also outside the environment.</p>",
        "id": 486418387,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733445418
    },
    {
        "content": "<p>I worry a lot about the current tools being built (including by me).  There is so little taken into account about user experience or practical ITP integration.</p>",
        "id": 486419079,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733445888
    },
    {
        "content": "<p>My interest is mainly in AI for code generation, but do have a shared interest in building AI-friendly infrastructure &amp; tools for Lean. <br>\nRight now I'm just writing some tools as I need them. E.g. <a href=\"https://github.com/GasStationManager/LeanTool\">a plugin to allow LLMs to invoke lean</a>. I try to make them generally useful, but they are still designed with my use cases in mind. The hope is that by open sourcing them, folks can build on these initial ideas, perhaps add features to suit their needs.</p>",
        "id": 486432799,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733455052
    },
    {
        "content": "<p>I think the reason usable tools don't get attention is because these types of contributions are not valued in academic hiring. Until academia changes the way it evaluates research contributions (especially in theoretical areas), I don't see this changing too much.</p>",
        "id": 486488860,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1733482888
    },
    {
        "content": "<p>Even if a non-academic person gets the grant, who is going to maintain these tools and how will they get paid, once the grant expires?</p>",
        "id": 486489665,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1733483112
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 486492116,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1733483915
    },
    {
        "content": "<p>Different institutions count beans differently, but winning a grant like this would count as professional development at some places. Of course, that doesn't scale. If there was an \"overlay\" journal that published (short) papers representing authors' work on math infrastructure, it would help a college T&amp;P committee recognize this labor as scholarship.</p>",
        "id": 486503438,
        "sender_full_name": "Steven Clontz",
        "timestamp": 1733488139
    },
    {
        "content": "<p>On the one hand, yes it could help. On the other hand, the term \"scholarship\" needs to  be defined more broadly</p>",
        "id": 486513490,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1733491465
    },
    {
        "content": "<p>Yeah, the journal solution is a polyfill for legacy metrics to evaluate faculty; helping institutions recognize modern academic labor more broadly is an important (and harder) problem as well.</p>",
        "id": 486590253,
        "sender_full_name": "Steven Clontz",
        "timestamp": 1733517160
    },
    {
        "content": "<p>Pantograph can be a successor of Lean REPL. (Disclaimer: I'm the main developer of Pantograph)</p>\n<ul>\n<li>Its Python bindings support MCTS (thanks to the contribution of one researcher from Cambridge)</li>\n<li>Its data extraction unit is very extensible, and now I'm just adding new extraction features based on user requests</li>\n<li>It has a uniform backend for running tactics and data extractions</li>\n<li>It can \"load sorries\" and that was a main contribution of our paper</li>\n</ul>\n<p>Pantograph is a vital component of my main research project and I don't see its development and maintenance could end as long as that project is active. I could also hand off maintenance of the project to other researchers in the Stanford Centaur Lab.</p>",
        "id": 486612450,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733529410
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/AI.20for.20Math.20fund/near/486418387\">said</a>:</p>\n<blockquote>\n<p>I second usable tools.  Ideally, such projects, if for Lean, should have a good mix of folks with skills across AI, Lean metaprogramming, and Lean practitioners with usable tools in mind.  Take premise selection for example from Kim's list above.  Let's say you want to make a useful premise selection tool for Lean users.  On the AI side, there are questions about how to design a good premise selection tool that is fast enough, doesn't break the bank, and is robust to new definitions and lemmas being added or changed.  On the metaprogramming side, there are different questions: If you are doing a key-query system, where are the keys stored?  What do you do if the premises change?  Can your system work across recent versions?  Is there a mechanism for loading previously computed keys?  How about computing keys for new definitions?  How about computing keys for definitions in the current file?  How about computing the keys during CI?  (This is all starting to look like .olean files, by the way.  Can one just piggyback on that system?)  If instead, it is more practical for the premise selector to live on a server (say because they are too big or because it can be integrated with LeanSearch), how does that work?  Is there a LeanSearch-like API, but one that can be prompted with the current theorem?  Can this be integrated with the tactic framework or with the vscode plugin or language server?  Can the Lean/Mathlib version be communicated automatically?  On the user experience side, there are questions about what premise selection is needed for, if it needs to be a tactic or plugin, if it would interfere with privacy to have it hosted on a server, and if one needs to only pick premises in the current environment or also outside the environment.</p>\n</blockquote>\n<p>I think the development of Lean tooling will be distributed. Maybe one tool handles data extraction, one handles proof search and execution, and one handles interactions in IDEs.</p>",
        "id": 486613291,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733529939
    },
    {
        "content": "<p>Distributed is fine and possibly good, but we also need good interoperability.  So far that has been a huge strength of Lean and Mathlib.  If you have lots of haphazard libraries that don't play well together they are going to be a pain to work with.</p>",
        "id": 486615948,
        "sender_full_name": "Jason Rute",
        "timestamp": 1733532019
    },
    {
        "content": "<p>I think the people with the most incentive to build tools are those building towards some use case beyond the tools, and need to be able to use the tools. Some of them (like me) have background in a different field (AI, or math) but are not necessarily Lean experts. And not necessarily aware of the other use cases. So an interesting question may be whether they can be paired up with the experts in a productive way</p>",
        "id": 486702138,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733603580
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"776090\">GasStationManager</span> <a href=\"#narrow/channel/113488-general/topic/AI.20for.20Math.20fund/near/486702138\">said</a>:</p>\n<blockquote>\n<p>I think the people with the most incentive to build tools are those building towards some use case beyond the tools, and need to be able to use the tools. Some of them (like me) have background in a different field (AI, or math) but are not necessarily Lean experts. And not necessarily aware of the other use cases. So an interesting question may be whether they can be paired up with the experts in a productive way</p>\n</blockquote>\n<p>Rather than pairing people up why not have the users of automation tools file feature requests in git?</p>",
        "id": 486715029,
        "sender_full_name": "Leni Aniva",
        "timestamp": 1733614817
    },
    {
        "content": "<p>What I had in mind was scenarios where the tool in question might not exist; the would-be user of the tool may be happy to try implement something, but would also benefit from help from more experienced people. But yeah likely a good starting point would be a feature request somewhere.</p>",
        "id": 486726226,
        "sender_full_name": "GasStationManager",
        "timestamp": 1733625432
    },
    {
        "content": "<p>I have no connections to this, but a reminder that this is due tomorrow.</p>",
        "id": 492785476,
        "sender_full_name": "Jason Rute",
        "timestamp": 1736445876
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/113488-general/topic/AI.20for.20Math.20fund/near/486409865\">said</a>:</p>\n<blockquote>\n<ul>\n<li>A general purpose library for Monte Carlo tree search for proofs.</li>\n</ul>\n</blockquote>\n<p>Is <code>canonical</code> the closest thing to that?</p>",
        "id": 517398905,
        "sender_full_name": "Louis Cabarion",
        "timestamp": 1746999804
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"683979\">Isak Colboubrani</span> <a href=\"#narrow/channel/113488-general/topic/AI.20for.20Math.20fund/near/517398905\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/113488-general/topic/AI.20for.20Math.20fund/near/486409865\">said</a>:</p>\n<blockquote>\n<ul>\n<li>A general purpose library for Monte Carlo tree search for proofs.</li>\n</ul>\n</blockquote>\n<p>Is <code>canonical</code> the closest thing to that?</p>\n</blockquote>\n<p>For what I think <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> meant here, I don't think Canonical would fit.  I don't think <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> meant general-purpose automation for Lean.  If she did, then Canonical would be one such example.  (I don't know enough about Canonical to know what it is good at, and how it compares to other general-purpose tools like Lean SMT, Grind, Lean Copilot, and general-purpose LLMs.)</p>\n<p>I think <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> was thinking of one of two things (or both):</p>\n<ul>\n<li>A library for RL training of neural agents using AlphaZero-style MCTS.  If so, then Pantograph comes close.  If the training code for ABEL is ever released, it would be the closest.  Of course, AlphaProof and HyperTree Proof Search (and to a lesser extent DeepSeek v1.5) are exactly that, but the training code is likely never going to be released.</li>\n<li>A tool for doing tree search-based inference with machine learning models (maybe strict MCTS or some similar best-first search) for users to prove theorems.  Lean Dojo and Pantograph are somewhat that, and Aesop (not the usual tactic, but the backend framework that lets you hook it to any tactic predictor, which is how LeanCopilot is implemented) is probably the closest work for usable tree-search-based inference inside Lean.</li>\n</ul>",
        "id": 517406458,
        "sender_full_name": "Jason Rute",
        "timestamp": 1747007516
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"683979\">@Isak Colboubrani</span> <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  Training with <a href=\"https://github.com/opendilab/LightZero\">LightZero</a>—one node multiple GPUs using PyTorch's Distributed Data Parallel (DDP)[<a href=\"https://github.com/opendilab/LightZero/issues/207#issuecomment-2041890501\">1</a>]—and using say <a href=\"https://github.com/stanford-centaur/PyPantograph\">PyPantograph</a> at runtime seems like a reasonable, current option. The downside of this setup being that <a href=\"https://github.com/opendilab/LightZero\">LightZero</a> is one node multiple GPUs not multiple nodes multiple GPUs.</p>",
        "id": 517454643,
        "sender_full_name": "Kelly Davis",
        "timestamp": 1747035774
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/113488-general/topic/AI.20for.20Math.20fund/near/486409865\">said</a>:</p>\n<blockquote>\n<ul>\n<li>A general purpose library for Monte Carlo tree search for proofs.</li>\n</ul>\n</blockquote>\n<p>I built <a href=\"https://github.com/sorgfresser/htps\">https://github.com/sorgfresser/htps</a> for this, which implements the HyperTreeProofSearch algorithm. Maybe that's helpful?</p>",
        "id": 517477907,
        "sender_full_name": "Simon Sorg",
        "timestamp": 1747041320
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I built something with RabbitMQ, HTPS, and Repl (using the wonderful LeanInteract by <span class=\"user-mention\" data-user-id=\"321854\">@Auguste Poiroux</span>) to run distributed proof searches over dozens of nodes at the same time. Lmk if someone is interested! Can provide sbatch files for the local cluster here in Cambs, maybe they can be also helpful on other clusters.</p>",
        "id": 517479140,
        "sender_full_name": "Simon Sorg",
        "timestamp": 1747041619
    },
    {
        "content": "<p>You might be interested in the action/state space generated by <code>canonical +refine</code><br>\n<a href=\"/user_uploads/3121/gGLhP9KMuZ6uI40h8woH-ZAG/refine.gif\">refine.gif</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/gGLhP9KMuZ6uI40h8woH-ZAG/refine.gif\" title=\"refine.gif\"><img data-animated=\"true\" data-original-content-type=\"image/gif\" data-original-dimensions=\"800x263\" src=\"/user_uploads/thumbnail/3121/gGLhP9KMuZ6uI40h8woH-ZAG/refine.gif/840x560-anim.webp\"></a></div>",
        "id": 517645801,
        "sender_full_name": "Chase Norman",
        "timestamp": 1747079016
    },
    {
        "content": "<p>I've been looking into the aesop code and it seems that it's the most efficient way to get tree search trajectories for RL. You can get the rewards after the fact by tracing the search tree at the end of the search process. I will post my code when I am done.</p>",
        "id": 520102584,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1748022549
    },
    {
        "content": "<p>The one issue is that aesop currently stops after finding the first solution. Ideally you want it to keep expanding the search tree until a maximum search depth has been reached. But I'm pretty sure the one line calling<code>finishIfProven</code> is responsible for controlling this, so shouldn't be that bad</p>",
        "id": 520102786,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1748022633
    },
    {
        "content": "<p>I think the problem with just doing mcts to get trajectories is that you waste a lot of time traversing over the same common nodes which does not scale very well for larging proof trajectories. I think doing space filling tree search and then extracting all rewards for backprop afterwards is the best way since that's what they do for chess and other combinatorial games</p>",
        "id": 520102963,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1748022718
    }
]