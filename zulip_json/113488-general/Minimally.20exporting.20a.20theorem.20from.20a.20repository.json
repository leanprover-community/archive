[
    {
        "content": "<p>Suppose Github repository X manages to formalize a proof of Theorem T using Mathlib version V.  Meanwhile, Github repository Y, using Mathlib version V', wants to use Theorem T to prove some other theorem T'.  Right now, one has the option of copying the entire formalization of T and its proof from repository X to repository Y, with whatever updates are needed to make it compatible with Mathlib V' instead of Mathlib V, and then import that file as part of the proof of T'.</p>\n<p>But a more lightweight option would be if there were an automated way to export the <em>statement</em> of T as an axiom in a file that uses as minimal a dependency on Mathlib as possible, so that the proofs in repository X give a formalized proof that the axiom in that exported file is a theorem.  Then, one could import that file to repository Y; and if one is lucky, the minimal Mathlib dependency has not changed (up to definitional equivalence) from version V to version V', so the axiom in repository Y has exactly the same type as the one that is proved in repository X.  Then, one could complete the formal proof of T' in repository Y using this axiom file as import, and then the combination of the formalizations in X and Y do give a completely Lean certified proof of T' by basically the nearly disjoint union of two repositories (glued together by the minimal Mathlib import).</p>\n<p>[EDIT: in the case where the minimal Mathlib import differs in some places from version V to version V', one could imagine a modified import in which any type used that is defined the same in both versions is unchanged, but types that are defined differently would be introduced with a slightly different name (e.g., the imported version of a type <code>SomeObject</code> might be introduced as <code>SomeObject'</code>), and then one would have to introduce some helper lemmas creating suitable equivalences between <code>SomeObject</code> with <code>SomeObject'</code> in order to make the imported version of T match the version of T one would actually want to use.]</p>\n<p>Is it feasible to automate this export and import process, and is the minimal core of Mathlib needed to do this stable enough that this would be a useful way for one repository to be able to formally cite another?</p>",
        "id": 539880138,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758051168
    },
    {
        "content": "<p>I would personally be surprised (but maybe not negatively so) if this direction produced something fruitful</p>",
        "id": 539880441,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1758051303
    },
    {
        "content": "<p>I guess I am basically asking for four tools:</p>\n<ol>\n<li>A tool that, given a repository that proves a complicated theorem <code>X:Prop</code> from Mathlib (and possibly some other axioms), generate a minimal import file that <em>defines</em> the type <code>X</code>, but does not provide a term of that type from the given axioms (but this is what the repository can easily accomplish).</li>\n<li>A tool that takes such a minimal import file defining a type <code>X:Prop</code> as input, defined using one version of Mathlib, and produces an analogous file, defined using a different version of Mathlib, that produces a type definitionally equal to <code>X</code>, using naming conventions as similar as possible to the original file (so that types defined in the first version of Mathlib are replaced with the corresponding types in the second version of Mathlib, if they are definitionally equal).</li>\n<li>A tool that, when given two separate files defining two separate types, can check whether these types are definitionally equal or not (I assume this capability already exists within Lean).</li>\n<li>A tool that, when given one repository that supplies a proof of <code>X:Prop</code>, and another repository that assumes as an axiom something definitionally equivalent to <code>X</code> to prove some other result <code>Y:Prop</code>, can combine the two to provide a larger but self-contained proof of <code>Y</code>.</li>\n</ol>\n<p>3 and 4 seem quite doable to me; 1 requires some effort but should be feasible; and 2 is perhaps the one that is a bit tricky unless the core definitions of Mathlib are quite stable.</p>",
        "id": 539901512,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758061051
    },
    {
        "content": "<p>I think generally here <code>axiom</code> is unhelpful: it doesn't really add anything.</p>\n<p>Better is to just ask in 4. that one repository supplies <code>x : X</code> where <code>X : Prop</code>, and the other repository supplies <code>h : X' → Y</code>, where <code>X</code> and <code>X'</code> are definitionally equal (I think defeq is <em>also</em> a red herring, and we don't lose anything to just say <code>X' := X</code>).</p>\n<p>Then of course it is easy to produce a new repository with a proof of <code>Y</code>. You just depend on both repos and write <code>theorem h' : Y := h' x</code>. (Lean just automatically checks the defeq between <code>X</code> and <code>X'</code> at this point.</p>",
        "id": 539903468,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1758062372
    },
    {
        "content": "<p>But what I'm missing is what the spec of step 2. is? Is it not just: \"bump the dependencies, fix errors, and review to check that meaning did not change?\"</p>",
        "id": 539903545,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1758062412
    },
    {
        "content": "<p>The problem is that if the original repository proved <code>X:Prop</code> using an old version of Mathlib, and some key definitions changed between that old version and the current one, then after bumping the type of <code>X</code> might not be definitionally equal to the one that the original repository was working with, but instead some slightly different <code>X'</code>.  So in such cases one would need to define both <code>X</code> and <code>X'</code> in the new repository and have some lemma that proves that <code>X ↔ X'</code>.</p>\n<p>But one would not want to import both the old version and new version of Mathlib together to state and prove <code>X ↔ X'</code>: one would need two versions of <code>Nat</code>, two versions of <code>Real</code>, etc., etc., and it would be a namespace mess.  But presumably many of the types needed to define <code>X</code> (or <code>X'</code>) would coincide: the old version of <code>Nat</code> should be defeq to the new version of <code>Nat</code>, etc..  So to a large extent one could use most of the new version of Mathlib to define the old version of <code>X</code>; there may only be a small number of types in which one has to explicitly re-introduce an older definition of the type (using a slightly different name) in order to define <code>X</code> in the new version of Mathlib.  This should hopefully make the lemma <code>X ↔ X'</code> relatively easy to prove, one just needs to prove some equivalences for the few types which have definitionally changed from one version of Mathlib to the next.</p>",
        "id": 539904000,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758062769
    },
    {
        "content": "<p>For instance, suppose one wants to cite a repository that proves a difficult result that all widgets are quasistable:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">TheoremA</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Widget</span><span class=\"w\"> </span><span class=\"n\">W</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">IsQuasiStable</span><span class=\"w\"> </span><span class=\"n\">W</span>\n</code></pre></div>\n<p>where <code>Widget</code> and <code>IsQuasiStable</code> are defined somewhere in Mathlib.  The idea is that tool 1 would provide a file that says something like</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span><span class=\"bp\">.</span><span class=\"n\">Widget</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span><span class=\"bp\">.</span><span class=\"n\">QuasiStable</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">TheoremA</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Widget</span><span class=\"w\"> </span><span class=\"n\">W</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">IsQuasiStable</span><span class=\"w\"> </span><span class=\"n\">W</span>\n</code></pre></div>\n<p>that one could hopefully just drop into the new repository as an import and prove theorems like <code>TheoremA → TheoremB</code> as you say.  But suppose that the definition of <code>Widget</code> has recently changed in Mathlib to something that is not definitionally equal to the old definition.  Then one has to now also import the deprecated definition of <code>Widget</code> and check compatibility:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Mathlib</span><span class=\"bp\">.</span><span class=\"n\">QuasiStable</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">Widget'</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">...</span><span class=\"w\"> </span><span class=\"c1\">-- this is the deprecated definition of Widget.  But `IsQuasiStable` remains unchanged</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">TheoremA'</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">Widget'</span><span class=\"w\"> </span><span class=\"n\">W</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">IsQuasiStable</span><span class=\"w\"> </span><span class=\"n\">W</span><span class=\"w\"> </span><span class=\"c1\">-- this was proven in the previous repository</span>\n\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">theoremA_bump</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">TheoremA'</span><span class=\"w\"> </span><span class=\"bp\">→</span><span class=\"w\"> </span><span class=\"n\">TheoremA</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"gr\">sorry</span><span class=\"w\"> </span><span class=\"c1\">-- a proof needs to be provided here</span>\n</code></pre></div>\n<p>The purpose of tool 2 is to provide such a file (and ideally, try to fill in the sorry, though perhaps that still has to be done by hand).</p>",
        "id": 539904870,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758063591
    },
    {
        "content": "<p>The algorithm that comes to mind here is content-addressing / hash-consing. I saw something about that from zero-knowledge folks a while back, I believe it was <a href=\"https://github.com/argumentcomputer/yatima\">https://github.com/argumentcomputer/yatima</a> authored by <span class=\"user-mention\" data-user-id=\"451983\">@Arthur Paulino</span></p>\n<p>For example, you would put something like</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">KnownFact</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">…</span>\n\n<span class=\"bp\">#</span><span class=\"n\">assert_hash_eq</span><span class=\"w\"> </span><span class=\"n\">KnownFact</span><span class=\"w\"> </span><span class=\"mi\">0123456789</span><span class=\"n\">abcdef</span>\n</code></pre></div>\n<p>This would protect against drift in the definition of <code>KnownFact</code> as desired, as long as the hashing scheme is stable.</p>\n<p>I guess the zero knowledge idea has an even grander aim, which is to be able to do something like</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">KnownFact</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">…</span>\n\n<span class=\"bp\">#</span><span class=\"n\">assert_hash_eq</span><span class=\"w\">  </span><span class=\"mi\">0123456789</span><span class=\"n\">abcdef</span>\n<span class=\"bp\">#</span><span class=\"n\">check_zk_proof</span><span class=\"w\"> </span><span class=\"mi\">0123456789</span><span class=\"n\">abcdef</span><span class=\"w\"> </span><span class=\"n\">fedcba9876543210</span>\n</code></pre></div>\n<p>which would also be stable</p>",
        "id": 539906899,
        "sender_full_name": "Lawrence Wu (llllvvuu)",
        "timestamp": 1758065361
    },
    {
        "content": "<p>I like this approach! If the hash is invariant with respect to minor updates such as renaming terms in mathlib while maintaining defeq then this could be stable enough to use results as is in many cases and only need minor bump lemmas in other cases.</p>",
        "id": 539910643,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758068370
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"432681\">@John Burnham</span> check this out</p>",
        "id": 539910697,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1758068416
    },
    {
        "content": "<p>It may be desirable to have a hash that can somehow ignore applications of propext. Eg if some object is constructed using some proof of a side condition, and a mathlib bump ends up changing the proof (but not the statement) of that side condition, it would be nice if this didnt affect the hash.</p>",
        "id": 539911172,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758068833
    },
    {
        "content": "<p>Having a useful hash that is stable across Lean + Mathlib updates sounds pretty unlikely to me; you run into immediate trouble with typeclass instances. Consider the <code>+</code> in the statement of FLT:</p>\n<ul>\n<li>Does <code>Nat.instAdd</code> have the same hash as <code>Nat.instSemiring.toAdd</code>? How can we ensure this while banning a <code>+</code> that is secretly <code>*</code>?</li>\n<li>If we add a new <code>op_nsmul</code> field to <code>Semiring</code> (something I've proposed), how can we preserve the hash of <code>Nat.instSemiring</code> when even the type has changed?</li>\n</ul>",
        "id": 539911561,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1758069196
    },
    {
        "content": "<blockquote>\n<p>Does <code>Nat.instAdd</code> have the same hash as <code>Nat.instSemiring.toAdd</code>?</p>\n</blockquote>\n<p>IIUC the aspiration here is yes, provided they reduce to the same recursor, though I don’t have a strong grasp of the performance implications of the amount of unfolding, reduction, and normalization involved. One could even imagine a model where <code>f</code> and <code>g</code> are not defeq and have different hashes, but <code>f x</code> and <code>g x</code> are defeq and have the same hash.</p>\n<p>Actually, I believe the instAddNat vs Nat.toSemiring.toAdd example does not need unfolding; indeed they already <code>#reduce</code> to the same expr. The example of adding a structure field is one that would require (while hashing downstream declarations) unfolding to simplify the projection and use the unchanged hash of the existing field instead of the changed hash of the overall structure. But it is worth noting that even if this isn’t done for computational reasons, it would only lead to spurious hash changes, but the system would still be sound.</p>",
        "id": 539912240,
        "sender_full_name": "Lawrence Wu (llllvvuu)",
        "timestamp": 1758069914
    },
    {
        "content": "<p>The hash doesn't have to stay invariant across <em>all</em> Lean/Mathlib updates; as long as it can be stable under minor ones (or even major updates, if they don't affect the necessary definitions needed to state a result), I think it can be a viable tool.  Suppose for instance that <span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span>'s FLT project is completed and posts a hash of the FLT statement, but then stops maintaining the repository as Mathlib continues to evolve.  Some time later, a new project wants to use FLT, but when computing its own hash of the statement of FLT in the latest version of Mathlib, finds that they no longer agree.  But one can do a bisection search in both repositories of intermediate definitions required to state FLT (the proof is irrelevant), and find the divergence - e.g., in some change to <code>Nat.instAdd</code>.  Then one can (either automatically or manually) introduce the deprecated version of <code>Nat.instAdd</code> (or whatever) into the new repository and build all the way back to a legacy version of the statement of FLT that matches the hash of Kevin's version, but which can be elaborated in the latest version of Mathlib.  Then it is just a matter of deriving the new version of FLT from the legacy one, which could be a tedious task, but probably significantly less time-consuming than bumping the entire proof of FLT to the most recent version of Mathlib.</p>",
        "id": 539912257,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758069934
    },
    {
        "content": "<p>We are developing a new version of the Yatima project called Ix <a href=\"https://github.com/argumentcomputer/ix\">https://github.com/argumentcomputer/ix</a>, which I think should provide some of the desired properties.</p>\n<p>At a high level, the goal of Ix is to be able to compress the typechecking of Lean proofs into succinct cryptographic proofs. The upshot that (assuming the protocol is correct) one will be able to take a long complex proof that may take hours to check with corresponding installation of the right toolchain, downloading of the right dependencies, etc, and generate a small ~1kb proof artifact that can be verified in &lt;100 ms. The fun meme example this enables is to embed a zk-proof of the Lean proof of FLT on a QR code that actually can fit in the margin :-)</p>\n<p>It turns out that a very large part of the work to do this though is figuring how to create meaningful cryptographic commitments to Lean4 kernel programs. In Ix the way we do this is with a new binary serialization format for Lean.Environment that turns the graph of Lean constants into a DAG and then hashes up through the leaves. Crucially, this DAG does <em>not</em> depend on Lean.Name, and creates alpha-invariant hashes for constants (with symbolic and other noncomputational metadata stored and hashed separately to allow for decompilation back into regular Lean types).</p>\n<p>This means that if you have a proof and someone refactors the dependencies in a way that only affects naming, but not the actual structure, the Ix hash of the proof will be stable.</p>\n<p>That said, this is for kernel terms after elaboration, so in practice I suspect that tactics and other metaprogramming will cause the hashes to change on many source changes, including imports, possibly renaming, or even entropy since metaprograms can do IO. So I don't know how useful this will be for you. I do think we should be able without too much extra work to use something like Ix to generate a minimal import file for a particular constant and provide a stable hash across versions that do not change the kernel, which is equal for all definitionally equal constants</p>",
        "id": 539912295,
        "sender_full_name": "John Burnham",
        "timestamp": 1758069977
    },
    {
        "content": "<p>From a programming languages perspective, our approach is essentially an adaptation of the Unison content-addressing model to Lean (<a href=\"https://www.unison-lang.org/docs/the-big-idea/\">https://www.unison-lang.org/docs/the-big-idea/</a>) and when Ix is more mature/stable should enable a lot of the same depedendency management and even distrubuted computing applications that Unison does.</p>\n<p>I do have to caveat though that Ix is not stable right now, and while it does currently work for small programs, all of Prelude and Init, it's not very optimized and a recent Lean version causes the Ix compiler to OOM at least on my machine when you try to compile Lean itself through Ix. But we have some fun toy examples, like a mini zero-knowledge voting app: <a href=\"https://github.com/argumentcomputer/ix/blob/main/Apps/ZKVoting/Prover.lean\">https://github.com/argumentcomputer/ix/blob/main/Apps/ZKVoting/Prover.lean</a>, an integration with the <a href=\"https://www.iroh.computer/\">https://www.iroh.computer/</a> p2p network and a very large amount of cryptographic machinery for our backend zk prover and DSL.</p>\n<p>If anything in here sounds potentially useful to anyone though, I would absolutely love to talk about it!</p>",
        "id": 539912856,
        "sender_full_name": "John Burnham",
        "timestamp": 1758070590
    },
    {
        "content": "<p>Once a stable hashing standard is established, one could imagine having some database of hashed theorems from various repositories bundled into Mathlib, an axiom that allows one to assume any theorem whose hash is in this database, and a tactic (analogous to <code>native_decide</code>) that allows one to close goals by hashing them and trying to apply the axiom.  This could allow one to scale up the mathematical capability of Mathlib without exploding the size of the codebase or overwhelming the approval process for contributions.</p>",
        "id": 539913916,
        "sender_full_name": "Terence Tao",
        "timestamp": 1758071579
    },
    {
        "content": "<blockquote>\n<p>one could imagine having some database of hashed theorems from various repositories bundled into Mathlib, an axiom that allows one to assume any theorem whose hash is in this database, and a tactic (analogous to <code>native_decide</code>) that allows one to close goals by hashing them and trying to apply the axiom</p>\n</blockquote>\n<p>Not a very important point, but this is one of those funny things where there are two ways of looking at it depending on what level of the stack the crypto is placed on. One way of implementing this is to maintain the whole database of theorems in your project and index into it whenever you want to apply one. But if the hash-addressing is tight enough, then if you just construct the conjunction of all the theorems in the database, and the hash of the resulting theorem can be considered as a <a href=\"https://www.mit.edu/~jlrubin/public/pdfs/858report.pdf\">Merklized Abstract Syntax Tree</a>, and the other theorems can be proved from it directly.</p>",
        "id": 540083456,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1758135566
    }
]