[
    {
        "content": "<p>I've always thought that these \"let the AI guess the next tactic\" tools were a bit of a gimmick, and the last time I tried to get one working was back in the Lean 3 days. But I'm giving a lecture tomorrow where I'm showcasing Lean in the context of AI and I think that showing off this functionality would go down well. Does it exist for Lean 4? What is the cheapest way of getting something like this up and running? Is there a chance that I can be using this by tomorrow on my laptop given that I don't have any relevant API keys etc today?</p>",
        "id": 387629533,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1693214240
    },
    {
        "content": "<p><a href=\"https://github.com/wellecks/llmstep\">https://github.com/wellecks/llmstep</a></p>",
        "id": 387643734,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693220237
    },
    {
        "content": "<p>Is the current winner in terms of ease of use.</p>",
        "id": 387643752,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693220244
    },
    {
        "content": "<p>No API keys needed, but a large download (~10gb) before you use it.</p>",
        "id": 387643799,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693220271
    },
    {
        "content": "<p>If you ping <span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span>, <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span> or me if you see us online we can help walk you through installing it.</p>",
        "id": 387643921,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693220328
    },
    {
        "content": "<p>If you want to show off <code>#formalize</code> <a href=\"https://github.com/leanprover-community/mathlib4/pull/3808\">#3808</a>, which often works quite nicely, I can loan you an API key. :-)</p>",
        "id": 387644133,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693220413
    },
    {
        "content": "<p>Ok I will experiment and report back! In my lecture today I spend a lot of time making fun of ChatGPT so it will be interesting to see it in action with lean :-)</p>",
        "id": 387648091,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1693222131
    },
    {
        "content": "<p>I find copilot not bad tbh, it certainly wins for ease of installation and use (if you have an academic email you are very quickly approved)</p>",
        "id": 387661112,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1693226633
    },
    {
        "content": "<p>OK so for <code>llmstep</code> I downloaded the 5GB model fine, but now my issue is (even after a reboot):</p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>buzzard@buster:~/lean-projects/llmstep$<span class=\"w\"> </span>python3<span class=\"w\"> </span>python/server.py\nLoading<span class=\"w\"> </span>model...\nKilled\nbuzzard@buster:~/lean-projects/llmstep$\n</code></pre></div>\n<p>and just before it gets killed it fills up memory:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>Tasks: 282 total,   3 running, 279 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 12.6 us,  3.0 sy,  0.0 ni, 84.2 id,  0.2 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :  15714.0 total,    151.3 free,  12932.2 used,   2630.5 buff/cache\nMiB Swap:   2048.0 total,   1199.6 free,    848.3 used.   2414.6 avail Mem\n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   8241 buzzard   20   0   19.3g  12.3g 192148 R 118.0  80.4   0:11.92 python3\n    129 root      20   0       0      0      0 R   5.7   0.0   0:04.64 kswapd0\n   2090 buzzard   20   0  489688  22636   7524 S   0.3   0.1   0:02.05 Xorg\n   2237 buzzard   20   0 4685192  65732  22608 S   0.3   0.4   0:03.87 gnome-s+\n      1 root      20   0  168804   4916   2112 S   0.0   0.0   0:01.03 systemd\n      2 root      20   0       0      0      0 S   0.0   0.0   0:00.00 kthreadd\n</code></pre></div>\n<p>Is a 16 gig laptop no good for this?</p>",
        "id": 387661242,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1693226678
    },
    {
        "content": "<p>Copilot: thanks! I have copilot up and running, with nothing to pay for 30 days...</p>",
        "id": 387663272,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1693227406
    },
    {
        "content": "<p>Yeah, it probably needs more RAM.</p>",
        "id": 387663477,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693227479
    },
    {
        "content": "<p>Maybe try using Colab to start the server then use environment variables to tell llmstep to connect to it: <a href=\"https://github.com/wellecks/llmstep/issues/6\">https://github.com/wellecks/llmstep/issues/6</a></p>\n<p>Colab might shut down the server if the notebook is idle, so better to do something with the notebook (e.g. editing a cell) every few minutes, or add a cell like this before starting the server and click to play the silent music:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">%%</span><span class=\"n\">html</span>\n<span class=\"bp\">&lt;</span><span class=\"n\">audio</span> <span class=\"n\">src</span><span class=\"bp\">=</span><span class=\"s2\">\"https://oobabooga.github.io/silence.m4a\"</span> <span class=\"n\">controls</span> <span class=\"bp\">/&gt;</span>\n</code></pre></div>\n<p>It looks like this:</p>\n<p><a href=\"/user_uploads/3121/EZq8DGom4m9Zdw6s7n_pYovM/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/EZq8DGom4m9Zdw6s7n_pYovM/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/EZq8DGom4m9Zdw6s7n_pYovM/image.png\"></a></div>",
        "id": 387664470,
        "sender_full_name": "Utensil Song",
        "timestamp": 1693227794
    },
    {
        "content": "<p>Are there smaller version of the model that could run with less RAM?</p>",
        "id": 387703083,
        "sender_full_name": "Miguel Marco",
        "timestamp": 1693238903
    },
    {
        "content": "<p>There's a WIP PR to quantize and load the model which consumes less RAM: <a href=\"https://github.com/zhangir-azerbayev/llmstep/tree/ggml#local-inference-with-llamacpp\">https://github.com/zhangir-azerbayev/llmstep/tree/ggml#local-inference-with-llamacpp</a></p>",
        "id": 387710460,
        "sender_full_name": "Utensil Song",
        "timestamp": 1693240867
    },
    {
        "content": "<p>Note that is quantizing a <em>different</em> model (trained by <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span>), not the original LLMStep model.</p>",
        "id": 387788986,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1693273408
    },
    {
        "content": "<p>Ah, I missed that.</p>\n<p>Unfortunately, llama.cpp (thus the quantized formats GGML/GGUF ) doesn't support pythia series that LLMStep was finetuned on. But <a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\">GPTQ using bitsandbytes</a> or <a href=\"https://github.com/mit-han-lab/llm-awq\">AWQ</a> should still be feasible and relatively simple to implement. But honestly, pythia is a bit too old to enjoy the ecosystem compared to other open LLMs .</p>",
        "id": 387796501,
        "sender_full_name": "Utensil Song",
        "timestamp": 1693277591
    },
    {
        "content": "<p>Has the talk been recorded?</p>",
        "id": 387879035,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1693314507
    },
    {
        "content": "<p>Yes, I've been running copilot and have had a lot of surprising success (especially if you're expecting utter failure, which is the case a lot of the time...). This includes the time it correctly guessed the <em>statement</em> of the next theorem I wanted to prove, and gave a correct proof! Saved me like 20 mins of work...</p>",
        "id": 387888124,
        "sender_full_name": "Alex Kontorovich",
        "timestamp": 1693317519
    },
    {
        "content": "<p>It certainly seems to know more about different parsers than me right now</p>",
        "id": 387890394,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1693318209
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> Too late for your lecture, but please check this out: <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanInfer.3A.20Neural.20Network.20Inference.20in.20Lean.204\">https://leanprover.zulipchat.com/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanInfer.3A.20Neural.20Network.20Inference.20in.20Lean.204</a>!</p>",
        "id": 390051746,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1694280262
    },
    {
        "content": "<p>There will be plenty more lectures :-)</p>",
        "id": 390052460,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1694280878
    }
]