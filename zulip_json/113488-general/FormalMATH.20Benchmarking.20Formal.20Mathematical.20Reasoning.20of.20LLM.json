[
    {
        "content": "<p>Excited to introduce FormalMATH: a large-scale formal math benchmark with 5,560 formally verified Lean 4 statements from Olympiad and UG-level problems. <br>\nBest model performance: just 16.46% — plenty of room for progress!<br>\nWe open-sourced the complete datasets with the one-click evaluation code.</p>\n<p>Github: <a href=\"https://github.com/Sphere-AI-Lab/FormalMATH-Bench\">https://github.com/Sphere-AI-Lab/FormalMATH-Bench</a><br>\nHuggingface: <a href=\"https://huggingface.co/datasets/SphereLab/FormalMATH-All\">https://huggingface.co/datasets/SphereLab/FormalMATH-All</a><br>\nWebsite: <a href=\"https://spherelab.ai/FormalMATH/\">https://spherelab.ai/FormalMATH/</a><br>\nLeaderBoard: <a href=\"https://spherelab.ai/FormalMATH/#toggleButton\">https://spherelab.ai/FormalMATH/#toggleButton</a></p>",
        "id": 517033211,
        "sender_full_name": "Zhouliang Yu",
        "timestamp": 1746768998
    },
    {
        "content": "<p>I worry about having too many benchmarks which are all the same.  What do you see this benchmark adding over say PutnamBench?</p>",
        "id": 517082336,
        "sender_full_name": "Jason Rute",
        "timestamp": 1746787721
    },
    {
        "content": "<p>The most interesting part of this benchmark is that it is IIUC automatically generated with a human in the loop.  But that probably means a lot of mistakes.  See the recent discussions on this Zulip about the CombiBench benchmark and the ProverBench benchmark for all the different ways that one can easily make mistakes with such benchmarks.  The first mistake I found in your is in problem <code>quantitative_reasoning_zh_blue_41</code>.  I think that is the forth problem I looked at.  You made a common mistake forgetting that subtraction of natural numbers is truncated subtraction.</p>",
        "id": 517082936,
        "sender_full_name": "Jason Rute",
        "timestamp": 1746787988
    },
    {
        "content": "<p>The problem is  false for x = 2.</p>",
        "id": 517083136,
        "sender_full_name": "Jason Rute",
        "timestamp": 1746788065
    },
    {
        "content": "<p>Also 5k problems is huge for a benchmark of this type.  Many training datasets aren’t this big.  Do you really intend this for benchmarking?</p>",
        "id": 517084762,
        "sender_full_name": "Jason Rute",
        "timestamp": 1746788747
    },
    {
        "content": "<p>Which version of Lean/Mathlib is this dataset for? Do you plan to update it for future versions?</p>",
        "id": 517085050,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1746788841
    },
    {
        "content": "<p>Could you give more details of the human verification stage? How much Lean experience did the \"12 International Mathematical Olympiad medalist-level human experts\" have? What instructions were they given for verification - for example, what common errors (such as natural number subtraction) were in their checklist of things to look for, if any? How many people checked each problem statement? How many errors were found? For errors in problems checked by more than one person, what are the statistics of how many people found each error out of all the checkers of that problem (allowing appropriately for cases where there were multiple errors in the same statement and different checkers found different subsets of them)? Based on such data, do you have any statistical estimate of the number of statements with undetected errors similar to the errors that did get detected?</p>",
        "id": 517183889,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1746822860
    }
]