[
    {
        "content": "<p>Hi friends, I wonder what is the most cost-effective way of using Lean? More specifically,</p>\n<ol>\n<li>Should I use AMD or Intel CPUs? (Some say AMD is more cost-effective, while some software are less optimized on it)</li>\n<li>How much memory should I have? (e.g. CPU:memory = 1:2, 1:4, or 1:8?)</li>\n<li>Any other suggestions about hardware?</li>\n<li>Is hyper-threading useful or useless for Lean? (e.g. a CPU with hyperthreading will say it is \"8 core 16 threads\", then will Lean work well on the doubled threads?)</li>\n</ol>\n<p>Some more context if you are interested: I am reproducing <a href=\"https://arxiv.org/abs/2205.11491\">a paper</a> that uses Lean to do theorem proving, thus uses Lean extensively by calling it really a lot lot of times. The original paper even uses 10000 cpu cores.</p>\n<p>Thanks for any ideas!</p>",
        "id": 326078809,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675676655
    },
    {
        "content": "<p>I don’t think this answers your question, but if you are not aware, there is a stream <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a> which discusses papers like (and including) this one.  (I think you question however is in the correct stream.)</p>",
        "id": 326109762,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675686308
    },
    {
        "content": "<p>I assume this is Lean 3, like in that paper?</p>",
        "id": 326109961,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675686378
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Hi thanks for the reply! I am in that stream (and has read many of your posts and has discussed with you indeed ;) ). Yes it is Lean 3 (though maybe after a few years we may use Lean 4?)</p>",
        "id": 326110897,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675686663
    },
    {
        "content": "<p>One idea is to reach out to the authors and ask them about some of these details.  I don’t know how much they optimized everything, but they have been pretty open about details of their paper.  I also imagine if you want to especially optimize this a lot of experimentation is needed.  Only Meta and OpenAI have done large scale reinforcement learning on Lean.  I think OpenAI just threw compute at the problem.  I know Meta did a bit more optimization such as writing some of the key parts in C++.</p>",
        "id": 326111272,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675686788
    },
    {
        "content": "<p>As for memory, Lean needs a lot of memory to compile mathlib.  But I don’t think you will be doing anything like that in your use case since it will likely be already compiled.  But you probably will be wanting to load all of mathlib, so you should see how much memory that uses.</p>",
        "id": 326112063,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675687037
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  Thanks for the insights!</p>\n<blockquote>\n<p>One idea is to reach out to the authors and ask them about some of these details</p>\n</blockquote>\n<p>I have background in open source (as can be seen in my <a href=\"https://github.com/fzyzcjy\">github profile</a>), so I originally expect people discuss details publicly (e.g. on GitHub). Since they do not release code (or more details), do you suggest me to send an email, or is there any other standard approaches to reach out that I (as a newbie) do not know?</p>\n<blockquote>\n<p>But you probably will be wanting to load all of mathlib, so you should see how much memory that uses.</p>\n</blockquote>\n<p>Totally agree I should not need to compile mathlib. So how do I measure \"load all of mathlib\"? I am using <a href=\"https://github.com/openai/lean-gym\">https://github.com/openai/lean-gym</a> with <code>minif2f</code> and <code>mathlib</code> (with your helpful <a href=\"https://github.com/jasonrute/lean_proof_recording\">https://github.com/jasonrute/lean_proof_recording</a> as training data btw). The question is, currently I just start the lean-gym command line and use it. Is it the memory that it loads <em>all</em> of mathlib, or shall I somehow trigger something before lean-gym loads the whole mathlib?</p>",
        "id": 326113388,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675687427
    },
    {
        "content": "<p>Loading all of mathlib in RAM requires ~16 GB to run comfy.</p>",
        "id": 326113641,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1675687503
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> Thank you for the numbers! So do you know how many CPU cores does one serial and full-speed-running Lean usually uses? (I have used Lean by myself, but not sure whether it is representative since I am still learning Lean and use it only for very simple examples)</p>\n<p>Background: The <a href=\"https://github.com/openai/lean-gym\">https://github.com/openai/lean-gym</a> I am using is a Lean package, which accepts inputs like \"execute <code>apply add_comm</code>\" by a machine and apply such tactics to goals. It is <em>serial</em>, i.e. can only process one single tactic at a time.</p>",
        "id": 326114365,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675687732
    },
    {
        "content": "<p>I run with 10 cpus, and I can easily max them out</p>",
        "id": 326115256,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1675688000
    },
    {
        "content": "<p>I guess you can probably run with up to 40 cpus and use all of them a large amount of the time, when compiling mathlib</p>",
        "id": 326115358,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1675688035
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> Thank you!<br>\nI will not compile mathlib indeed, but will run proof searches on <a href=\"https://github.com/facebookresearch/miniF2F\">https://github.com/facebookresearch/miniF2F</a>, such as:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">theorem</span> <span class=\"n\">imo_1974_p3</span>\n  <span class=\"o\">(</span><span class=\"n\">n</span> <span class=\"o\">:</span> <span class=\"n\">ℕ</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"bp\">¬</span> <span class=\"mi\">5</span><span class=\"bp\">∣∑</span> <span class=\"n\">k</span> <span class=\"k\">in</span> <span class=\"n\">finset.range</span> <span class=\"o\">(</span><span class=\"n\">n</span> <span class=\"bp\">+</span> <span class=\"mi\">1</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"n\">nat.choose</span> <span class=\"o\">(</span><span class=\"mi\">2</span> <span class=\"bp\">*</span> <span class=\"n\">n</span> <span class=\"bp\">+</span> <span class=\"mi\">1</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"mi\">2</span> <span class=\"bp\">*</span> <span class=\"n\">k</span> <span class=\"bp\">+</span> <span class=\"mi\">1</span><span class=\"o\">))</span> <span class=\"bp\">*</span> <span class=\"o\">(</span><span class=\"mi\">2</span><span class=\"bp\">^</span><span class=\"o\">(</span><span class=\"mi\">3</span> <span class=\"bp\">*</span> <span class=\"n\">k</span><span class=\"o\">))</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"gr\">sorry</span>\n<span class=\"kd\">end</span>\n</code></pre></div>",
        "id": 326115560,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675688107
    },
    {
        "content": "<p>But do you want to run many of them at the same time?</p>",
        "id": 326115801,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1675688180
    },
    {
        "content": "<p>I don't know how those Lean-AI packages work. But I imagine they benefit from big GPUs.</p>",
        "id": 326115914,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1675688214
    },
    {
        "content": "<p>Yes and no. The <code>lean-gym</code> currently does not support parallelism at all, so I have to wait for one tactic to finish before trying the second tactic. But if it is a big problem, I may try to hack that code.</p>\n<p>Totally agree, the most expensive part is using a ton of big GPUs to run the AI. But at the same time the paper uses Lean - so I have to know CPU/memory usage of Lean as well.</p>",
        "id": 326116239,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675688305
    },
    {
        "content": "<p>To load all of mathlib, search on Zulip on how to make an <code>all.lean</code>file.  There is a script in mathlib that will do it for you.</p>",
        "id": 326116370,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675688345
    },
    {
        "content": "<p>I think running LeanGym on all theorems using one call of simp would give a decent baseline benchmark also.  Or maybe try other variations closer to your workflow.</p>",
        "id": 326116676,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675688428
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Thanks! The lean-gym <a href=\"http://setup.sh\">setup.sh</a> already makes an <code>all.lean</code> including all <code>import</code>s.</p>\n<blockquote>\n<p>I think running LeanGym on all theorems using one call of simp would give a decent baseline benchmark also.</p>\n</blockquote>\n<p>I see. will try that.</p>",
        "id": 326116844,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675688471
    },
    {
        "content": "<p>As for contacting the authors, I’d shoot them an email or direct message.</p>",
        "id": 326116957,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675688504
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Thanks! So about \"direct message\", do you mean Zulip or Twitter or somewhere else?</p>",
        "id": 326117140,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675688544
    },
    {
        "content": "<p>most of them are on zulip</p>",
        "id": 326117353,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1675688609
    },
    {
        "content": "<p>Wherever you can find them, but they are on Zulip, yes.</p>",
        "id": 326117453,
        "sender_full_name": "Jason Rute",
        "timestamp": 1675688632
    },
    {
        "content": "<p>Get it. Thank you guys for the kind replies!</p>",
        "id": 326117562,
        "sender_full_name": "fzyzcjy",
        "timestamp": 1675688653
    }
]