[
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/lots.20of.20theorems/near/378298875\">said</a>:</p>\n<blockquote>\n<p>FWIW I consider this a strong negative aspect of lean in general: it is not suitable for doing anything \"archival quality\" because in a year you will have difficulty getting it to compile. Mathematics is supposed to \"stay proved\", and lean overhauling itself on a regular basis does not help in this regard. Not all theorem provers have this issue, although the approaches used to deal with it vary.</p>\n</blockquote>\n<p>Is the problem here really lean overhauling itself or is it mathlib overhauling itself?</p>",
        "id": 385870175,
        "sender_full_name": "Maarten Derickx",
        "timestamp": 1692351127
    },
    {
        "content": "<p>It's a pretty severe problem / feature on both sides. :-) Mathlib is worse, and unlikely to get better. Regular official releases of Lean 4 itself should start soon.</p>",
        "id": 385870404,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692351223
    },
    {
        "content": "<p>P.s. I realized that Joseph Myers post right before that of Mario Carneiro is actually a better context for the discussion I want to start. He raises some excellent challenges. And I am just curious what kind of things can be done to adres those challenges.<br>\n<span class=\"user-mention silent\" data-user-id=\"266253\">Joseph Myers</span> <a href=\"#narrow/stream/113488-general/topic/lots.20of.20theorems/near/378297465\">said</a>:</p>\n<blockquote>\n<p>I think having a new list of formalization targets is worthwhile, but ideally it would look somewhat different to the old list, reflecting the different challenges for formalization today. The challenge now isn't so much \"can we formalize X?\" for various X - by now it's clear that just about any individual result X can be formalized on its own with enough effort. And the old list effectively provided lots of examples of such individual results.</p>\n<p>Rather, I think challenges now are more about issues such as scaling, breadth, integration, maintenance. Can mathlib scale to 100 times its present size, with a community 100 times its present size and commits going in at 100 times the present rate? Can it reach the point where it has not merely most of an undergraduate degree but many of the key results used by people working in all fields of mathematics? Can all the results be formalized not just on their own, but with all the underlying mathematics being properly integrated in mathlib? Will the proofs be maintained afterwards (arguably several of the entries on Freek's original list should now have some kind of red flag in the list of Lean proofs, that they were only done in Lean 3 in an external repository and so haven't been done in Lean 4 / with current mathlib4 - and I'd guess much the same applies for some of the other theorem provers that have been used to formalize entries on that list)?</p>\n<p>Considering those challenges suggests that maybe some of the following would make sense for a new list of formalization targets. (a) It should have a lot more than 100 theorems, to provide broad coverage of different areas of mathematics. (b) It should routinely get more theorems added (whenever new results come to prominence), rather than being a fixed and frozen list (but once an entry is there, it should remain there). (c) While \"interesting oddities\" as described in a comment on <a href=\"https://github.com/leanprover-community/mathlib4/pull/6091\">#6091</a> are worth including, maybe also get people working in (or familiar with) many different fields of mathematics to suggest lists of major theorems in their fields for inclusion. (You might get the \"interesting oddities\" by counting \"mathematics that attracts popular interest or coverage\" as being a field there.) (d) There should be an associated convention that a theorem on the list isn't considered done until all the underlying mathematics is fully integrated in the relevant libraries for the theorem prover being used, with the result itself being somewhere that gets maintained on an ongoing basis independent of the original authors.</p>\n</blockquote>",
        "id": 385872484,
        "sender_full_name": "Maarten Derickx",
        "timestamp": 1692351963
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"637200\">Maarten Derickx</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/385870175\">said</a>:</p>\n<blockquote>\n<p>Is the problem here really lean overhauling itself or is it mathlib overhauling itself?</p>\n</blockquote>\n<p>To a certain extend 'mathlib overhauling itself' probably also reflects mathematics in general overhauling itself, just there it's less obvious. E.g. I've just been working on some lecture notes and they are full of comments about incompatible notions coming different papers over time; or some authors giving a noun/adjective completely different meaning than others do.</p>",
        "id": 385874880,
        "sender_full_name": "Jon Eugster",
        "timestamp": 1692352830
    },
    {
        "content": "<p>For example I myself have no idea on how to actually write mainainable proofs in Lean. I know thechniques of how to do it in other programming languages. For example:<br>\nOne important principle in software engineering is isolation. When you are designing a system with multiple components, it is usually a good idea to isolate them from each other so that a change in one component doesnâ€™t have undesired effects on other components.<br>\nIn a lot of programming languages.<br>\nI know that lean has the private keyword to help facilitate this. However it is not explained properly in theorem proving in lean 4, and is also rarely used in mathlib (my grepping skills seem to indicate that less then 1% of all theorems and definitions in mathlib are private). So basically everything in our api is considdered fair play to depend on, which makes it really hard to do refactors.</p>",
        "id": 385874983,
        "sender_full_name": "Maarten Derickx",
        "timestamp": 1692352866
    },
    {
        "content": "<p>I think you might be mistaken about refactors here. Yes, everything is public. On the other hand, the fact that everything is so thoroughly spec'd (i.e. by having theorems about the definitions), and Lean's type theory is able to pinpoint exactly where things fail, means that it is actually possible to do refactors.</p>\n<p>The first time you make a change deep in mathlib and only have to wait a few minutes before Lean tells you about a consequence way off in a different subject, is kind of amazing.</p>",
        "id": 385882136,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692354821
    },
    {
        "content": "<p>It's easy enough to refactor things within mathlib, but that does have a negative impact on code outside mathlib (which is one of the reasons we're in favor of putting as much as possible in mathlib)</p>",
        "id": 385886142,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1692356172
    },
    {
        "content": "<p>I'm just a little bit worried that this is something that will come back and make it more difficult for us as mathlib grows and scales. If there is no thought about isolation, at some point certain parts wil become defacto unchangable just because changing them will cause so much breakage higher up. And I think getting everything into mathlib as a slotion to avoid causing problems to things outside mathlib is also doesn't feel like a solution that scales, and will only work  as long as mathlib is small enough. Especially since code in PR's also count as code outside mathlib. So that if more people start contributing, then one also will notice that more pr's will go stale and will need updating before they can be merged.</p>",
        "id": 385900183,
        "sender_full_name": "Maarten Derickx",
        "timestamp": 1692360988
    },
    {
        "content": "<p>Anyway maybe time will tell if these kind of things will actually be problems or are just hypothetical. What I am more interested in is for myself to know how to write maintainable code in lean. I tried to find some resources on this, but couldn't really find any. Does someone have any good pointers for this?</p>",
        "id": 385900418,
        "sender_full_name": "Maarten Derickx",
        "timestamp": 1692361083
    },
    {
        "content": "<blockquote>\n<p>If there is no thought about isolation, at some point certain parts wil become defacto unchangable just because changing them will cause so much breakage higher up</p>\n</blockquote>\n<p>To some extent this is already the case; but these parts are typically refactors of the interfaces between \"isolated\" components. The one that comes to mind is when I tried to add another data field (<code>op_smul</code>) to <a href=\"https://leanprover-community.github.io/mathlib4_docs/find/?pattern=Algebra#doc\">docs#Algebra</a>, which meant that every single downstream instance now needed to expose a larger interface than before.</p>",
        "id": 385900626,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692361172
    },
    {
        "content": "<p>My two cents on \"archival quality\" and \"theorems should stay proved\":</p>\n<ul>\n<li>I think there is a great value in having \"all da mafs\" in one library. Yes, it comes at a cost. Maintenance load is typically superlinear w.r.t. the size of the library. However, I am not trying to justify \"let it grow organically\". There are certainly some strategic decisions that can significantly influence the overhead. I am not convinced, however, that isolation is a principle that should be broadly applied to mathlib.</li>\n<li>I think that pen-and-paper proofs are less likely to stand the test of time than Lean proofs. For a Lean proof, even if it depends on a very old version of mathlib, it can be always unambiguously resolved what definitions and lemmas it depends on. Yes, it might be sometimes hard to make it compatible with the present version of mathlib, especially if it was abandoned in an \"private archive\" (any repo that <code>leanprover-community</code> does not maintain), but it does \"stay proved\". In contrast, pen-and-paper proofs have a lot of \"implicit dependencies\". As <span class=\"user-mention\" data-user-id=\"385895\">@Jon Eugster</span> said, many nouns and adjectives mean different things to different people. And even if there was a consensus about their meaning 10 years ago, the default meaning might be different for researchers now. Zeitgeist also influences which informal proof techniques are accepted by the community. Leaving out certain steps might be OK in one epoch but frowned upon later. When working in Lean, we may get more automation over time, but it doesn't mean that some old proofs that don't use the automation become inferior. I believe that Lean proofs are immortal.</li>\n</ul>",
        "id": 385911381,
        "sender_full_name": "Martin DvoÅ™Ã¡k",
        "timestamp": 1692364807
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"637200\">Maarten Derickx</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/385900418\">said</a>:</p>\n<blockquote>\n<p>Anyway maybe time will tell if these kind of things will actually be problems or are just hypothetical. What I am more interested in is for myself to know how to write maintainable code in lean. I tried to find some resources on this, but couldn't really find any. Does someone have any good pointers for this?</p>\n</blockquote>\n<p>I think these things will become problems eventually, we shall see if we can come up with technical or organisational solutions in time, I'm quite hopeful though, we've already overcome several hurdles .<br>\nAs for writing maintainable proofs, part of it is helping people fix the proof later if it breaks, i.e. using <code>have</code> and <code>suffices</code> statements and focussing <code>\\.</code>'s to make the proof structure clear and simple to fix. Also using tactics like <code>ring</code> and <code>linarith</code> that don't care precisely what the input is can be more robust compared of individual rewites / very manual proofs. On the flip side not using tactics whose output can easily change (i.e. unbounded <code>simp</code>s) in the middle of a proof (use <code>simp only [blah]</code> instead) is best practice that we try to follow.</p>",
        "id": 385911411,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1692364816
    },
    {
        "content": "<p>My experience is much more limited than others here, but my little experience mainly during the port: extreme golfing of proofs seems to make them more fragile. If there is a chain of steps one can see more easily where it breaks and why.</p>",
        "id": 385923334,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1692367405
    },
    {
        "content": "<p>Note though that the same is true in a different way for breakages to lemma that used to be proved <code>by simp</code>; in both the super-golfed case and the super-automated case, you can't tell what the proof was supposed to do without further inspection of the working version.</p>",
        "id": 385930161,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692369145
    },
    {
        "content": "<p>Eric, I think this is true, but only to some extent. During the port, I think our <code>simp</code> debugging was enormously hindered by <code>simp</code>'s (recently fixed!) silent failure mode.</p>",
        "id": 385941313,
        "sender_full_name": "Jireh Loreaux",
        "timestamp": 1692372177
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"637200\">Maarten Derickx</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/385900183\">said</a>:</p>\n<blockquote>\n<p>Especially since code in PR's also count as code outside mathlib. So that if more people start contributing, then one also will notice that more pr's will go stale and will need updating before they can be merged.</p>\n</blockquote>\n<p>I think good refactoring tools will help a lot here. (Google apparently has such tools to help maintaining their enormous monorepo.) Say mathlib is 100 times its present size, developing at 100 times its present speed, and someone changes the definition of a group. (a) It would be very helpful if 99% of the consequent changes in mathlib are done by an automated refactoring tool. (b) The instructions given to that tool should be preserved, and associated with the commit to mathlib master that does the refactor, rather than lost as a transient part of developing that commit. (c) There should then be automation that makes it easy, when updating a PR to a new mathlib version, to apply (to the new/changed code in the PR only) all the automated refactoring instructions for commits newly merged into the PR. (d) Likewise, there should be automation that makes it easy to apply such automated refactoring changes to another project depending on mathlib, when updating its mathlib version.</p>\n<p>(That's just one guess as to how such scalability issues might be addressed - but it seems plausible, given various reports on how Google handles a huge monorepo.)</p>",
        "id": 385997036,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1692397807
    },
    {
        "content": "<p>I really hope that we can move towards golfing proofs much less.</p>",
        "id": 386021547,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692408568
    },
    {
        "content": "<p>I once found this tool for Java: <a href=\"https://spoon.gforge.inria.fr/\">https://spoon.gforge.inria.fr/</a> . Reading the discussion here made me think about this tool. It might be useful to have a similar approach here. I think it would be very powerful to reason about Lean code in Lean. This would allow you to do complicated rewrites of the entire codebase.</p>",
        "id": 386046084,
        "sender_full_name": "Wouter Smeenk",
        "timestamp": 1692421760
    },
    {
        "content": "<p>A lot of the time, golfing is a good thing. When PR review turns a 50-line proof into a 5-line proof, with three new lemmas with two-line proofs being split out from the original proof, the result is probably faster to build <em>and</em> more maintainable <em>and</em> more useful to subsequent contributors <em>and</em> helps the original contributor learn about how to improve their future proofs.</p>",
        "id": 386087217,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1692440619
    },
    {
        "content": "<p>I think we're sometimes confusing ourselves by using \"golfing\" to mean both \"reduce to the absolute least number of characters, ignoring readability and maintainability\" (which is arguably the original meaning in this context) and what you said, which is more about splitting out reusable parts of a proof and reusing existing results in the library - I think we approve of the latter but not necessarily of the former</p>",
        "id": 386091129,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1692442378
    },
    {
        "content": "<p>Yes, breaking out lemmas has nothing to do with golfing.</p>",
        "id": 386103035,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692450372
    },
    {
        "content": "<p>I'm more interested in the \"let's turn every <code>apply x; rw [y]; exact z</code> into a <code>x (y â–¸ z)</code>\". The former gives clues about how the author knew how to construct the proof that have vanished in the latter.</p>",
        "id": 386103192,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692450518
    },
    {
        "content": "<p>Interestingly that seems to be the routine for Agda users. In effect the encouraged workflow of Agda lets the user enter a series of keychords (enter <code>x</code>, hit C-c C-r, enter <code>subst y</code>, hit C-c C-r, enter <code>z</code> and C-c C-g), and the resulting thing is <code>x (subst y z)</code> with the construction process vanishing.</p>",
        "id": 386103926,
        "sender_full_name": "Trebor Huang",
        "timestamp": 1692451020
    },
    {
        "content": "<p>I have this gut feeling that automated proofs (even <code>simp</code>s) should be somehow replaced by their expanded form. If not in the code directly, then somewhere else (a mechanic that does not exist in Lean, at least not yet). This would make compilation faster (because searching would no longer be needed) and also more robust (because the proofs become static code)</p>\n<p>For example, if you have</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">theorem</span> <span class=\"n\">foo</span> <span class=\"o\">:</span> <span class=\"n\">goo</span> <span class=\"o\">:=</span> <span class=\"kd\">by</span> <span class=\"n\">simp</span>\n</code></pre></div>\n<p>Then somewhere else, like an automated cache file, you'd have</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">foo</span> <span class=\"o\">:</span> <span class=\"n\">goo</span> <span class=\"o\">:=</span> <span class=\"kd\">by</span> <span class=\"n\">simp</span> <span class=\"n\">only</span> <span class=\"o\">[</span><span class=\"n\">zoo</span><span class=\"o\">]</span>\n</code></pre></div>\n<p>Then Lean would look for the cached proof first. And if something goes wrong with it, retry with the original code (just <code>simp</code>)</p>",
        "id": 386106224,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1692452555
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386103192\">said</a>:</p>\n<blockquote>\n<p>I'm more interested in the \"let's turn every <code>apply x; rw [y]; exact z</code> into a <code>x (y â–¸ z)</code>\". The former gives clues about how the author knew how to construct the proof that have vanished in the latter.</p>\n</blockquote>\n<p>I don't really agree with this (at least for this contrived example of only three lemmas); there's pretty much only one way to end up at <code>x (y â–¸ z)</code>, and it's exactly that sequence of tactics. Furthermore, I think you can see all the same goal states by putting the cursor in the appropriate places in the term.</p>",
        "id": 386106718,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692452933
    },
    {
        "content": "<p>Admittedly <code>rw</code> gives you better error messages than <code>â–¸</code> when something breaks</p>",
        "id": 386106773,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692452998
    },
    {
        "content": "<p>But the places where you'd write <code>x (y â–¸ z)</code> in lean are presumably the places where you'd write \"it follows from<code>x</code>, <code>y</code>, and <code>z</code>\" on paper; the details are boring anyway</p>",
        "id": 386107404,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692453379
    },
    {
        "content": "<blockquote>\n<p>I have this gut feeling that automated proofs (even <code>simp</code>s) should be somehow replaced by their expanded form. If not in the code directly, then somewhere else (a mechanic that does not exist in Lean, at least not yet). This would make compilation faster (because searching would no longer be needed) and also more robust (because the proofs become static code)</p>\n</blockquote>\n<p>This could be very useful, but I think it would only make sense to cache lists of lemmas used by \"searching tactics\" like simp. If we reduce the proof to its term mode and cache that, we'd probably insist on making sure that the term is up to date, and we'd  have to recompile it all the time. Then this feature would basically just be olean files. (Actually I'm not sure oleans store the proof but it would probably be just as expensive as producing them)</p>\n<p>I'm also not sure how we would prevent proofs from going out of sync with the cache. If the cache breaks and the original works, it isn't that big of a deal, but if the original breaks and the cache works, we wouldn't find out until much later. That might not be that big of a deal, but we would still have a broken proof in mathlib. Checking that the original is still in sync with the cached one has to be at least as expensive as compiling it.</p>",
        "id": 386134773,
        "sender_full_name": "Niels Voss",
        "timestamp": 1692469984
    },
    {
        "content": "<p>But this expensive compilation has to be done only once (per mathlib commit), by some powerful CI machine. The rest of the world could just use the cache and benefit from it.</p>",
        "id": 386137084,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1692471847
    },
    {
        "content": "<p>To some extent, these ideas can already be used right now, using the <code>says</code> tactic combinator in mathlib.</p>",
        "id": 386137102,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1692471878
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/lots.20of.20theorems/near/378298875\">said</a>:</p>\n<blockquote>\n<p>it is not suitable for doing anything \"archival quality\" because in a year you will have difficulty getting it to compile.</p>\n</blockquote>\n<p>This criticism feels with misplaced. With git, <code>lean-toolchain</code>, and <code>lake-manifest</code>, all requirements should be pinned indefinitely. So it should be a simple matter of <code>lake build</code> to get something (even very old) to compile in Lean 4. And, if mathlib keeps it cache around indefinitely, a  <code>lake exe cache get; lake build</code> should also work.</p>",
        "id": 386148291,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692481758
    },
    {
        "content": "<p>This even applies to pre-Lake Lean code.  My old leanpkg project <a href=\"https://github.com/tydeu/lean4-papyrus\">Papyrus</a> still builds two years later (on all platforms were LLVM 12 is available -- which is not a limitation of Lean).</p>",
        "id": 386148398,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692481904
    },
    {
        "content": "<p>This is still a far cry from other projects that can compile code 10 or 20 years later. 2 years is a long time in lean years</p>",
        "id": 386149761,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692483191
    },
    {
        "content": "<p>if you want to archive an old lean project you should probably not just pin a version of lean but also vendor it</p>",
        "id": 386149838,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692483280
    },
    {
        "content": "<p>and the fact that Lean requires LLVM and LLVM has a short shelf life is absolutely a design decision that lean made which is anti-archival.</p>",
        "id": 386150059,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692483505
    },
    {
        "content": "<p>I don't want to insist too much on the <code>x (y â–¸ z)</code> example, which obviously is at the very short end because I was too lazy to come up with a good example.</p>\n<p>Nevertheless, I think there are two very different ways to produce that term!</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">apply</span> <span class=\"n\">x</span>\n<span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"n\">y</span><span class=\"o\">]</span>\n<span class=\"n\">exact</span> <span class=\"n\">z</span>\n</code></pre></div>\n<p>and</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"k\">have</span> <span class=\"n\">h</span> <span class=\"o\">:=</span> <span class=\"n\">z</span>\n<span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"n\">y</span><span class=\"o\">]</span> <span class=\"n\">at</span> <span class=\"n\">h</span>\n<span class=\"k\">have</span> <span class=\"o\">:=</span> <span class=\"n\">x</span> <span class=\"n\">h</span>\n<span class=\"n\">assumption</span>\n</code></pre></div>\n<p>Both schemes are reasonable (backwards reasoning vs forwards reasoning), but when golfed they turn into indistinguishable terms that obfuscate whether it was obvious to the human that one needed to works backwards, starting using <code>x</code>, or that one needed to work forwards, starting using <code>z</code>.</p>\n<p>(i.e. it might not be obvious that <code>z</code> is relevant until you start thinking about <code>x</code>, or vice versa, and it is very helpful to reflect this!)</p>",
        "id": 386183391,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692505840
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386150059\">said</a>:</p>\n<blockquote>\n<p>and the fact that Lean requires LLVM and LLVM has a short shelf life is absolutely a design decision that lean made which is anti-archival.</p>\n</blockquote>\n<p>Lean's LLVM is built from the source and thus is long lasting. My example was about bindings to an external LLVM, which was were that shelf problem came in. That is, normal Lean is not tied to LLVM's shelf life. Sorry for any confusion I caused with that mention.</p>",
        "id": 386186062,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692508370
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386149761\">said</a>:</p>\n<blockquote>\n<p>This is still a far cry from other projects that can compile code 10 or 20 years later.</p>\n</blockquote>\n<p>Getting anything to last 20 years in CS requires substantial independent resources to vendor and maintain old technology.  20 years ago virtually all user applications were 32-bit only which modern OSes do not even support (e.g., Windows 11 and Mac post-Catalina). Even 10 years is a big ask. Much technology that old has also well since past end of life (and the threshold for any long-term maintenance). Even code in well-maintained largely platform-independent languages would likely not last 20 years (e.g. the Python 2 to Python 3 breakage). As such, I am curious as to your point of reference here.</p>",
        "id": 386186924,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692509284
    },
    {
        "content": "<p>TeX?</p>",
        "id": 386186976,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1692509385
    },
    {
        "content": "<p>The one on my mind is metamath, which, by virtue of being a spec that has been left largely unchanged and has many independent implementations, has managed to weather a number of technological waves without any required changes to the archive itself - you can use 20 year old tech to check today's <a href=\"http://set.mm\">set.mm</a> and vice versa</p>",
        "id": 386187054,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509441
    },
    {
        "content": "<p>here archival is decidedly <em>not</em> achieved by simply pinning old everything</p>",
        "id": 386187124,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509529
    },
    {
        "content": "<p>but rather by decoupling the archival language from the tech stack</p>",
        "id": 386187134,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509554
    },
    {
        "content": "<p>i.e. having a standard</p>",
        "id": 386187138,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509565
    },
    {
        "content": "<p>plus, if we're comparing with paper mathematics we have to compete with shelf lives on the order of 100 years</p>",
        "id": 386187256,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509717
    },
    {
        "content": "<p>But note that <span class=\"user-mention\" data-user-id=\"417654\">@Martin DvoÅ™Ã¡k</span> put some valid qualifications on the archival quality of paper maths. See <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/385911381\">https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/385911381</a></p>",
        "id": 386187315,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1692509788
    },
    {
        "content": "<p>I think that is quite overblown, one can routinely and reasonably read papers which are 50 years old with only a modest amount of difficulty</p>",
        "id": 386187371,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509877
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110031\">Patrick Massot</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386186976\">said</a>:</p>\n<blockquote>\n<p>TeX?</p>\n</blockquote>\n<p>At least in my experience with TeX, there are a number of old packages that old code is fond of using that do not work properly on more modern systems, so code there still seems to have an expiry date. Furthermore, TeX package management is lacking so it is easy to sometimes get the wrong copy of a package and then everything breaks hard.</p>",
        "id": 386187450,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692509892
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> But there are also a whole bunch of examples that you can no longer read. And it's exactly those that are problematic.</p>",
        "id": 386187461,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1692509921
    },
    {
        "content": "<p>Unless a newer variant was written by someone who went through the trouble of updating the language and notation.</p>",
        "id": 386187500,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1692509942
    },
    {
        "content": "<p>sure, you might need to \"port\" old maths just like with software</p>",
        "id": 386187528,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692509975
    },
    {
        "content": "<p>But it might be easier to port old Lean than to port old paper maths</p>",
        "id": 386187606,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1692510020
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386187256\">said</a>:</p>\n<blockquote>\n<p>plus, if we're comparing with paper mathematics we have to compete with shelf lives on the order of 100 years</p>\n</blockquote>\n<p>I would think that is more like being able to read code versus being able to compile it.</p>",
        "id": 386187608,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510023
    },
    {
        "content": "<p>but the stuff Mac is talking about (where you have slight incompatibilities and need to work a bit to get things working) happens around the 2 year mark for lean, the 5 year mark for general compiler software, 5-20 years for tex development and 20-200 years for paper mathematics</p>",
        "id": 386187638,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510049
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> My example had no incompatibles? The Lean code compiled with no issues. The problem was the goal of the project was to binding LLVM code which had since past end-of-life on MSYS2. So the problem was with my choice of project and Windows MSYS2's shelf life for LLVM. (MSYS2 actually does not actually officially support anything but the most recent version of a package and their unofficial archive only holds a few old versions).</p>",
        "id": 386187816,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510188
    },
    {
        "content": "<p>I'm talking specifically about lean for mathematics</p>",
        "id": 386187855,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510235
    },
    {
        "content": "<p>it's a much more complicated story when using lean for other purposes because then the usefulness of the tool depends on the state of the tech world</p>",
        "id": 386187911,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510274
    },
    {
        "content": "<p>but my original claim that started this was \"mathematics should stay proved\"</p>",
        "id": 386187928,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510302
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> Yes. That was sort of the point I was trying to make. <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span> As long as the goal is just to run old Lean, afaik, all existing Lean 3 / Lean 4 projects can still be compiled today without changes.</p>",
        "id": 386187990,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510361
    },
    {
        "content": "<p>the forward compatibility story is pretty bad though</p>",
        "id": 386188082,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510418
    },
    {
        "content": "<p>(Less sure about Lean 3, because I did not have much experience with package management in it, but at least getting the toolchain still works via elan).</p>",
        "id": 386188087,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510424
    },
    {
        "content": "<p>it's good enough to identify a version</p>",
        "id": 386188129,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510442
    },
    {
        "content": "<p>I don't have any sense that we are converging on a language for lean 4 that will stabilize</p>",
        "id": 386188234,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510499
    },
    {
        "content": "<p>I expect changes to just keep coming for the foreseeable future</p>",
        "id": 386188275,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510520
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> For \"mathematics should stay proved\" isn't the goal just to be able to verify that  the proof was correct (i.e., in Lean, to be able to compile and run the proof through a verifier)? Why would forward compatibility matter in that?</p>",
        "id": 386188348,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510552
    },
    {
        "content": "<p>because you want to use that proof from a masters thesis 4 years ago in your project which uses a lemma from last month's mathlib</p>",
        "id": 386188386,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510585
    },
    {
        "content": "<p>I don't want every nightly to be a new incompatible dialect of a language</p>",
        "id": 386188551,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510659
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> Sure, but at least to my understanding, that would be a separate goal from \"mathematics should stay proved\".</p>",
        "id": 386188563,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510675
    },
    {
        "content": "<p>is it?</p>",
        "id": 386188576,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510682
    },
    {
        "content": "<p>Yes, at least in how I would understand the concept.</p>",
        "id": 386188597,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510698
    },
    {
        "content": "<p>it should still be true and usable, just like old theorems are still usable today</p>",
        "id": 386188628,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510720
    },
    {
        "content": "<p>we aren't reinventing the logic every year and throw away the library of accumulated mathematics</p>",
        "id": 386188727,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510750
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> If that is goal, wouldn't we need interoperability with every other proof system as well? So that a proof in one system is usable for all time as well?</p>",
        "id": 386188777,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510790
    },
    {
        "content": "<p>of course I want that</p>",
        "id": 386188793,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510806
    },
    {
        "content": "<p>In fact, one could argue the need for formalization itself indicates that paper proofs themselves were insufficient for this theorem provers cannot directly import a proof from a text document.</p>",
        "id": 386188893,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510859
    },
    {
        "content": "<p>Similarly, translating a proof from natural language to natural language or from print to digital also violate this goal.</p>",
        "id": 386188947,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692510909
    },
    {
        "content": "<p>I don't disagree with that, formalizations are certainly value added on the original. But to the extent that people prefer to read the paper proof than a formalization when re-formalizing in another system, the formal language is also lacking in exactly the same way</p>",
        "id": 386189114,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692510993
    },
    {
        "content": "<p>the formalization \"should\" be automatically facilitating its translation into other formal (or informal) languages, that's supposed to be the value add of digitizing the mathematics</p>",
        "id": 386189190,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511078
    },
    {
        "content": "<p>if the artifact is intimately tied to details of the system and is not exportable anywhere else because of this, then it is anti-archival, because one day the system will be out of fashion and people will have difficulty upgrading the proof</p>",
        "id": 386189297,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511159
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386189114\">said</a>:</p>\n<blockquote>\n<p>But to the extent that people prefer to read the paper proof than a formalization when re-formalizing in another system, the formal language is also lacking in exactly the same way</p>\n</blockquote>\n<p>This is hard to measure objectively, though. Mathematicians lean proofs initially from papers and natural languages, so there is inherit basis there that affects this result. A bias that could very well be inverted if the opposite occurred (in fact, the differing attitude of CS people towards math may suggest this is likely the case).</p>",
        "id": 386189403,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692511238
    },
    {
        "content": "<p>I'm willing to condition that statement on people who are very comfortable with the formal language</p>",
        "id": 386189453,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511274
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386189190\">said</a>:</p>\n<blockquote>\n<p>the formalization \"should\" be automatically facilitating its translation into other formal (or informal) languages, that's supposed to be the value add of digitizing the mathematics</p>\n</blockquote>\n<p>I personally agree that this is a great feature. However, my understanding from the formal verification community is that goal is generally certainty in correctness, not actually reproducibility itself (except to the extent it assists the former).</p>",
        "id": 386189562,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692511368
    },
    {
        "content": "<p>I've done a fair amount of that kind of work myself, and I will usually split 50/50 between a formalization and the informal version when trying to understand the proof and recreate it</p>",
        "id": 386189566,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511374
    },
    {
        "content": "<p>Which I think seems to accurately measure the degree to which you are CS and a Math person (i.e,, 50/50). <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 386189790,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692511565
    },
    {
        "content": "<p>It's more like, it reflects the degree to which I am not a computer and cannot read text which requires a computer to process, especially if I can't get it to compile and have to compile it in my head (because it's too old and something about the setup is broken now)</p>",
        "id": 386189838,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511637
    },
    {
        "content": "<p>Well I guess I am a computer then. <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span>  I always much prefer reading code (and formal proofs) than paper descriptions.</p>",
        "id": 386190062,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692511686
    },
    {
        "content": "<p>I think you might not have tried to read some lean/coq/hol light proofs then</p>",
        "id": 386190154,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511731
    },
    {
        "content": "<p>it's not about preference, the context required to understand the words just isn't there</p>",
        "id": 386190203,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511756
    },
    {
        "content": "<p>reading mathlib without lean running is a real challenge</p>",
        "id": 386190211,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511794
    },
    {
        "content": "<blockquote>\n<p>I personally agree that this is a great feature. However, my understanding from the formal verification community is that goal is generally certainty in correctness, not actually reproducibility itself (except to the extent it assists the former).</p>\n</blockquote>\n<p>Goals vary. The point I am making is that archival is often not a goal, and it is easy to lose sight of it, but in the long run people will wish we paid more attention to it. In the CS world the \"long run\" isn't even that long, and lean is one of the shortest shelf life languages I have ever worked in</p>",
        "id": 386190308,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692511920
    },
    {
        "content": "<p>In fact, archival quite often pulls in the opposite direction to \"progress\", which is why it is no surprise it is not a priority for a language like lean. But we <em>are</em> giving up some important properties in doing so, including ones that affect the growth of the language into other areas where this is a user requirement</p>",
        "id": 386190469,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692512077
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> On the proof point, I actually kind of agree. I do find Lean/Coq/HOL proofs hard to read because they make heavy use of automation tactics that I have a hard time figuring out what they actually did. But, in my opinion, that is because they are not really formal proofs, but a program to generate a formal proof. I remember Patrick demonstrating a really cool html proof explore that took a Lean theorem converted it into HTML page that contain a formal natural language proof that you could arbitrarily drill down. That seemed to me to be essentially the perfect merger of all three styles.</p>",
        "id": 386190629,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692512191
    },
    {
        "content": "<p>As much as I like that display, it is not really archival either though. I'm not sure you could reconstruct the formal proof from it</p>",
        "id": 386190689,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692512252
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386190308\">said</a>:</p>\n<blockquote>\n<p>lean is one of the shortest shelf life languages I have ever worked in<br>\n[...[<br>\nIn fact, archival quite often pulls in the opposite direction to \"progress\", </p>\n</blockquote>\n<p>I agree and this why I consider it be one of Lean's great features. Most languages take forever to update, which means a lot of really cool ideas are left on the cutting room floor and a lot of time is waste working around deficiencies of the language. With Lean, a lot of this is solved with frequent language updates and this combined with the ability to easily pin a lean toolchain mans that downstream code still does not need to necessarily keep up the pace (mitigating most of the cons such an approach in my view).</p>",
        "id": 386190983,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692512442
    },
    {
        "content": "<blockquote>\n<p>But, in my opinion, that is because they are not really formal proofs, but a program to generate a formal proof</p>\n</blockquote>\n<p>I agree with this 100%, and this is one of the key reasons metamath proofs have such a long shelf life compared to the programs that generated the proofs. Speccing a proof generation language stably is way harder than speccing a proof language</p>",
        "id": 386190986,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692512446
    },
    {
        "content": "<p>Lean is great for people heavily involved in the language process itself like you and me. For people who are only watching on the fringes it's a tornado of activity and not fun to bump against if you want to make and maintain a project</p>",
        "id": 386191084,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692512535
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386190986\">said</a>:</p>\n<blockquote>\n<p>Speccing a proof generation language stably is way harder than speccing a proof language</p>\n</blockquote>\n<p>I agree, which is why I think they should generally be separate things. In Lean 3, the \"proof language\" was the output format. Unfortunately, Lean adds a number of new features to the proof language (e.g., mutual definitions) that make the old format insufficient. Designing a new proof  language that is interoperable with Lean 4 and external checkers (and maybe one that can be shared also by other proof systems outside of Lean) seems like a good idea.</p>",
        "id": 386191305,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692512778
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386191084\">said</a>:</p>\n<blockquote>\n<p>For people who are only watching on the fringes it's a tornado of activity and not fun to bump against if you want to make and maintain a project</p>\n</blockquote>\n<p>Why, exactly? When writing projects as user, my approach has been to pin some version at the start of the project and only update when I wanted some new feature. In that way, it works very much like any other language. The same more or less holds true for dependencies: get the version you can at the start and only update when you want new features, which is pretty much the same approach as I had other languages. Even in languages with semantic versioning, a lot of major projects don't follow it properly (i.e., use <a href=\"https://0ver.org/\">zero versioning</a>), so updating everything can still be a hassle.</p>",
        "id": 386191769,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513162
    },
    {
        "content": "<p>most libraries and projects I do in other languages (and lean too) I try to keep up to date</p>",
        "id": 386191819,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513215
    },
    {
        "content": "<p>because I want them to be used by others</p>",
        "id": 386191826,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513228
    },
    {
        "content": "<p>I might bump them only infrequently or on request, but I don't want to leave a project that is known out of date unless I am really abandoning it</p>",
        "id": 386191892,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513275
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> I try to do so as well for much the same reason, but I have always found that to be potentially challenging in every language I've used. Furthermore, when doing research on patching security holes in old software, I/we found many of the most popular software repositories rarely had maintenance releases, so this seems to be a general problem.</p>",
        "id": 386191946,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513343
    },
    {
        "content": "<p>the difficulty depends heavily on the language's backward compatibility stance. For C projects you basically never have to revisit them, for rust you might want to fix some warnings, for lean it's broken again every month</p>",
        "id": 386192001,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513407
    },
    {
        "content": "<p>this is basically by design in all three cases</p>",
        "id": 386192058,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513468
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> For JavaScript (i.e., web dev), its often also broken every month due to small incompatibilities in the ginormous number of packages a web project generally uses.</p>",
        "id": 386192114,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513502
    },
    {
        "content": "<p>if you write vanilla then the breakage is essentially zero</p>",
        "id": 386192128,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513531
    },
    {
        "content": "<p>but sure, if you are chasing the trends then god help you</p>",
        "id": 386192144,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513552
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> lol, vanilla JavaScript is not a thing at scale.</p>",
        "id": 386192163,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513584
    },
    {
        "content": "<p>I know, I've tried and it just made everything harder.</p>",
        "id": 386192215,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513611
    },
    {
        "content": "<p>(and eventually I had to switch to a framework anyway.)</p>",
        "id": 386192225,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513626
    },
    {
        "content": "<p>I believe you. But you know what the archival choice is? Vanilla.</p>",
        "id": 386192243,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513657
    },
    {
        "content": "<p>(which was fun, because then the code became a mix of framework code and vanilla because we did not have the time to rewrite everything which made everything even more unwieldly to work with.)</p>",
        "id": 386192250,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513667
    },
    {
        "content": "<p>well, actually it's not so bad with the frameworks since at least in the web dev world you are vendoring everything anyway</p>",
        "id": 386192298,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513724
    },
    {
        "content": "<p>but your build tools might rot</p>",
        "id": 386192310,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513735
    },
    {
        "content": "<p>what do you mean by \"vendoring\" in this context?</p>",
        "id": 386192324,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513756
    },
    {
        "content": "<p>That term is heavily overloaded in web dev</p>",
        "id": 386192335,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513774
    },
    {
        "content": "<p>e.g. shipping a specific version of jquery along with your jquery app</p>",
        "id": 386192340,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513778
    },
    {
        "content": "<p>you have to do this in every javascript app these days, you can't just assume the target has react, it's a security issue</p>",
        "id": 386192364,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513814
    },
    {
        "content": "<p>so you bundle react along with your program</p>",
        "id": 386192383,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513836
    },
    {
        "content": "<p>which means that 20 years later it will still work as long as the browser still understands the JS</p>",
        "id": 386192460,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513890
    },
    {
        "content": "<p>In that case, it is not much different from Lean. You downloaded packages, run a build, and the final product is bundle of everything. The main difference with Lean, of course, is that bundle is currently a binary blob (which is platform-dependent in the executable case).</p>",
        "id": 386192461,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513891
    },
    {
        "content": "<p>we don't typically actually ship lean executables with our projects</p>",
        "id": 386192504,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513937
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386192460\">said</a>:</p>\n<blockquote>\n<p>which means that 20 years later it will still work as long as the browser still understands the JS</p>\n</blockquote>\n<p>Note that JS has switched much closer to a Lean model in its revisions.</p>",
        "id": 386192509,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513947
    },
    {
        "content": "<p>not sure what you mean, JS has backward compatibility AFAIK</p>",
        "id": 386192560,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692513965
    },
    {
        "content": "<p>i.e. it is quite easy for old JS to break on newer browser versions.</p>",
        "id": 386192562,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513968
    },
    {
        "content": "<p>Absolutely not, browsers remove/disable old features of JS all the time.</p>",
        "id": 386192575,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692513991
    },
    {
        "content": "<p>(often for security and/or privacy reasons)</p>",
        "id": 386192587,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514010
    },
    {
        "content": "<p>that is still covered as backward compatibility though</p>",
        "id": 386192645,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514055
    },
    {
        "content": "<p>what do you mean by that?</p>",
        "id": 386192656,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514071
    },
    {
        "content": "<p>the features aren't removed such that errors are generated, they are neutered to avoid the security issues but still exist</p>",
        "id": 386192705,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514089
    },
    {
        "content": "<p>security patches are famous for their tends to be backwards incompatible</p>",
        "id": 386192716,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514096
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> no, many features are flat removed and break tons of sites.</p>",
        "id": 386192730,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514121
    },
    {
        "content": "<p>Think third-party cookies, Adobe Flash, Java Widgets, IFrame changes, Content Security Policies, etc.</p>",
        "id": 386192748,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514146
    },
    {
        "content": "<p>I see. In that case you probably shouldn't be using those kind of features if you want things to work long term</p>",
        "id": 386192875,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514266
    },
    {
        "content": "<p>web 1.0 stuff still works</p>",
        "id": 386192893,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514276
    },
    {
        "content": "<p>Also, pop-ups, event handlers. It is also important to note that while many changes are ostensible for security, a lot of legitimate use cases depended on such features to provide their services.</p>",
        "id": 386192912,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514304
    },
    {
        "content": "<p>the boring stuff always lasts the longest</p>",
        "id": 386192914,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514308
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> IFrames, Flash, and Widgets were abundant in Web 1.0?</p>",
        "id": 386192995,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514390
    },
    {
        "content": "<p>I mean plain HTML</p>",
        "id": 386193016,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514424
    },
    {
        "content": "<p>but even granting your premise, I'm not sure what it is intended to show. All languages struggle with balancing backward compatibility and language evolution</p>",
        "id": 386193025,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514442
    },
    {
        "content": "<p>and ignoring it completely doesn't mean that it isn't still important for a specific set of use cases</p>",
        "id": 386193082,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514476
    },
    {
        "content": "<p>My point was about forward combability. It is a very rare feature that almost never lasts 10-20 years without explicit dedication. I agree it is important, but it generally requires  an additional effort separate from the main progression track.</p>",
        "id": 386193194,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514607
    },
    {
        "content": "<blockquote>\n<p>almost never lasts 10-20 years without explicit dedication</p>\n</blockquote>\n<p>of course! that's also my point, you have to try to make it happen, it won't just happen on its own</p>",
        "id": 386193295,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514697
    },
    {
        "content": "<p>Yes, and its generally accomplished through a project whose primary purpose is to forward port a specific old technology to the new era rather than being an official part of the technology itself (e.g., emulators, remasters, reworks, etc.)</p>",
        "id": 386193449,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692514851
    },
    {
        "content": "<p>I can't think of any case where that actually worked at scale, compared to just incorporating backward compatibility in the language itself</p>",
        "id": 386193576,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692514962
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> Some examples: video game console emulators and the Newgrounds flash player.</p>",
        "id": 386193613,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515020
    },
    {
        "content": "<p>and it's certainly not achieving the goal of keeping theorems interoperable</p>",
        "id": 386193643,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515043
    },
    {
        "content": "<p>In Lean, though, I do hope an archival format will eventually be developed and supported. I agree that it would be a great boon.</p>",
        "id": 386193692,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515065
    },
    {
        "content": "<p>an emulator is just like a low level port of the source system</p>",
        "id": 386193717,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515100
    },
    {
        "content": "<p>Yes?</p>",
        "id": 386193734,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515124
    },
    {
        "content": "<p>i.e. like binport running on lean 3 code</p>",
        "id": 386193748,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515150
    },
    {
        "content": "<p>there are a lot of reasons the result is difficult to use</p>",
        "id": 386193812,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515174
    },
    {
        "content": "<p>Yep, mathport was essentially one of these kinds of projects. Just with more official support than most get.</p>",
        "id": 386193838,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515208
    },
    {
        "content": "<p>binport, not synport. Binport would port the exprs, not the syntax</p>",
        "id": 386193884,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515255
    },
    {
        "content": "<p>I know, but I think both qualify as the kind of project I was describing.</p>",
        "id": 386193911,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515279
    },
    {
        "content": "<p>so you get oleans only, and the notations and attributes wouldn't really work</p>",
        "id": 386193914,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515281
    },
    {
        "content": "<p>Synport would be more like a source to source translator or auto-fix migration assistant</p>",
        "id": 386193996,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515327
    },
    {
        "content": "<p>As far as I can tell there is always going to be a trade-off between an stability and innovation. If you choose the stable route, it will be hard to get new features. If you chose innovation, it will be hard to keep things stable. An archival format focuses on stability, whereas actively developed technology tends to focus on innovation.</p>",
        "id": 386194107,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515422
    },
    {
        "content": "<p>You should read the rust edition RFC, aka \"stability without stagnation\"</p>",
        "id": 386194133,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515451
    },
    {
        "content": "<p>Yes, but that is still a trade-off between man-hours spent on maintaining stability versus man-hours spent on revolutionary improvements.</p>",
        "id": 386194172,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515500
    },
    {
        "content": "<p>Which is why there is always a new popular language every so many years. The old one gets bogged down maintaining stability and the new one gets to exemplify the best modern technology can offer (in some way).</p>",
        "id": 386194274,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515586
    },
    {
        "content": "<p>yep, it's great when you don't have any pesky users to worry about and can focus on making new things</p>",
        "id": 386194369,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515664
    },
    {
        "content": "<p>My hope is that Lean can eventually break this cycle through its powerfully metaprogramming, allowing files to be fixed to a specific stable spec that can be interposable with code running newer versions. However, determining when to start spending man hours maintaining support for an old code base is a hard decision.</p>",
        "id": 386194431,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515724
    },
    {
        "content": "<p>lol no, lean isn't even trying to solve this problem</p>",
        "id": 386194445,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692515744
    },
    {
        "content": "<p>At the moment, no. Lean does make regular breaking changes.</p>",
        "id": 386194460,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515756
    },
    {
        "content": "<p>After all, Lean 4 still has not had a proper official release. <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 386194523,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692515783
    },
    {
        "content": "<p>Once Lean has a stable release schedule, I imagine more guarantees will be made.</p>",
        "id": 386194867,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692516132
    },
    {
        "content": "<p>The first goal will likely be to avoid regressions, but hopefully stuff like Rust editions will eventually be possible.</p>",
        "id": 386194934,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692516219
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"315577\">Mac Malone</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386194867\">said</a>:</p>\n<blockquote>\n<p>Once Lean has a stable release schedule, I imagine more guarantees will be made.</p>\n</blockquote>\n<p>But <a href=\"#narrow/stream/236604-Zulip-meta/topic/Lean.203.20vs.20Lean.204/near/328016487\">when will Lean have a stable release?</a></p>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/scalability.20and.20maintainability/near/386194445\">said</a>:</p>\n<blockquote>\n<p>lol no, lean isn't even trying to solve this problem</p>\n</blockquote>\n<p>Is it even a good idea to use Lean to build a huge library of mathematics in the long run?</p>",
        "id": 386210151,
        "sender_full_name": "Bulhwi Cha",
        "timestamp": 1692528663
    },
    {
        "content": "<p>I think mathlib can weather the changes pretty well, it itself is the cause of more churn than lean 4 core and has enough weight to avoid the worst issues with backward incompatibility (e.g. if a nightly breaks something requiring too much mathlib change, we put the bump on hold and reconsider the lean 4 feature upstream). But it means that it's difficult for single users to build libraries atop mathlib that don't rot quickly</p>",
        "id": 386248067,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692556147
    },
    {
        "content": "<p>What is changing in Lean? Is it just the surrounding front end or is it the core logic too?</p>",
        "id": 386303424,
        "sender_full_name": "Kayla Thomas",
        "timestamp": 1692593275
    },
    {
        "content": "<p>the core logic of lean 4 has basically not changed at all since the beginning of lean 4 (it was changed somewhat from lean 3)</p>",
        "id": 386303636,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692593442
    },
    {
        "content": "<p>but the front end changes routinely</p>",
        "id": 386303648,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692593456
    },
    {
        "content": "<p>Is there a low level spec that the proof tactics get expanded to that remains unchanging?</p>",
        "id": 386303945,
        "sender_full_name": "Kayla Thomas",
        "timestamp": 1692593681
    },
    {
        "content": "<p>yes, that's the lean kernel</p>",
        "id": 386303992,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692593738
    },
    {
        "content": "<p>Could that act as the archive?</p>",
        "id": 386304180,
        "sender_full_name": "Kayla Thomas",
        "timestamp": 1692593869
    },
    {
        "content": "<p>Given a program to translate it back to a standardized set of tactics, if that is theoretically possible.</p>",
        "id": 386305473,
        "sender_full_name": "Kayla Thomas",
        "timestamp": 1692594802
    },
    {
        "content": "<p>Sorry, I'm probably in over my head.</p>",
        "id": 386305854,
        "sender_full_name": "Kayla Thomas",
        "timestamp": 1692595087
    },
    {
        "content": "<p>The kernel is very low level. There wouldn't be much hope of translating back to anything recognisable.</p>",
        "id": 386307549,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692596064
    },
    {
        "content": "<p>Could the tactics expand to some intermediate recognizable standard on their way to being expanded to the kernel?</p>",
        "id": 386307889,
        "sender_full_name": "Kayla Thomas",
        "timestamp": 1692596265
    },
    {
        "content": "<p>in theory yes, in practice tactics don't really target any such intermediate level</p>",
        "id": 386308453,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692596594
    }
]