[
    {
        "content": "<p>I'm thinking about modifying LeanAide to work with other language models such as LLAMA and Deepseek, which lean file actual handles making the API calls?</p>",
        "id": 494068337,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737004423
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"266304\">@Siddhartha Gadgil</span></p>",
        "id": 494069813,
        "sender_full_name": "Jason Rute",
        "timestamp": 1737005333
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"676310\">@Frederick Pu</span> <br>\nIt is the file <code>https://github.com/siddhartha-gadgil/LeanAide/blob/main/LeanCodePrompts/ChatClient.lean</code>. It is already abstracted to use a couple of protocols, including the OpenAI one given a url.</p>\n<p>Ideally I should replace my ad hoc curl calls with a dependency like <a href=\"https://reservoir.lean-lang.org/@cmu-l3/llmlean\">LLMLean</a> in the spirit of centralizing (and then contributing to) common resources (pinging <span class=\"user-mention\" data-user-id=\"243791\">@David Renshaw</span>  ). This is on my to-do list but I of course will welcome PRs doing this.</p>",
        "id": 494070413,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737005706
    },
    {
        "content": "<p>where is the ChatServer type defined?</p>",
        "id": 494180582,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737042987
    },
    {
        "content": "<p>In the same file at <a href=\"https://github.com/siddhartha-gadgil/LeanAide/blob/dfe05dd025e5a132629b95b95674abd1bffe29db/LeanCodePrompts/ChatClient.lean#L64\">line 63</a></p>",
        "id": 494182559,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737043486
    },
    {
        "content": "<p>so which lanugage model is used to specified just by changing the auth header?</p>",
        "id": 494183174,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737043662
    },
    {
        "content": "<p>This is really written only for three cases I used. For others one has to use <code>.generic</code> and specify the url. I have only used this with local models run using vLLM.</p>\n<p>Most likely more constructors will be needed to handle more general cases.</p>",
        "id": 494183739,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737043809
    },
    {
        "content": "<p>how would you pick which one to use? is it using set_option? also if your using gemini would you have to setup an api key or smth?</p>",
        "id": 494187335,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737044760
    },
    {
        "content": "<p>It is passed as a parameter for calls. Yes, there are some <code>set_option</code> parameters but mostly I experimented with using other models with compiled Lean programs.</p>\n<p>For gemini, I used <code>gcloud</code> to authenticate my system. Then just calling a url worked fine.</p>",
        "id": 494188655,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737045145
    },
    {
        "content": "<p>The setup is really ad hoc at present, essentially to allow the 3-4 cases I used.</p>",
        "id": 494188753,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737045176
    },
    {
        "content": "<p>ig having a general call chatbot function instead of changing the auth header would be a good first step</p>",
        "id": 494188846,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737045204
    },
    {
        "content": "<p>that way you could use offline models or embedded cpp models like they do in lean copilote</p>",
        "id": 494188910,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737045223
    },
    {
        "content": "<p>It is convenient to abstract away the client as a term of a <code>ChatServer</code> type. Maybe a good way is to have a more generic constructor than what I call <code>.generic</code> or maybe a few cases.</p>",
        "id": 494189488,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737045388
    },
    {
        "content": "<p>realistically the only two types of chatbots are servers and ffi</p>",
        "id": 494189821,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737045489
    },
    {
        "content": "<p>cause even a locally hosted chat bot would need to be accessed via server if it isnt compiled into cpp</p>",
        "id": 494189864,
        "sender_full_name": "Frederick Pu",
        "timestamp": 1737045506
    },
    {
        "content": "<p>And I believe essentially all servers support the OpenAI API. So instead of <code>generic</code> one can have <code>server</code> and have an additional <code>ffiLocal</code> constructor. Otherwise have say <code>openAIServer</code> and <code>llamaServer</code> for two different protocols.</p>\n<p>I don't know what arguments are needed for ffi. Maybe an enumeration is enough - each ffi takes work and one can add cases as they come.</p>",
        "id": 494190293,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1737045650
    },
    {
        "content": "<p>I've been toying around with this for <a href=\"https://github.com/cmu-l3/llmlean\">LLMLean</a>. Unfortunately it seems like the <code>deepseek-reasoner</code> API is too overloaded and I never get any responses back. <code>deepseek-chat</code> works though and the performance seems to be similar to gpt-4o</p>",
        "id": 496196730,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1738005956
    },
    {
        "content": "<p>In LeanAide this is now implemented but only partially tested. To use with the command line program, use something similar to the following (this is querying <code>o1-mini1</code> but as if it were a generic model). You have to give the correct url as well as model name.</p>\n<p>lake exe translate \"There are infinitely many even numbers.\" --auth_key \"&lt;authentication-key&gt;\" --url \"<a href=\"https://api.openai.com/v1/chat/completions\">https://api.openai.com/v1/chat/completions</a>\" --model \"o1-mini\" --temperature 10 -n 1 --no_sysprompt</p>\n<p>Some of the parameters are because reasoning models (at least from OpenAi) need responses set to 1, temperature to $1$  and do not allow system prompts.</p>\n<p>There are similar settings to use in the interpreter mode. Namely, one uses <code>set_option</code> with <code>leanaide.translate.url?</code>, <code>leanaide.translate.authkey</code> and also <code>leanaide.translate.choices</code> and <code>leanaide.translate.temperature10</code>.</p>\n<p>I hope to have an easier to use server-client setup as well as a fully open-source option (currently the embeddings use OpenAI even if a different model is used). I will post more detailed instructions then.</p>",
        "id": 496249121,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1738033276
    }
]