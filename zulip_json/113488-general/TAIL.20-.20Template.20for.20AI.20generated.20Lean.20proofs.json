[
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"687698\">Chris Henson</span> <a href=\"#narrow/channel/113488-general/topic/Standards.20for.20Lean.20proofs.20of.20unsolved.20problems/near/565570968\">said</a>:</p>\n<blockquote>\n<ul>\n<li>it is valuable to have a canonical, relatively user friendly tool like <code>Comparator</code> to evaluate Lean proofs</li>\n</ul>\n</blockquote>\n<p>I have been working on something in this direction. Initiated by this <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/Discussion.3A.20AI-written.20mathematical.20proofs/near/556956070\">discussion/comment</a> I put together a <a href=\"https://github.com/e-vergo/TAIL/blob/main/docs/standard.md\">template for AI generated lean proofs</a>, or 'TAIL', along with a verification tool that checks a repo against the template. It offers two modes, a 'strict' mode which does not allow the introduction of new definitions, and a more relaxed mode that does. </p>\n<p>High level, the intent is to automate some of the review process as well as minimize the review burden for anyone checking the proof. There are about a dozen or so things I don't like about it right now, and it's still very much a work in progress. The tool is designed so that it can be used locally while -vibe proving to ensure that a major refactor does not need to happen before sharing. So far I have personally used it to vibe-prove a formalization of <a href=\"https://github.com/e-vergo/TDCSG\">an old result of mine</a>, as well as refactor the proof discovered <a href=\"#narrow/channel/219941-Machine-Learning-for-Theorem-Proving/topic/AI-discovered.20proof.20on.20psi-integrals.20.28with.20Lean.20formalizat.2E.29/near/563828163\">here</a>. I have found it quite useful in steering the LLM while it is vibe proving, as its easy for the LLM to make a catastrophic error that needs to be corrected and this tool helps expose them. </p>\n<p>Feedback/comments/suggestions on all levels are welcome.</p>",
        "id": 565574878,
        "sender_full_name": "Eric Vergo",
        "timestamp": 1766934283
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"988234\">@Eric Vergo</span>, I have a lot of reservations about your system.</p>",
        "id": 565592947,
        "sender_full_name": "Jason Rute",
        "timestamp": 1766952130
    },
    {
        "content": "<p>What I like:</p>\n<ul>\n<li>Your template separates out what is being proven from the proof, making it so that the “referee” only needs to check the statement and definitions used in the statement.</li>\n<li>Unlike SafeVerify and Comparator, this doesn’t make the (often unrealistic) assumption that the statement of the theorem came from an independent trusted party.</li>\n<li>It should be compatible with Comparator, and you can use Comparator to check the proof (with a bit of metaprogramming to separate it into a challenge and solution project).</li>\n</ul>",
        "id": 565592951,
        "sender_full_name": "Jason Rute",
        "timestamp": 1766952136
    },
    {
        "content": "<p>What I don’t like:</p>\n<ul>\n<li>Unlike Comparator and SafeVerify, which have very principled designs, I think your checks are pretty ad hoc, leading to both allowing (fake) proofs of <code>False</code> and rejecting perfectly legitimate developments.</li>\n<li>As far as (fake) proofs of <code>False</code>, I haven’t read your code, but I can think of a few ways that I could trick your system into (fake) proving <code>False</code> given your description of the checks.  (I’m not going to say my strategy, because I think even if you ban those keywords, there are ways around them.)  Again, SafeVerify and Comparator work on the term proof level, so they are much more robust.</li>\n<li>As far as rejecting good proofs, the thing I’m most worried about is definitions.  You require definitions to go into the same place, whether they are definitions needed for the theorem statement or the proof.  This is too restrictive, I think.  Many proofs are very definition-heavy.  Consider constructing a counterexample or developing a new mathematical object to prove a theorem.  Or sometimes definitions are just needed for bookkeeping.  There is no need for the “referee” to have to check each and every intermediate definition by hand (or at least no more than the need to check every lemma statement or proof).  [Edit: And some definitions also depend on theorems, like definitions using choice.]</li>\n<li>Less troublesome, some banned keywords, like <code>partial</code>/<code>unsafe</code>, could have legitimate uses, like making new tactics.  Similarly with banning <code>opaque</code>, which can be useful at times (and <code>opaque</code> has no soundness—or fake soundness—issue that I’m aware of, right?).</li>\n</ul>",
        "id": 565592956,
        "sender_full_name": "Jason Rute",
        "timestamp": 1766952142
    },
    {
        "content": "<p>So I would be supportive of something like your directory structure, but with more principled checking of the result (reducing the need to ban keywords or do other brittle checks).</p>",
        "id": 565592957,
        "sender_full_name": "Jason Rute",
        "timestamp": 1766952144
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/Standards.20for.20Lean.20proofs.20of.20unsolved.20problems/near/565592947\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"988234\">Eric Vergo</span>, I have a lot of reservations about your system.</p>\n</blockquote>\n<p>All of the points here are well taken. In the interest of keeping this thread focused I will start a new thread for this tool. (maybe a mod should break it off?)</p>",
        "id": 565681013,
        "sender_full_name": "Eric Vergo",
        "timestamp": 1767019355
    },
    {
        "content": "<p>(I think you can do that yourself, hotkey \"m\" or under the kebab menu)</p>",
        "id": 565687523,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1767021865
    },
    {
        "content": "<p><a href=\"#narrow/channel/113488-general/topic/TAIL.20-.20Template.20for.20AI.20generated.20Lean.20proofs/near/565574878\">A message</a> was moved here from <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/Standards.20for.20Lean.20proofs.20of.20unsolved.20problems/with/565690854\">#general &gt; Standards for Lean proofs of unsolved problems</a> by <span class=\"user-mention silent\" data-user-id=\"988234\">Eric Vergo</span>.</p>",
        "id": 565693151,
        "sender_full_name": "Notification Bot",
        "timestamp": 1767024414
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  responding in-line to your points, but a lot of the details here may be irrelevant given the proposal I make afterwards. Additionally, this is my first real interaction with the lean backend and this effort was partially a forcing-function for me to learn about the details of it.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/TAIL.20-.20Template.20for.20AI.20generated.20Lean.20proofs/near/565592956\">said</a>:</p>\n<blockquote>\n<ul>\n<li>Unlike Comparator and SafeVerify, which have very principled designs, I think your checks are pretty ad hoc, leading to both allowing (fake) proofs of <code>False</code> and rejecting perfectly legitimate developments.<br>\n</li>\n</ul>\n</blockquote>\n<p>They are somewhat ad-hoc and are mostly the result of preventing unwanted behavior I personally witnessed an LLM engage in while vibe-proving. Of course, this should not be what is driving the list of things we need to check; it was just how I got things off the ground.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/TAIL.20-.20Template.20for.20AI.20generated.20Lean.20proofs/near/565592956\">said</a>:</p>\n<blockquote>\n<p>What I don’t like:</p>\n<ul>\n<li>As far as (fake) proofs of <code>False</code>, I haven’t read your code, but I can think of a few ways that I could trick your system into (fake) proving <code>False</code> given your description of the checks.  (I’m not going to say my strategy, because I think even if you ban those keywords, there are ways around them.)  Again, SafeVerify and Comparator work on the term proof level, so they are much more robust.<br>\n</li>\n</ul>\n</blockquote>\n<p>I assume this unease is because the tool checks olean files for keywords, and does not use the elaboration tooling directly. At one point I was checking things on the term level but found it to be much slower. There were also some challenges with the module system and having things marked as axioms, but that may have been due to a misunderstanding on my end.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/TAIL.20-.20Template.20for.20AI.20generated.20Lean.20proofs/near/565592956\">said</a>:</p>\n<blockquote>\n<p>What I don’t like:</p>\n<ul>\n<li>As far as rejecting good proofs, the thing I’m most worried about is definitions.  You require definitions to go into the same place, whether they are definitions needed for the theorem statement or the proof.  This is too restrictive, I think.  Many proofs are very definition-heavy.  Consider constructing a counterexample or developing a new mathematical object to prove a theorem. Or sometimes definitions are just needed for bookkeeping.  There is no need for the “referee” to have to check each and every intermediate definition by hand (or at least no more than the need to check every lemma statement or proof).  [Edit: And some definitions also depend on theorems, like definitions using choice.]<br>\n</li>\n</ul>\n</blockquote>\n<p>Indeed. This is poorly (incorrectly?) explained in the documentation and might not even be implemented correctly. As it stands, definitions with propositional type are allowed outside of the definitions directory. See <a href=\"https://github.com/e-vergo/Balanced_Vectors/blob/f2baf6875c4fe320e340d06a0a2dd7c6f0ed5533/BalancedVectors/Proofs/helper_lemmas.lean#L51\">this example</a>. Maybe the right approach is to drop the definitions directory and update the language/rules to quarantine all declarations of non-propositional types instead. This could be done in a way that partitions them into ones needed for the statement of the main theorem, and ones needed for the proof. This may create complicated import trees as some declarations would be used in both. </p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/TAIL.20-.20Template.20for.20AI.20generated.20Lean.20proofs/near/565592956\">said</a>:</p>\n<blockquote>\n<ul>\n<li>Less troublesome, some banned keywords, like <code>partial</code>/<code>unsafe</code>, could have legitimate uses, like making new tactics.  Similarly with banning <code>opaque</code>, which can be useful at times (and <code>opaque</code> has no soundness—or fake soundness—issue that I’m aware of, right?).<br>\n</li>\n</ul>\n</blockquote>\n<p>My understanding is that ‘unsafe’ marks a declaration from being skipped during the kernel checking and can introduce soundness issues, <a href=\"https://lean-lang.org/doc/reference/latest/Definitions/Modifiers/#declaration-modifiers\">as suggested here</a>. If it does have legitimate uses but may also introduce soundness issues, my opinion is that we should err on the side of caution and not allow it. </p>\n<p>As for opaque definitions, I think you are correct in that it does not create soundness issues; although it is not clear to me as to why when reading the documentation.</p>",
        "id": 565914963,
        "sender_full_name": "Eric Vergo",
        "timestamp": 1767200042
    },
    {
        "content": "<p>My proposal:</p>\n<ul>\n<li>Update the TAIL standard as we see fit (discussion ongoing)</li>\n<li>Re-architect TailVerify to focus on checking for compliance to our standard using the more robust tools</li>\n<li>Drop the custom implemented soundness checks and leverage one or more of Lean4Checker/SafeVerify/comparator/etc. instead.</li>\n</ul>\n<p>There are negative tradeoffs associated with this, mainly maintenance costs in making sure the tools integrate with each other properly. What I do like about it is that once we have this standard set, it is unlikely to change much, while the other tools will likely have to constantly update as lean evolves.</p>",
        "id": 565915078,
        "sender_full_name": "Eric Vergo",
        "timestamp": 1767200111
    }
]