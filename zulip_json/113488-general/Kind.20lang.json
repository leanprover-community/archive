[
    {
        "content": "<p>Kind is a pure functional programming language and proof assistant. (Slightly similar to Lean 4.)<br>\nIt claims to be even faster than Lean 4 due to parallelism.</p>\n<p><a href=\"https://github.com/HigherOrderCO/Kind\">https://github.com/HigherOrderCO/Kind</a></p>\n<p><a href=\"https://github.com/HigherOrderCO/Functional-Benchmarks\">https://github.com/HigherOrderCO/Functional-Benchmarks</a></p>",
        "id": 430905520,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1712110604
    },
    {
        "content": "<p>Can we take this as good news that there is still room for Lean 4 to be faster?</p>",
        "id": 430905669,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1712110701
    },
    {
        "content": "<p>Last commit 7 months ago? Not sure it is an active project.</p>",
        "id": 430906256,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1712111201
    },
    {
        "content": "<p>oh…</p>",
        "id": 430906298,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1712111243
    },
    {
        "content": "<p>It would be interesting to see the Lean code for the slow benchmarks at <a href=\"https://github.com/HigherOrderCO/Functional-Benchmarks\">https://github.com/HigherOrderCO/Functional-Benchmarks</a></p>",
        "id": 430906320,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1712111275
    },
    {
        "content": "<p>what do you mean? they are bad examples?</p>",
        "id": 430906725,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1712111541
    },
    {
        "content": "<p>The cases where they claim Lean is slow --- it would be interesting to see the Lean code they wrote, and to understand if it code be done better, and if not, why it is slow.</p>",
        "id": 430906918,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1712111714
    },
    {
        "content": "<p>But they don't seem to have links, and I didn't want to dig.</p>",
        "id": 430906934,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1712111722
    },
    {
        "content": "<p>They're the lean files in the Checker and Runtime folders. They've also got <code>Base.lean</code>, which seems to be some common code (but it's not imported. <a href=\"https://github.com/HigherOrderCO/Functional-Benchmarks/blob/master/benchmark.js#L205\">Looks to be concatenated</a> instead.)</p>",
        "id": 430907488,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1712112044
    },
    {
        "content": "<p>LeanFRO should say “Lean is perfect and ultimate next-gen functional language!” to gather money..? <span aria-label=\"smirk\" class=\"emoji emoji-1f60f\" role=\"img\" title=\"smirk\">:smirk:</span></p>",
        "id": 430907694,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1712112166
    },
    {
        "content": "<p>(its a joke.)</p>",
        "id": 430907766,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1712112200
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224323\">Junyan Xu</span> has marked this topic as resolved.</p>",
        "id": 430912763,
        "sender_full_name": "Notification Bot",
        "timestamp": 1712115554
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"224323\">Junyan Xu</span> has marked this topic as unresolved.</p>",
        "id": 430912768,
        "sender_full_name": "Notification Bot",
        "timestamp": 1712115558
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/430906918\">said</a>:</p>\n<blockquote>\n<p>The cases where they claim Lean is slow --- it would be interesting to see the Lean code they wrote, and to understand if it code be done better, and if not, why it is slow.</p>\n</blockquote>\n<p>Most of what is slow in their benchmarks are kernel reduction things of large expression trees, the programming parts are all fast.</p>",
        "id": 430932847,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1712128188
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"626349\">Asei Inoue</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/430905669\">said</a>:</p>\n<blockquote>\n<p>Can we take this as good news that there is still room for Lean 4 to be faster?</p>\n</blockquote>\n<p>Also yes there is always room for Lean to be faster. But I don't think it will be in the fashion that Kind and their other toy languages demonstrate</p>",
        "id": 430933214,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1712128349
    },
    {
        "content": "<p>I've been planning a rewrite of the kernel evaluator for some time. Maybe this is a good source to look at for alternative implementations; the Coq kernel is my other major guide on how to do better. The kernel evaluator has a lot of clear room for improvement</p>",
        "id": 430934330,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1712128838
    },
    {
        "content": "<p>This is tied up in progress with lean4lean because I absolutely don't want to accidentally introduce soundness issues rewriting a fundamental soundness-critical part of the system. I'm pretty sure this is the same reason it is relatively unoptimized until now</p>",
        "id": 430934504,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1712128915
    },
    {
        "content": "<p>The secret sauce behind Kind is HVM, which is based on Interaction Nets, or more specifically, Symmetric Interaction Combinators:</p>\n<blockquote>\n<p><strong>Higher-order Virtual Machine (HVM)</strong> is a pure functional runtime that is <strong>lazy</strong>, <strong>non-garbage-collected</strong> and <strong>massively parallel</strong>. It is also <strong>beta-optimal</strong>, meaning that, for higher-order computations, it can, in some cases, be exponentially (in the asymptotical sense) faster than alternatives, including Haskell's GHC.<br>\nThat is possible due to a new model of computation, the <strong>Interaction Net</strong>, which supersedes the <strong>Turing Machine</strong> and the <strong>Lambda Calculus</strong>. Previous implementations of this model have been inefficient in practice, however, a recent breakthrough has drastically improved its efficiency, resulting in the HVM. Despite being relatively new, it already beats mature compilers in some cases, and is being continuously improved.</p>\n</blockquote>\n<p>The first work on Kind lang was based on HVM1, which now has been superseded by HVM2. HVM2 is a combination of two projects: hvm-core and hvm-lang. The current work is focusing heavily on running computations on the GPU reaching <a href=\"https://x.com/VictorTaelin/status/1778838338880831547\">billions of reductions</a> per second. </p>\n<p><a href=\"https://github.com/HigherOrderCO/hvm-core\">hvm-core</a>:</p>\n<blockquote>\n<p>HVM-Core is a parallel evaluator for extended <a href=\"https://www-lipn.univ-paris13.fr/~mazza/papers/CombSem-MSCS.pdf\">Symmetric Interaction Combinators</a>.<br>\nWe provide a raw syntax for specifying nets and a Rust implementation that achieves up to <strong>10 billion rewrites per second</strong> on Apple M3 Max CPU. HVM's optimal evaluation semantics and concurrent model of computation make it a great compile target for high-level languages seeking massive parallelism.</p>\n</blockquote>\n<p><a href=\"https://github.com/HigherOrderCO/hvm-lang\">hvm-lang</a>:</p>\n<blockquote>\n<p>HVM-Lang is a lambda-calculus based language and serves as an Intermediate Representation for HVM-Core, offering a higher level syntax for writing programs based on the <a href=\"https://github.com/VictorTaelin/Interaction-Calculus#interaction-calculus\">Interaction-Calculus</a>.<br>\nCompilers that want to target the HVM should compile the source language to HVM-lang, which takes care of converting interaction calculus into the underlying interaction networks.<br>\nNote that HVM-lang is untyped and does not guarantee correctness or soundness. Compilers that don't want to implement the necessary check can instead transpile to the <a href=\"https://github.com/HigherOrderCO/kind2\">Kind language</a>, which compiles to HVM-lang and implements type-level checking.<br>\nProgrammers looking for an HVM-based programming language should also use Kind, which is designed to be user-interfacing.</p>\n</blockquote>\n<p>This is a quite interesting work especially since Interaction Net (IN) computations are massively parallel. <code>hvm-lang</code> which provides a lambda calculus layer on top of this pure IN runtime is able to reduce lambdas lazily and in parallel.</p>\n<p>Kind is a dependently typed language (as far as I know, based on self types) that focuses on exploiting the massively parallel runtime for elaboration, etc.</p>",
        "id": 433318272,
        "sender_full_name": "Yuri",
        "timestamp": 1713194134
    },
    {
        "content": "<p>Would Kind be good for writing an external proof checker then?</p>",
        "id": 433353511,
        "sender_full_name": "Jason Rute",
        "timestamp": 1713203769
    },
    {
        "content": "<p>There is a note that says it's not complete for lambda calculus</p>",
        "id": 433373192,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1713210433
    },
    {
        "content": "<p><a href=\"https://github.com/HigherOrderCO/HVM/blob/master/guide/HOW.md\">https://github.com/HigherOrderCO/HVM/blob/master/guide/HOW.md</a> is a good overview, although there are other linked papers with string diagrams instead of this rule-based presentation</p>",
        "id": 433375245,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1713211093
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/430934330\">said</a>:</p>\n<blockquote>\n<p>I've been planning a rewrite of the kernel evaluator for some time.</p>\n</blockquote>\n<p>I've been working on this as well, hmu if you want to compare notes.</p>",
        "id": 433390849,
        "sender_full_name": "Chris Bailey",
        "timestamp": 1713216560
    },
    {
        "content": "<p>Not much notes on my end, just collecting candidate implementations like Coq, GHC, HVM</p>",
        "id": 433395191,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1713218460
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"228466\">Chris Bailey</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/433390849\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/430934330\">said</a>:</p>\n<blockquote>\n<p>I've been planning a rewrite of the kernel evaluator for some time.</p>\n</blockquote>\n<p>I've been working on this as well, hmu if you want to compare notes.</p>\n</blockquote>\n<p>Where is the kernel evaluator located code-wise? And are there any docs on the specifics of the kernel?</p>",
        "id": 433528748,
        "sender_full_name": "Enrico Borba",
        "timestamp": 1713275850
    },
    {
        "content": "<p>it's in <code>src/kernel</code> and that depends on how specific you want to get, for the general idea there is <a href=\"https://github.com/digama0/lean-type-theory/releases/tag/v1.0\">#leantt</a> but the precise details are not documented anywhere to my knowledge</p>",
        "id": 433531036,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1713276442
    },
    {
        "content": "<p>There is also the book <span class=\"user-mention\" data-user-id=\"228466\">@Chris Bailey</span> wrote, <a href=\"https://ammkrn.github.io/type_checking_in_lean4/\">https://ammkrn.github.io/type_checking_in_lean4/</a></p>",
        "id": 433550113,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1713281475
    },
    {
        "content": "<p>Hi! I'm the one who tried to create this version of the compiler for the Kind language (the one that is not maintained anymore and that is going to be replaced with a new one :P created by Taelin). HVM1 had a lot of problems that it inherits from the Interaction Nets model and if you take a look at the type checker, it is a simple CoC that does not implement any advanced technique. I think that it's fast because it does not implement anything that is useful for a theorem prover and you can reach unsoundness easily. </p>\n<p>I'm saying this for my version of the compiler, the one that Taelin is writing right now is based on Interaction Type Theory which I don't know much about. I'm almost sure that Taelin is not going to implement a lot of things that modern theorem provers have in to make sure the type checker is fast.</p>\n<p>The Lean4 type checker looks really slow in comparison with other ones, I <strong>think</strong> that Leo said that the type checker uses simple substitution instead of HOAS and HOAS based type checkers usually are faster. When we tried to benchmark the runtime, Lean4 performed really well and we got really impressed.</p>",
        "id": 433856310,
        "sender_full_name": "Sofia Rodrigues",
        "timestamp": 1713380722
    },
    {
        "content": "<p>yes that's right, the type checker is just doing substitution (and more than that, it uses locally nameless so it performs substitution much more often than you might think)</p>",
        "id": 433860289,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1713382150
    },
    {
        "content": "<p>Hi there. Just to clear up some misconceptions:</p>\n<p>Kind is a general-purpose programming language that compiles to HVM, a parallel runtime. It does feature dependent types, which lets you prove theorems. Its type-level evaluator is very fast, because, as pointed, it is a thin CoC-based, NbE checker on top of HVM, thus, it inherits HVM's performance. That said, there is more to fast type-checking than type-level evaluation. Kind has no plans to add advanced optimizations, and its speed is merely due to \"raw horsepower\".</p>\n<p>While Kind features theorem proving, its main goal isn't to be a fully-featured proof assistant. There are no plans to support tactics, cubical types, and so on. It isn't a full replacement to Lean, Coq and similar. That said, she is confused about some assertions. \"Interaction Type Theory\" is a research experiment that is unrelated to it. Instead, Kind2 is based on, and inspired by, András Kovac's smalltt. It features inductive families via λ-encodings, and termination will be ensured via EAL-typeability (which also makes evaluation sound under HVM runtime, removing the \"problems\" that Sofia mentions).</p>\n<p>So, in short - Kind2 can be seen as a general-purpose functional programming language that targets the HVM, and aims to be as fast as possible. It does feature theorem proving and will be consistent, but isn't meant to be a full-featured proof assistant and isn't a substitute to Lean, Coq and similar.</p>\n<p>Hope that clears it up!</p>",
        "id": 435004297,
        "sender_full_name": "Victor Maia",
        "timestamp": 1713884266
    },
    {
        "content": "<p>Also, it isn't maintained because we're moving to Kind2, which is based on HVM2 (a simpler, faster, formally-verified distillation of HVM1!). We had some misdirections on the way, which delayed things a little bit, but it will be there soon. These benchmarks will be updated eventually - but again, they're mostly regarding type-level evaluation, which is a small part of dependent type checking.</p>",
        "id": 435005569,
        "sender_full_name": "Victor Maia",
        "timestamp": 1713884568
    },
    {
        "content": "<p>I think Lean is as fascinating as Kind as a general-purpose programming language.<br>\nWhat do you think are the drawbacks of Lean as a programming language?</p>",
        "id": 435005981,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1713884684
    },
    {
        "content": "<p>I don't think there are many! Lean is amazing and I'm highly impressed by it, specially by things like the interior mutation optimization. Its runtime performance surpassed my former expectations. I do think its type-checker could be faster, but I'm sure it is being worked. I also think Kind is cool on its own ways - for example, being able to run on GPUs is handy nowadays. Hopefully both can coexist!</p>",
        "id": 435007242,
        "sender_full_name": "Victor Maia",
        "timestamp": 1713885052
    },
    {
        "content": "<p>Victor, do you think an inet-based kernel for Lean for the purpose of speed up is a realistic idea?</p>",
        "id": 435082117,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1713914430
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"372571\">@Victor Maia</span> <br>\nThanks for the answer.<br>\nIt is certainly nice to have a GPU available.<br>\nI hadn't thought about whether GPUs are available in lean.</p>",
        "id": 435600105,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1714142477
    },
    {
        "content": "<p>How difficult is it to actually use GPUs in Lean?</p>",
        "id": 435600206,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1714142511
    },
    {
        "content": "<p>There is essentially no support. My guess is that you would need to link to OpenGL or something to use \"compute shaders\"</p>",
        "id": 435606160,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1714144456
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> I want MLIR support someday. Closest I could find was <a href=\"https://github.com/opencompl/ssa\">https://github.com/opencompl/ssa</a>. Combined with <span class=\"user-mention\" data-user-id=\"346070\">@Tomas Skrivan</span> 's scilean, it would be a nice little world to do deep learning in.</p>",
        "id": 435660033,
        "sender_full_name": "Alok Singh",
        "timestamp": 1714166076
    },
    {
        "content": "<p>Wouldn't GPU support reduce build times for e.g. mathlib, or make tactics more lightweight? I think GPU support will one day be needed if we are going to do numerical computations with Lean, like sciLean.</p>",
        "id": 435729529,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1714205213
    },
    {
        "content": "<p>Using GPUs in theorem provers is quite difficult, because GPUs are not like CPUs, they work best when you have a lot of data to process similarly and it is difficult to get compilation, interpretation and elaboration into that form.</p>",
        "id": 435781156,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1714243286
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/435781156\">said</a>:</p>\n<blockquote>\n<p>Using GPUs in theorem provers is quite difficult, because GPUs are not like CPUs, they work best when you have a lot of data to process similarly and it is difficult to get compilation, interpretation and elaboration into that form.</p>\n</blockquote>\n<p>agree, gpus are breadthwise and a lot of proving is variable depth recursive structures. that's why the inet approach is interesting to me, for its autoparallelization, when it's possible to use Big compute on code. rewriting code to be parallel as an optimization is one thing but this tries to improve the underlying execution model</p>",
        "id": 435781385,
        "sender_full_name": "Alok Singh",
        "timestamp": 1714243505
    },
    {
        "content": "<p>One of the authors of the book that describes the optimal reduction strategy used by OP published some reflections on optimal reduction relatively recently, including some thoughts about optimal reduction as it applies to theorem provers (<a href=\"https://arxiv.org/abs/1701.04240v1\">https://arxiv.org/abs/1701.04240v1</a>).</p>",
        "id": 435792461,
        "sender_full_name": "Chris Bailey",
        "timestamp": 1714255061
    },
    {
        "content": "<p>I only skimmed some of the relevant literature but  my impression is that \"parallelism\" in this context seems to be about identifying related expressions and exploiting that to reuse the results of a reduction step, not necessarily about parallel computation in the sense of splitting a workload across hardware or threads or whatever?</p>",
        "id": 435792786,
        "sender_full_name": "Chris Bailey",
        "timestamp": 1714255376
    },
    {
        "content": "<p>Thank you for your kindness in teaching me in my ignorance</p>",
        "id": 435805255,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1714267810
    },
    {
        "content": "<p>Is it difficult to implement parallel processing in Lean?</p>",
        "id": 435805272,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1714267836
    },
    {
        "content": "<p>Yes. :-) It's being worked on, nonetheless, at least in terms of parallel elaboration of theorems in a single file. Completely irrelevant for GPUs.</p>",
        "id": 435843267,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1714288603
    },
    {
        "content": "<p>Thank you and I look forward to Lean's future development...!</p>",
        "id": 435843354,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1714288692
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"337670\">Alok Singh</span> <a href=\"#narrow/stream/113488-general/topic/Kind.20lang/near/435660033\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> I want MLIR support someday. Closest I could find was <a href=\"https://github.com/opencompl/ssa\">https://github.com/opencompl/ssa</a>. Combined with <span class=\"user-mention silent\" data-user-id=\"346070\">Tomas Skrivan</span> 's scilean, it would be a nice little world to do deep learning in.</p>\n</blockquote>\n<p>that repo is more about modeling MLIR('s semantics) in Lean, not so much about using it for compilation, although of course it could help in the future. I know that <span class=\"user-mention\" data-user-id=\"395550\">@Henrik Böving</span>, <span class=\"user-mention\" data-user-id=\"122318\">@Tobias Grosser</span>  and <span class=\"user-mention\" data-user-id=\"130575\">@Siddharth Bhat</span> are working on an LLVM backend of the compiler (in a similar spirit to <a href=\"https://ieeexplore.ieee.org/document/9741279\">https://ieeexplore.ieee.org/document/9741279</a>), but I'm not sure about the current status of that one.</p>",
        "id": 435857366,
        "sender_full_name": "Andrés Goens",
        "timestamp": 1714299476
    },
    {
        "content": "<p>in the context of optimal λ-calculus reduction, \"parallelism\" is often used to mean both:</p>\n<ul>\n<li>\n<p>with optimal sharing, a single beta-reduction can \"represent\" many beta-reductions in \"parallel\".</p>\n</li>\n<li>\n<p>interaction nets, which are used to implement optimal λ-calculus reduction, are very prone to parallel evaluation, because its reduction rules are fully local, require constant time/space, are asynchronous and strongly confluent (which means that not just the \"final result\" is the same, but the \"work done\" is the same, regardless of the order of operations; or, in other words, you can reduce things in parallel without the risk of generating extra work)</p>\n</li>\n</ul>\n<p>regarding running Lean on GPUs - if Lean has a simple core and you manage to target the HVM (which shouldn't be harder than, say, targeting JavaScript) - then that could be one way to run it on GPUs (:</p>\n<p>it is true that GPUs are mostly breadth than depth, so, the kinds of program that benefit are these that recurse in a branching fashion, and then do a lot of work on the \"tips\". hopefully it isn't hard to design purely functional algorithms that behave this way. it often boils down to working more with trees and less with lists</p>",
        "id": 436076471,
        "sender_full_name": "Victor Maia",
        "timestamp": 1714404585
    }
]