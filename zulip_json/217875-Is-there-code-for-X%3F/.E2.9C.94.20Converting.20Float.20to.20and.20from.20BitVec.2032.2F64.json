[
    {
        "content": "<p>As far as I understand, Floats in Lean follow the ieee floating point format. So is there code to convert Floats to and from a single precision (32 bit) or double precision (64 bit) representation as a BitVec using sign, mantissa, and biased exponent? Or is this not advisable to do?</p>",
        "id": 479912482,
        "sender_full_name": "Paula Neeley",
        "timestamp": 1730390948
    },
    {
        "content": "<p>Is <a href=\"https://leanprover-community.github.io/mathlib4_docs/find/?pattern=Float.toRatParts#doc\">docs#Float.toRatParts</a> what you want?</p>",
        "id": 479985618,
        "sender_full_name": "Daniel Weber",
        "timestamp": 1730427682
    },
    {
        "content": "<p>Yeah, I think I can build something around toRatParts and ofBinaryScientific to make it work</p>",
        "id": 480045698,
        "sender_full_name": "Paula Neeley",
        "timestamp": 1730465060
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690087\">Paula Neeley</span> has marked this topic as resolved.</p>",
        "id": 480045712,
        "sender_full_name": "Notification Bot",
        "timestamp": 1730465067
    }
]