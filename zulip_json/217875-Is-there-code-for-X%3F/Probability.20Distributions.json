[
    {
        "content": "<p>I had a look at the probability module in mathlib and it seems there are lots of fairly advanced statements about probability, but I can't find any concrete probability distributions (like uniform, normal, exponential, etc).</p>\n<p>Are those distributions well hidden or not implemented? If it's the latter, I would like to work on that.</p>",
        "id": 357300245,
        "sender_full_name": "Aaron Bies",
        "timestamp": 1683727031
    },
    {
        "content": "<p>(cc <span class=\"user-mention\" data-user-id=\"572064\">@Peter Pfaffelhuber</span> who is also interested in that area, and I think has distributions high on his wish list)</p>",
        "id": 357300993,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1683727173
    },
    {
        "content": "<p>I personally have the Erdös-Réniy distribution in <a href=\"http://github.com/YaelDillies/LeanCamCombi\">http://github.com/YaelDillies/LeanCamCombi</a></p>",
        "id": 357305296,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1683727883
    },
    {
        "content": "<p>The answer is that they are mostly no implemented, although we have some results like the value of the Gaussian integral (but no Gaussian probability distribution). Any work towards getting those distributions is welcome!</p>\n<p>I am currently PRing work that will give us the definition of the cdf of a real probability distribution as a by-product (<a href=\"https://github.com/leanprover-community/mathlib/pull/18834\">#18834</a>, needs to be split into smaller pieces).</p>",
        "id": 357306337,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1683728058
    },
    {
        "content": "<p>One way of expressing distribution is by using <code>measure_theory.has_pdf</code>, e.g. the uniform distribution can be written using <a href=\"https://leanprover-community.github.io/mathlib_docs/probability/density.html#measure_theory.pdf.is_uniform\"><code>measure_theory.pdf.is_uniform</code></a></p>",
        "id": 357306544,
        "sender_full_name": "Jason KY.",
        "timestamp": 1683728094
    },
    {
        "content": "<p>As a newbie to Lean, I started to define the binomial distribution (as an outer measure on \\R) using pmf(=probability mass functions) from the measure-theory section.  (I can share the code, if you are interested.) Hypergeometric distributions should work very similarly, geometric and Poisson are different, since they are not defined on a finset. <br>\nHowever, I am not sure if it is best to use pmf's. There is also measure.has_density -- or .has_pdf, as Rèmy said --  which is more general in my opinion. So, you could define a binomial distribution as a measure with density with respect to the counting measure on range (n-1), and a Poisson distribution with respect to the counting measure on \\N.</p>",
        "id": 357330018,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1683732173
    },
    {
        "content": "<p>I think in general it is best to use pmf for discrete distributions (note that you can use pmf for countably infinite support since we are using <code>tsum</code>to define it) since you can get pdfs from pmfs for free after we've developed more API. Moreover, pmfs are easier to use since most measurability/integrablity problems won't apply.</p>",
        "id": 357334723,
        "sender_full_name": "Jason KY.",
        "timestamp": 1683733116
    },
    {
        "content": "<p>Further on this subject, do we have basic bounds like the union bound or concentration inequalities? For CS work, these inequalities are (part of) our bread and butter.</p>",
        "id": 357345475,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683735338
    },
    {
        "content": "<p>Union bound : <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measure_theory.measure_Union_le\">docs#measure_theory.measure_Union_le</a><br>\nFor concentration inequalities, it depends on which inequality you want. Markov inequality: <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measure_theory.mul_meas_ge_le_lintegral\">docs#measure_theory.mul_meas_ge_le_lintegral</a><br>\nI have an open PR with Hoeffding's inequality somewhere, but it is on pause because I want to have the tools to define conditionally sub-Gaussian random variables first</p>",
        "id": 357350712,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1683736604
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243312\">Jason KY.</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357334723\">said</a>:</p>\n<blockquote>\n<p>I think in general it is best to use pmf for discrete distributions (note that you can use pmf for countably infinite support since we are using <code>tsum</code>to define it) since you can get pdfs from pmfs for free after we've developed more API. Moreover, pmfs are easier to use since most measurability/integrablity problems won't apply.</p>\n</blockquote>\n<p>Yes and no. The thing is that you can either define the distribution concretely and prove things about it, or you can define the predicate of a random variable being distributed according to the distribution and prove things about those. Your message applies to the first case, but Bhavik, you and I agreed before that the second approach was better because more flexible.</p>",
        "id": 357355663,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1683737990
    },
    {
        "content": "<p>I too need a lot of concentration inequalities for probabilistic combinatorics. Not sure how much overlap with CS there is.</p>",
        "id": 357355971,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1683738057
    },
    {
        "content": "<p>What Bhavik, you and I talked about before was about whats is the best way to say X is distributed according to a specific distribution. Here I am talking about whats the best way of setting up an abstract distribution.</p>",
        "id": 357356985,
        "sender_full_name": "Jason KY.",
        "timestamp": 1683738359
    },
    {
        "content": "<p>Quick question: <code>measure_theory.has_pdf</code>needs three tings, ℙ, X, and μ as an input, but in the end, only μ and the push-forward  of ℙ along X (i.e. a probability distribution on E) is used. Why doesn't the structure need only two things, μ and a probability distribution on E, as input? With such a definition, some X could be <code>has_pdf</code>if its push-forward has this property. Wouldn't this be even more flexible?</p>",
        "id": 357379765,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1683744776
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"387244\">Yaël Dillies</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357355971\">said</a>:</p>\n<blockquote>\n<p>I too need a lot of concentration inequalities for probabilistic combinatorics. Not sure how much overlap with CS there is.</p>\n</blockquote>\n<p>Chernoff, multiplicative chernoff, hoeffding, azuma, mcdiarmird (and if the most general versions of these use measure theoretic stuff, then APIs for discrete versions)</p>",
        "id": 357380147,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683744877
    },
    {
        "content": "<p>Additionally, some distributions : binomial, geometric, bernoulli, poisson.</p>",
        "id": 357380434,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683744970
    },
    {
        "content": "<p>If the theorem statements get tedious then it might be nice to have suitable domain specific tactics to work with them.</p>",
        "id": 357380924,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683745109
    },
    {
        "content": "<p>The lovasz local lemma would also be very useful (including constructive versions. see : <a href=\"https://dl.acm.org/doi/pdf/10.1145/2049697.2049702?casa_token=qAxZX-_SYNcAAAAA:efAO3p4clkT5J7xqMVbEwj2Vohxpuz28V_Yw9sm7_oc1T-GPRi6ZaDg1sdC4SJgy-n4eqmDGakqO5Q\">https://dl.acm.org/doi/pdf/10.1145/2049697.2049702?casa_token=qAxZX-_SYNcAAAAA:efAO3p4clkT5J7xqMVbEwj2Vohxpuz28V_Yw9sm7_oc1T-GPRi6ZaDg1sdC4SJgy-n4eqmDGakqO5Q</a>)</p>",
        "id": 357381538,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683745248
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"387244\">Yaël Dillies</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357355971\">said</a>:</p>\n<blockquote>\n<p>I too need a lot of concentration inequalities for probabilistic combinatorics. Not sure how much overlap with CS there is.</p>\n</blockquote>\n<p>I suspect a non-trivial amount of overlap would exist. You might also want the Chebyshev inequality because of the second moment method.</p>",
        "id": 357382210,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683745438
    },
    {
        "content": "<p>Yes, I definitely need Chebyshev. Chernoff is one of those standard tools that is interchangeable with a bunch of other inequalities, so it would be useful to me but not crucial. I don't know about the other ones.</p>",
        "id": 357382888,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1683745640
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357380147\">said</a>:</p>\n<blockquote>\n<p>Chernoff, multiplicative chernoff, hoeffding, azuma, mcdiarmird (and if the most general versions of these use measure theoretic stuff, then APIs for discrete versions)</p>\n</blockquote>\n<p>We are really not far from having these. In fact we have Chernoff bounds: <a href=\"https://leanprover-community.github.io/mathlib_docs/find/probability_theory.measure_ge_le_exp_cgf\">docs#probability_theory.measure_ge_le_exp_cgf</a> . I have code for Hoeffding (<a href=\"https://github.com/leanprover-community/mathlib/pull/15141\">#15141</a>). Azuma requires conditionally sub-Gaussian to have it in the generality I want, but I will add that as soon as I finish PRing stuff related to the existence of conditional distributions.</p>",
        "id": 357382970,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1683745673
    },
    {
        "content": "<p>Look up Lovazs Local Lemma on Zulip. It's already formalised.</p>",
        "id": 357383177,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1683745730
    },
    {
        "content": "<p>Basically everything in Appendix C and D here : <a href=\"https://cs.nyu.edu/~mohri/mlbook/\">https://cs.nyu.edu/~mohri/mlbook/</a></p>",
        "id": 357383373,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1683745792
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"572064\">Peter Pfaffelhuber</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357379765\">said</a>:</p>\n<blockquote>\n<p>Quick question: <code>measure_theory.has_pdf</code>needs three tings, ℙ, X, and μ as an input, but in the end, only μ and the push-forward  of ℙ along X (i.e. a probability distribution on E) is used. Why doesn't the structure need only two things, μ and a probability distribution on E, as input? With such a definition, some X could be <code>has_pdf</code>if its push-forward has this property. Wouldn't this be even more flexible?</p>\n</blockquote>\n<p>I defined <code>has_pdf</code>for the sole purpose of saying that a random variable has a density. What you suggest can already be expressed using <code>measure.have_lebesgue_decomposition</code> (+ absolute continuity) and would not work well if I want it to be a type class on a random variable.</p>",
        "id": 357383680,
        "sender_full_name": "Jason KY.",
        "timestamp": 1683745863
    },
    {
        "content": "<p>Thanks, this makes sense.</p>",
        "id": 357385140,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1683746279
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"350992\">Rémy Degenne</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357382970\">schrieb</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Probability.20Distributions/near/357380147\">said</a>:</p>\n<blockquote>\n<p>Chernoff, multiplicative chernoff, hoeffding, azuma, mcdiarmird (and if the most general versions of these use measure theoretic stuff, then APIs for discrete versions)</p>\n</blockquote>\n<p>We are really not far from having these. In fact we have Chernoff bounds: <a href=\"https://leanprover-community.github.io/mathlib_docs/find/probability_theory.measure_ge_le_exp_cgf\">docs#probability_theory.measure_ge_le_exp_cgf</a> . I have code for Hoeffding (<a href=\"https://github.com/leanprover-community/mathlib/pull/15141\">#15141</a>). Azuma requires conditionally sub-Gaussian to have it in the generality I want, but I will add that as soon as I finish PRing stuff related to the existence of conditional distributions.</p>\n</blockquote>\n<p>It appears to me that you are suggesting to formalize parts of large deviation theory...</p>",
        "id": 357385511,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1683746402
    },
    {
        "content": "<p>Well a small part, yes. Large deviations and concentration inequalities have a lot in common.</p>",
        "id": 357387307,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1683746936
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"350992\">@Rémy Degenne</span> Thanks for helping me with the ennreal -&gt; real -business today. <br>\n<span class=\"user-mention\" data-user-id=\"373970\">@Aaron Bies</span> <br>\nHere<br>\n<a href=\"https://github.com/pfaffelh/some_probability/blob/master/src/binomial.lean\">https://github.com/pfaffelh/some_probability/blob/master/src/binomial.lean</a><br>\nis an implementation of the binomial distribution using pmf's. Other distributions with finite support should work similarly, once you show that the pmf is non-negative and sums to one.</p>",
        "id": 357706107,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1683842647
    },
    {
        "content": "<p>Actually, for characterizing probability distributions, is there something like the following implemented? <br>\n(In words: Two random variables X and Y have the same distribution iff E[f(X)]=E[f(Y)] for all bounded and continuous f. <br>\n(The → should follow from some lemma in <code>probability_theory.ident_distrib</code>, but the ← requires results from measure theory. I am not sure if E must be metric.)</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">variables</span> <span class=\"o\">{</span><span class=\"bp\">Ω</span> <span class=\"bp\">Ω'</span> <span class=\"n\">E</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"bp\">*</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">measurable_space</span> <span class=\"bp\">Ω</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">measurable_space</span> <span class=\"bp\">Ω'</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">topological_space</span> <span class=\"n\">E</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">measurable_space</span> <span class=\"n\">E</span><span class=\"o\">]</span>\n<span class=\"o\">[</span><span class=\"n\">borel_space</span> <span class=\"n\">E</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">metric_space</span> <span class=\"n\">E</span><span class=\"o\">]</span>  <span class=\"o\">{</span><span class=\"n\">P</span> <span class=\"o\">:</span> <span class=\"n\">measure</span> <span class=\"bp\">Ω</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">is_probability_measure</span> <span class=\"n\">P</span><span class=\"o\">]</span> <span class=\"o\">{</span><span class=\"n\">P'</span> <span class=\"o\">:</span> <span class=\"n\">measure</span> <span class=\"bp\">Ω'</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">is_probability_measure</span> <span class=\"n\">P'</span><span class=\"o\">]</span>\n<span class=\"o\">{</span><span class=\"n\">X</span> <span class=\"o\">:</span> <span class=\"bp\">Ω</span> <span class=\"bp\">→</span> <span class=\"n\">E</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">Y</span> <span class=\"o\">:</span> <span class=\"bp\">Ω'</span> <span class=\"bp\">→</span> <span class=\"n\">E</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">ae_measurable</span> <span class=\"n\">X</span> <span class=\"n\">P</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">ae_measurable</span> <span class=\"n\">Y</span> <span class=\"n\">P'</span><span class=\"o\">]</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">ident_distrib_iff</span>  <span class=\"o\">:</span> <span class=\"o\">(</span><span class=\"n\">probability_theory.ident_distrib</span> <span class=\"n\">X</span> <span class=\"n\">Y</span> <span class=\"n\">P</span> <span class=\"n\">P'</span><span class=\"o\">)</span> <span class=\"bp\">↔</span>\n<span class=\"bp\">∀</span> <span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"o\">:</span> <span class=\"n\">bounded_continuous_function</span> <span class=\"n\">E</span> <span class=\"n\">ℝ</span><span class=\"o\">),</span> <span class=\"bp\">∫</span> <span class=\"n\">ω</span><span class=\"o\">,</span> <span class=\"n\">f</span> <span class=\"o\">(</span><span class=\"n\">X</span> <span class=\"n\">ω</span><span class=\"o\">)</span> <span class=\"bp\">∂</span><span class=\"n\">P</span> <span class=\"bp\">=</span> <span class=\"bp\">∫</span> <span class=\"n\">ω'</span><span class=\"o\">,</span> <span class=\"n\">f</span> <span class=\"o\">(</span><span class=\"n\">Y</span> <span class=\"n\">ω'</span><span class=\"o\">)</span> <span class=\"bp\">∂</span><span class=\"n\">P'</span>\n</code></pre></div>\n<p>This would be a first step in proving that characteristic functions characterize the distribution of a random variable.</p>",
        "id": 357982288,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1683924115
    },
    {
        "content": "<p>The API on these is not in a great shape yet... I intend to work on improving the API, but during the semesters I have had no time for Lean/mathlib. I was also thinking of doing the improvements directly in mathlib4, so as not to add material to be ported.</p>\n<p>But essentially these types of results are in mathlib in a rudimentary form. For example <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measure_theory.tendsto_lintegral_thickened_indicator_of_is_closed\">docs#measure_theory.tendsto_lintegral_thickened_indicator_of_is_closed</a> is close to what you are asking for (it assumes <code>pseudo_emetric_space</code> so covers the metric space case), although it needs to be combined with the facts that closed sets form a pi-system that generate the Borel sigma-algebra, and that such pi-systems determine finite measures.</p>\n<p>We also have somewhat cleaner looking related things like <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measure_theory.finite_measure.tendsto_of_forall_integral_tendsto\">docs#measure_theory.finite_measure.tendsto_of_forall_integral_tendsto</a>, but this should be combined with uniqueness of limits in the topology of weak convergence of measures, which is essentially equivalent to your question.</p>\n<p>So currently you can certainly get this with finite amount of work from existing results, but in the summer I hope to clean up the API about these and once that is done, a better direct statement should be available (maybe in mathlib4?).</p>\n<p>Of course if you want to add a good version of the statement, it is certainly something we'd like to have!</p>",
        "id": 358122093,
        "sender_full_name": "Kalle Kytölä",
        "timestamp": 1683989301
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"373986\">@Kalle Kytölä</span>  Many thanks! I was not aware that weak convergence is essentially implemented. And yes, my question is equivalent to uniqueness of the weak limit (which is surprisingly missing, although Portmanteau Theorem etc are implemented). Since my far-in-the-future-goal are weak convergence results (on continuous-time stochastic processes), I will invest some time in the next few weeks.</p>",
        "id": 358325572,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1684103380
    },
    {
        "content": "<p>Even portmanteau in mathlib is still missing a few implications plus lots of API (like the uniqueness of the weak limits and other standard things). But I do intend to PR the missing parts, too.</p>\n<p>Nevertheless, for example the uniqueness of the limits is very natural and independent of the rest (and amounts to combining a few already existing results), so feel free to add it if you are quicker (I still have a few weeks of teaching and grading and admin for the semester left before I have hope to resume Leaning).</p>\n<p>One thing I don't know is what is the policy on mathlib3 PRs at the moment --- I had basically mentally prepared to continue weak convergence in Lean4 once I have Lean time again. Maybe someone closer to the porting effort can comment on mathlib3 vs mathlib4 PRs. (I guess probability theory and much of measure theory is not yet ported, so at the moment, one can't directly start in mathlib4, but the porting tide also looks not too far away.)</p>",
        "id": 358326585,
        "sender_full_name": "Kalle Kytölä",
        "timestamp": 1684104297
    },
    {
        "content": "<p>I am still new to Lean, and never wrote a single line of Lean4, but I can maybe find out about the policy. For me, showing uniqueness of the weak limit is certainly a good exercise, even when using mathlib3. <br>\n<span class=\"user-mention\" data-user-id=\"373986\">@Kalle Kytölä</span> <br>\nBy the way, you have nice lecture notes on \"Metric spaces\" and \"Large random systems\".</p>",
        "id": 358327890,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1684105438
    },
    {
        "content": "<p>The Lean3 result on uniqueness of weak limits should indeed be a rather nice thing to do. But it might involve passing between some <code>lintegral</code>s and <code>integral</code>s,  some <code>bounded_continuous_function</code>s with codomain <code>nnreal</code> and some with codomain <code>real</code>, coercions to <code>ennreals</code>, and other usual formalization things. A key is of course also to locate the lemmas saying that pi-systems determine finite measures (this exists for sure, but I forgot the name) etc. I hope you'll have fun with it (but the current API holes may be a source of some hiccups).</p>",
        "id": 358328435,
        "sender_full_name": "Kalle Kytölä",
        "timestamp": 1684105884
    },
    {
        "content": "<p>The lemma stating that pi-systems determine finite measures is <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measure_theory.ext_of_generate_finite\">docs#measure_theory.ext_of_generate_finite</a> . The more general lemma about induction using pi-systems is <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measurable_space.induction_on_inter\">docs#measurable_space.induction_on_inter</a> . I always have trouble remembering that last name, because I don't really understand the <code>inter</code> in it.</p>",
        "id": 358375662,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1684135102
    },
    {
        "content": "<p>It sounds like that lemma deserves a docstring!</p>",
        "id": 358845493,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1684270451
    },
    {
        "content": "<p>Just working on the uniqueness of the weak limit... One question: In textbooks, I never saw the <code>ae_measurable</code> property of a random variable, only <code>measurable</code>.  (The <code>ae_measurable</code> is e.g. used in <code>probability_theory.ident_distrib</code>.)  Does this distinction have any real use? So, is there an example, where you really want the image measure of a random variable which is only <code>ae_measurable</code> rather than <code>measurable</code>?</p>",
        "id": 359764780,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1684529346
    },
    {
        "content": "<p>This is useful because an a.e limit of measurable functions is not always measurable since we don’t require the sigma algebra to be complete with respect to a measure. I’m not sure I can give a more specific example in the context you’re interested in, but the high level answer is that we try to assume <code>ae_measurable</code> as much as possible so that we don’t get stuck when working in non-complete measure spaces.</p>",
        "id": 359767649,
        "sender_full_name": "Anatole Dedecker",
        "timestamp": 1684530407
    },
    {
        "content": "<p>Ok, I see. You avoid the usual completeness of the sigma-algebra. I think <a href=\"https://leanprover-community.github.io/mathlib_docs/measure_theory/integral/bochner.html#measure_theory.integral_map_of_strongly_measurable\"><code>measure_theory.integral_map_of_strongly_measurable</code></a> could use a version with only assuming <code>hφ : ae_measurable φ</code> then. (Will also try this...)</p>",
        "id": 359779246,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1684535752
    },
    {
        "content": "<p>Isn't that <a href=\"https://leanprover-community.github.io/mathlib_docs/find/measure_theory.integral_map\">docs#measure_theory.integral_map</a> ? Of course you need to be careful about the measures then...</p>",
        "id": 359779446,
        "sender_full_name": "Anatole Dedecker",
        "timestamp": 1684535840
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"268315\">@Anatole Dedecker</span> <br>\nYes, you are right!</p>",
        "id": 359779983,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1684536127
    },
    {
        "content": "<p>Finally, <a href=\"https://github.com/pfaffelh/some_probability/blob/master/src/separating.lean\">here</a> is a version of uniqueness of the weak limit.</p>",
        "id": 364376899,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1686165902
    },
    {
        "content": "<p>This is great!<br>\nIt would be easier to read if you cut the lines at 100 characters though (and in general see <a href=\"https://leanprover-community.github.io/contribute/style.html\">https://leanprover-community.github.io/contribute/style.html</a>).</p>",
        "id": 364381837,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1686167421
    },
    {
        "content": "<p>The first lemma is <a href=\"https://leanprover-community.github.io/mathlib_docs/find/ennreal.coe_to_nnreal\">docs#ennreal.coe_to_nnreal</a></p>",
        "id": 364382000,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1686167460
    },
    {
        "content": "<p>Also I see that you declare all the types each time you write a lemma: you could write <code>variables {E : Type*} [measurable_space E]</code> at the top of the file to define it once and have it available everywhere.</p>",
        "id": 364384537,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1686168197
    },
    {
        "content": "<p>Thanks for the feedback! I got rid of the first lemma now. <br>\nI tried to use <code>variables ...</code>, but it gave me error messages I could not get rid of. Maybe you can show me next week...</p>",
        "id": 364388799,
        "sender_full_name": "Peter Pfaffelhuber",
        "timestamp": 1686169618
    },
    {
        "content": "<p>A potential error you could get with <code>variables</code> is that if you name an instance it will not be included automatically everywhere. To ensure that your <code>measurable_space</code> hypothesis is found  you should not name the instance (you should write <code>[measurable_space E]</code> and not <code>[m : measurable_space E]</code>).</p>",
        "id": 364390182,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1686170157
    },
    {
        "content": "<p>I don't know why it behaves that way. If something is an instance there should be only one so there is arguably no need to give it a name anyway, but that's not an explanation of the change of behaviour.</p>",
        "id": 364390717,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1686170346
    }
]