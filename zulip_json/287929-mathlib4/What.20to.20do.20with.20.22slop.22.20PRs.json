[
    {
        "content": "<p>These past few months there's been a sharp decrease in the quality of the average new contributor's PR. These PRs will be hundreds or thousands of lines long while proving a comparatively simple result, or they'll be filled with dubious definitions and only their trivial consequences. The effort that it takes to fix them is comparable to the effort it takes to re-do things from scratch. </p>\n<p>I don't want to name names, but I'd imagine most maintainers have come across multiple of these PRs as of late. What's to be done about them? Should they be left unreviewed to accrue dust until they inevitably fall off the queue? Should they be brought to the maintainers' attention so they can deal with them accordingly? Should we leave some standard message like \"this PR doesn't meet our quality standards, see <strong>here</strong>\"? I'm worried about potentially shooing away a new contributor, but I'm also worried about the queue becoming filled with increasingly sloppy material.</p>",
        "id": 576166954,
        "sender_full_name": "Violeta Hern√°ndez",
        "timestamp": 1772160811
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"459227\">Violeta Hern√°ndez</span> <a href=\"#narrow/channel/287929-mathlib4/topic/What.20to.20do.20with.20.22slop.22.20PRs/near/576166954\">said</a>:</p>\n<blockquote>\n<p>These PRs will be hundreds of thousands of lines long</p>\n</blockquote>\n<p>Just thousands, not hundreds of thousands. I feel that such PRs should be closed on sight, explaining that the new contributor must tighten their standards</p>",
        "id": 576176247,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1772166321
    },
    {
        "content": "<p>I meant hundreds <em>or</em> thousands, not hundreds <em>of</em> thousands! Thankfully we haven't seen that just yet.</p>",
        "id": 576176323,
        "sender_full_name": "Violeta Hern√°ndez",
        "timestamp": 1772166358
    },
    {
        "content": "<p>This must have been the same for many open source projects recently. How do other projects deal with this?</p>",
        "id": 576177052,
        "sender_full_name": "Weiyi Wang",
        "timestamp": 1772166696
    },
    {
        "content": "<p><a href=\"https://fluxer.app\">Fluxer</a> is an open-source chat client closely modelled after Discord. Its guidelines for those who want to contribute with LLMs mandate that a human-written explanation accompany each PR</p>",
        "id": 576177525,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1772166848
    },
    {
        "content": "<p>\"Contributor must thoroughly understand what they're adding to the project\"</p>",
        "id": 576177718,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1772166906
    },
    {
        "content": "<p>Could the generality and broadness of something like mathlib / what it is trying to accomplish make such PRs more voluminous, in comparison to other \"narrower\" repositories?</p>",
        "id": 576178294,
        "sender_full_name": "Yan Yablonovskiy üá∫üá¶",
        "timestamp": 1772167310
    },
    {
        "content": "<p>Since it is \"cheap\" (in terms of the blood, sweat, and tears that the PR <em>author</em> needs to invest) to create such a PR, I think there has to be a much higher burden of proof that the code is useful, robust, maintainable, high quality.</p>\n<p>One way to establish that is by showing:</p>\n<ul>\n<li>that this code has been sitting in the <code>ForMathlib/</code> folder of downstream repo <code>XyzzyTheory</code></li>\n<li>was refactored and generalized <code>i</code> times in the past</li>\n<li>but now has been stable for <code>k</code> time (<code>k &gt; 2 months</code>??)</li>\n<li>and has <code>N</code> declarations depending on it, with a DAG depth of <code>d</code> (maybe <code>N &gt; 400</code> and <code>d &gt; 10</code>)</li>\n</ul>\n<p>If </p>\n<ul>\n<li>I know something about Xyzzy theory, and</li>\n<li>I recognize the theorems that are being developed in the <code>XyzyyTheory</code> repo, and</li>\n<li>the blueprint is passing snif tests,<br>\nthen I might be quite inclined to believe that moving (without refactoring!) this piece of code from <code>ForMathlib/</code> into Mathlib proper is a worthwhile thing, and the code has proven itself to be worth its LOC in gold.</li>\n</ul>\n<p>This is a higher standard than we apply today to human contributions.<br>\nIf your AI complains about that, then I'm inclined to tell it</p>\n<blockquote>\n<p><span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span> C'est la vie. Deal with it.</p>\n</blockquote>\n<hr>\n<p>Ok, that was a bit provocative. But I'm trying to work out whether we can find some pragmatic and deterministic measures of high-quality code. The above is an attempt to have a measurable way of determining that code is useful in/for the broader ecosystem.</p>",
        "id": 576193788,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1772176961
    },
    {
        "content": "<p>And then hope you don't get <a href=\"https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/\">a hit piece</a> published about you, I guess</p>",
        "id": 576206642,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1772182137
    },
    {
        "content": "<p>Perhaps one possible approach is a policy that your first 10/20/N PRs must be entirely handwritten. After that, you can be said to have a passable enough understanding of mathlib style to decide on appropriate use of AI. At a minimum we know that you _actually know some Lean_. This requires honesty from reviewees but for the moment it‚Äôs not hard to spot AI proofs</p>",
        "id": 576207700,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1772182518
    },
    {
        "content": "<p>Or put length limits on first PRs? I think that might even be good for completely human-written contributions?</p>",
        "id": 576217705,
        "sender_full_name": "Thomas Browning",
        "timestamp": 1772185583
    },
    {
        "content": "<p>I like both Vlad's and Thomas' suggestions. (Though I bet some would object to the first requirement.)</p>",
        "id": 576248025,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1772195170
    },
    {
        "content": "<p>I would be more inclined to implement the minimum PR requirement than the length limit. I'd put the floor at 5 PRs</p>",
        "id": 576293241,
        "sender_full_name": "Jeremy Tan",
        "timestamp": 1772207421
    },
    {
        "content": "<p>I like the length limit idea (although this can vary depending on how git actually evaluates diffs which is heuristic). </p>\n<p>In addition, I think maybe it is a good idea to institute an \"approved PR\" badge. One gets this badge for their PR by proposing a PR in a dedicated zulip channel and explaining it succinctly (without sounding like an LLM). Maybe they also submit a draft PR. Maintainers can skim it and give a badge of approval. For example maintainers could tell the contributor to shorten the PR or focus on something specific. Only PRs with the <code>approved</code> badge are accepted from contributors with less than 10 or 20 merged PRs.</p>",
        "id": 576311207,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1772212148
    },
    {
        "content": "<p>Right now, new contributors' contributions are often implicitly approved (by virtue of being mentored by experienced community members). For those PRs, adding another hoop to jump through feels not great. (Mathlib's PR process is intimidating enough already.)</p>",
        "id": 576314591,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1772213163
    },
    {
        "content": "<p>I'd rather make it easier to quickly close slop PRs. We're seeing an increase of new members on zulip posting about AI-powered formalisation - so getting better at directing their enthusiasm in useful ways would help more.</p>",
        "id": 576315213,
        "sender_full_name": "Michael Rothgang",
        "timestamp": 1772213349
    },
    {
        "content": "<p>I think Michael has a good point. Certain regulations can have an asymmetric affect on well meaning newcomers versus bad actors. </p>\n<p>It seems Violeta's original question about a standardized message for these cases could be useful for a standard route to quickly closing these PRs. (Maybe it also triggers a queue tag for AI PRs to close??)</p>",
        "id": 576319672,
        "sender_full_name": "Chris Henson",
        "timestamp": 1772214683
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/287929-mathlib4/topic/What.20to.20do.20with.20.22slop.22.20PRs/near/576311207\">said</a>:</p>\n<blockquote>\n<p>In addition, I think maybe it is a good idea to institute an \"approved PR\" badge. One gets this badge for their PR by proposing a PR in a dedicated zulip channel and explaining it succinctly (without sounding like an LLM).</p>\n</blockquote>\n<p>Instead of an ‚Äúapproved PR‚Äù label for all new contributors, would it make sense to flip this and label LLM-generated PRs with an <code>LLM</code> label? (Or <code>uses-LLM</code>, <code>LLM-assisted</code>?) Then when <code>LLM</code> and <code>new-contributor</code> appear together, we know how to gently suggest better ways to start contributing.</p>\n<p>It could be taken as a bit of a scarlet letter, but maybe that‚Äôs just the value-neutral cost of making review take more time? <span aria-label=\"grinning face with smiling eyes\" class=\"emoji emoji-1f601\" role=\"img\" title=\"grinning face with smiling eyes\">:grinning_face_with_smiling_eyes:</span> We do want LLM use to be declared explicitly anyway.</p>\n<p>Perhaps (only) <code>LLM</code> PRs specifically could then receive an additional label (attached by reviewers/maintainers) showing the PR authors are attempting to meet some standards we put in place for high-quality contributions in good-faith. (<code>human-curated</code>?)</p>",
        "id": 576327083,
        "sender_full_name": "Thomas Murrills",
        "timestamp": 1772217383
    },
    {
        "content": "<p>I have a tangential question here. Are there any good ways to identify LLM generated code? While reviewing I often see some highly suboptimal use of tactics and lemmas that looks suspicious but to me it might as well be a novice monkey-typing random tactics that they know until Lean said yes. (I also did this when I started, and frankly I still do it now at times, and it is just that my experience gives me a fairly good random distribution that usually make things work nicely). Also another telltale sign is having weird comments like</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"bp\">/‚Äî</span><span class=\"w\"> </span><span class=\"n\">Let</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">be</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">natural</span><span class=\"w\"> </span><span class=\"n\">number</span><span class=\"w\"> </span><span class=\"k\">with</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">value</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"w\"> </span><span class=\"bp\">-/</span>\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"mi\">3</span>\n</code></pre></div>\n<p>But it might as well be some novice who genuinely have no idea what‚Äôs going on without the comments or mistakenly thought that comments are required at every step of proofs. I do not want to mistakenly accuse people (especially newcomers) of LLM slop but I fear I might be wasting my reviewing energy otherwise.</p>",
        "id": 576360198,
        "sender_full_name": "Andrew Yang",
        "timestamp": 1772231631
    },
    {
        "content": "<p>LLM-generated Lean code usually comes with an LLM-generated PR description (and the reverse is also true), which is usually easy to identify. Use of any markdown headings is a pretty good indicator. (excluding the <code>---</code> in PR descriptions which sometimes creates accidental headings)</p>",
        "id": 576360969,
        "sender_full_name": "Snir Broshi",
        "timestamp": 1772232051
    },
    {
        "content": "<p>Making some thoughts a bit more concrete, here's a proposal for what the overall flow of social actions/responsibilities/expectations could look like.</p>\n<p>Note: the goal of this proposal is to smooth out the \"what do I do about this\" problem reviewers face when encountering an LLM PR, while letting us both encourage good code and more easily close PRs that are not meeting the standards. (It's kind of a wrapper around our standards for LLM code, so we can decouple the social part of this problem.)</p>\n<ul>\n<li>triage: LLM PRs are identified and <code>LLM</code> label is added.<ul>\n<li>PR authors are encouraged to add <code>LLM</code> label themselves by commenting <code>LLM</code></li>\n<li>reviewers suspecting an LLM PR should<ul>\n<li>gently ask the author if there's any doubt (maybe we could have a template for doing this to help reviewers)</li>\n<li>add the label if there is no doubt</li>\n<li>in all cases comment encouraging PR author to do so themselves</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>then, when LLM label is applied to PRs, we communicate of expectations and extend trust (or not):<ul>\n<li>a bot comments on LLM-labeled PRs and points to [LLM Contribution Responsibilities]<ul>\n<li>if <code>new-contributor</code> is present as well, the bot also points to [Different ways to start contributing]</li>\n<li>if PR author has several LLM-labeled PRs already merged, the bot auto-applies <code>human-curated</code> label‚Äîi.e. the PR author is a trusted LLM user, and this saves some further reviewer triage (note: bot should still comment with [LLM Contribution Responsibilities] in this case to give reviewers quick access to the standards)</li>\n<li>if PR is over the length limit and does not have <code>human-curated</code>, the bot also applies <code>awaiting-author</code> and explains the length limit requirement</li>\n</ul>\n</li>\n<li>reviewers then further triage PRs with (only) the LLM label:<ul>\n<li>if the author is acting in good faith/working to meet the responsibilties, apply <code>human-curated</code> label (does not have to yet fully meet the high standards, the author just has to show that they are definitely acting in good faith to do so)</li>\n<li>if not (e.g. it's been a while since the bot has commented about [LLM Contribution Responsibilities] and there is still no movement in that direction from the PR author)<ul>\n<li>comment to please meet the [LLM Contribution Responsibilities] and apply <code>awaiting-author</code><ul>\n<li>if <code>awaiting-author</code> has been removed by the author without material good-faith effort to meet the responsibilities, close the PR with a stock comment that the responsibilities haven't been fulfilled and standards are high for LLM PRs, and review time is at a premium. <code>new-contributor</code>s are told future contributions are welcome.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>This is of course just a proposal to iterate on, so I've made some specific choices :)</p>\n<hr>\n<p>This is a bit outside the proposal per se, but I think the [LLM Contribution Responsibilities] document/blurb that this process wraps should (at least) include the responsibilities of:</p>\n<ul>\n<li>asking the contributor to lay out the \"here is why this is good code\" evidence Johan mentioned in the PR description (or in a comment)</li>\n<li>meeting a length limit</li>\n<li>meeting high quality standards, with examples of what LLMs frequently get wrong</li>\n</ul>\n<p>and should include</p>\n<ul>\n<li>clear communication/warning about what happens if you <em>don't</em> try to meet these responsibilities, i.e. that your PR may be closed promptly</li>\n<li>why these standards are necessary: short explanation about review time, etc. :)</li>\n</ul>\n<hr>\n<p>Likewise the [Different ways to start contributing] document/blurb should explain <em>why</em> we want new contributors to contribute differently, in a welcoming way: it builds trust with the contributor and helps the contributor familiarize themselves with code standards that LLMs struggle to meet. Communicate also that LLM users are expected to closely pilot LLM contributions, and familiarization with ordinary contribution saves a lot of review time.</p>\n<p>It should also perhaps(?) gently suggest converting LLM PRs to draft PRs until the contributor is no longer a new-contributor.</p>",
        "id": 576361730,
        "sender_full_name": "Thomas Murrills",
        "timestamp": 1772232478
    }
]