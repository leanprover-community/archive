[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"315577\">@Mac Malone</span> has put up the PR <a href=\"https://github.com/leanprover-community/mathlib4/pull/6603\">#6603</a>, which modifies the <code>lakefile</code> so that every <code>lake build</code> will first run <code>lake exe cache get</code> automatically.</p>\n<p>The thinking here is that <code>lake exe cache get</code> is now very fast (thanks <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>!) and you want to use the cache a sufficiently high fraction of the time that its worth the slight cost of having it run when you don't.</p>\n<p>Thoughts?</p>",
        "id": 385393589,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692173231
    },
    {
        "content": "<p>What does this do if there is no network connection?</p>",
        "id": 385394047,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692173274
    },
    {
        "content": "<p>Or worse, if there <em>is</em> a network connection, but you're paying by the byte?</p>",
        "id": 385394506,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1692173313
    },
    {
        "content": "<p>Personally I'd rather this be opt-in rather than default.</p>",
        "id": 385400627,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1692173857
    },
    {
        "content": "<p>I think this should hold off until <a href=\"https://github.com/digama0/lean-cache\">https://github.com/digama0/lean-cache</a>, which is even faster than the current <code>lake exe cache get</code></p>",
        "id": 385400719,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692173864
    },
    {
        "content": "<p>With no network connection available, and no cache available, <code>time lake exe cache get</code> takes about 9.5s for me. With a full cache available it takes about 4.5s, without or without network.</p>",
        "id": 385401836,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692173963
    },
    {
        "content": "<p>I think we should try to avoid unsolicited network requests, but we should also try to avoid unsolicited major local builds</p>",
        "id": 385401918,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692173971
    },
    {
        "content": "<p>I think this behavior is more important for actions other than <code>lake build</code>, like opening a random file in the editor</p>",
        "id": 385402426,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692174008
    },
    {
        "content": "<p>Without committing to this specific approach, note that it means you don't have to ever interact with the cmdline in order to get started with mathlib4. Of course many variations are conceivable that could also achieve this.</p>",
        "id": 385403734,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692174123
    },
    {
        "content": "<p>I think the sharpest edge here is still that <code>cache get</code> makes a mess if you already have a file open in the editor (or depending on your point of view, the editor is making the mess)</p>",
        "id": 385403956,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692174141
    },
    {
        "content": "<p>Eric, what is the mess here?</p>",
        "id": 385404636,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174202
    },
    {
        "content": "<p>If you change lean-toolchain with the editor open, then yes there is a giant mess as the server for the editor and <code>lake</code> on the command line fight each other.</p>",
        "id": 385404922,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174235
    },
    {
        "content": "<p>And it is annoying that opening a file can trigger a <code>lake build</code>.</p>",
        "id": 385405664,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174306
    },
    {
        "content": "<p>If you change lean-toolchain with the editor open, then yes there is a giant mess as the server for the editor and <code>lake</code> on the command line fight each other.</p>",
        "id": 385405665,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174306
    },
    {
        "content": "<p>But I don't experience problems caused by running <code>cache get</code> and then reloading a file in the editor...?</p>",
        "id": 385405666,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174306
    },
    {
        "content": "<p>I thought the issue was that, if you have a file open in vscode, this will write oleans, which makes <code>cache get</code> stop before it's done</p>",
        "id": 385407660,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1692174483
    },
    {
        "content": "<p>My usual case is that I have a fresh checkout with no oleans, run <code>lake exe cache get</code>, then open a file to start reading it while the cache is running</p>",
        "id": 385408768,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692174578
    },
    {
        "content": "<p>The result is that the cache ends up incomplete in some way and I have to close the editor, <code>pkill lean</code>, and run <code>lake exe cache get</code> one more time</p>",
        "id": 385409032,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692174611
    },
    {
        "content": "<p>I think you created a thread about this before!</p>",
        "id": 385409134,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692174627
    },
    {
        "content": "<p>Ah, I thought you meant just having a file open, rather than opening a new one while <code>cache</code> was running.</p>",
        "id": 385409864,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174725
    },
    {
        "content": "<p>Yes, this is exactly the request that opening a file doesn't start a build.</p>",
        "id": 385409966,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692174742
    },
    {
        "content": "<p>Ah, we have <a href=\"https://github.com/leanprover/lean4/pull/2385\">lean4#2385</a> now but <code>cache</code> doesn't use the lock yet! Does anyone want to give using <code>Lake.withLockFile</code> a try?</p>",
        "id": 385410625,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692174834
    },
    {
        "content": "<p>At least that would be necessary for explicit invocations of <code>cache</code>. Making caching part of the build like in the PR at hand addresses the issue as well of course.</p>",
        "id": 385411074,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692174903
    },
    {
        "content": "<p>This will requiring bumping to the newest nightly (just landed), but I'm having some trouble there. <code>lake</code> has a new manifest version number, and I think we are going to have to get this updated in all the dependencies (Std, Aesop, Qq, Cli, ProofWidgets) before we can actually bump.</p>",
        "id": 385414620,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692175548
    },
    {
        "content": "<p>Pinging <span class=\"user-mention\" data-user-id=\"315577\">@Mac Malone</span> in case I'm missing something there.</p>",
        "id": 385414693,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692175563
    },
    {
        "content": "<p>Just to say that if we can make it so that mathematicians can download and install a project without ever touching the command line then hang the people who are paying for their internet connection by the byte because the mathematicians who can't use the command line outnumber them 100-1</p>",
        "id": 385415350,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1692175746
    },
    {
        "content": "<p>A \"would you like to try downloading pre-compiled oleans\" popup would make both camps happy</p>",
        "id": 385415600,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1692175817
    },
    {
        "content": "<p>I'm with Kevin on this one --- as close as we can get to single click installation is really important.</p>",
        "id": 385415697,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692175842
    },
    {
        "content": "<p><a href=\"https://github.com/JLimperg/aesop/pull/64\">https://github.com/JLimperg/aesop/pull/64</a> is the bump PR for aesop, <span class=\"user-mention\" data-user-id=\"256311\">@Jannis Limperg</span>.</p>",
        "id": 385415743,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692175860
    },
    {
        "content": "<p>Hopefully that is the only one we need, as <code>Std</code> / <code>quote4</code> / <code>lean4-cli</code> / <code>ProofWidgets4</code> don't actually have lake-manifests (on account of having no dependencies).</p>",
        "id": 385416248,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692175987
    },
    {
        "content": "<p>Those of us who don't want unsolicited network request could presumably override this behaviour with a command-line flag to <code>lake build</code> and a setting in the VS Code extension?</p>",
        "id": 385417156,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1692176267
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/385410625\">said</a>:</p>\n<blockquote>\n<p>Ah, we have <a href=\"https://github.com/leanprover/lean4/pull/2385\">lean4#2385</a> now but <code>cache</code> doesn't use the lock yet! Does anyone want to give using <code>Lake.withLockFile</code> a try?</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span>, I've done this in <a href=\"https://github.com/leanprover-community/mathlib4/pull/6611\">#6611</a>, and it works, but there are a few other PRs that will need to be merged before this can be.</p>",
        "id": 385426080,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692179159
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/385415743\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://github.com/JLimperg/aesop/pull/64\">https://github.com/JLimperg/aesop/pull/64</a> is the bump PR for aesop, <span class=\"user-mention silent\" data-user-id=\"256311\">Jannis Limperg</span>.</p>\n</blockquote>\n<p>On a clean checkout of this PR, I get the following warning, which I don't understand and which breaks my golden tests:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>warning: manifest out of date: git revision of dependency 'std' changed, use `lake update` to update\n</code></pre></div>\n<p><code>lake update</code> runs successfully, but doesn't change the manifest. <span class=\"user-mention\" data-user-id=\"315577\">@Mac Malone</span> could you take a look at this?</p>",
        "id": 385442681,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1692185734
    },
    {
        "content": "<p>Reported as <a href=\"https://github.com/leanprover/lean4/pull/2427\">lean4#2427</a> (by Scott)</p>",
        "id": 385445236,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1692186811
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"240862\">Oliver Nash</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/385417156\">said</a>:</p>\n<blockquote>\n<p>Those of us who don't want unsolicited network request could presumably override this behaviour with a command-line flag to <code>lake build</code> and a setting in the VS Code extension?</p>\n</blockquote>\n<p>Such a flag would be awesome! I'm quite frequently in networks that dont work well (e.g. train wifi/hotspot) and I see a lot of other programs just hanging up forever because there is a network, just no response coming back... (or if there is an answer it might be horrendously slow) but <code>lake build --no-download</code> would be conpletely satisfactory.</p>",
        "id": 385504189,
        "sender_full_name": "Jon Eugster",
        "timestamp": 1692205151
    },
    {
        "content": "<blockquote>\n<p>With a full cache available it takes about <strong>4.5s</strong>, without or without network.</p>\n</blockquote>\n<p>This is pretty bad.  (In addition to downloading over metered/slow/unavailable connections.)  Does this also apply to <code>lake build</code> in downstream projects that have mathlib as a dependency?</p>",
        "id": 385705552,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1692287772
    },
    {
        "content": "<p><code>lake exe noop</code> takes almost 2s on my system, and <code>lake exe cache</code> (i.e. just printing the message) takes <code>3.5s</code>.</p>",
        "id": 385763260,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692311827
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> The good thing is this will remove the <code>noop</code> overhead as lake is already loaded. I haven't look deep into the cache code, but I  believe before the first message it computes all the traces of all files to determine what to download. In addition, lake checks all of cache's dependencies to see if it needs rebuilding. In the auto scenario, cache still sadly needs to do the former (compute traces to determine what to download). However, the later (cache dependency checking) will actually shave some time off the post-cache build because lake will not need to recheck the files it already checked to build cache.</p>",
        "id": 385766706,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692313937
    },
    {
        "content": "<p>A potential extension to his PR is to pass to cache the files it needs to download so it does not have to do trace checking.  This potentially be done by passing the module names line-by-line on stdin. Using command line arguments is probably infeasible because it will almost certainly pass Windows command size limit on a fresh <code>cache get</code>.</p>",
        "id": 385766982,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692314121
    },
    {
        "content": "<p>I think any move away from duplicating Lake trace logic in Cache would be a good move - not just for performance but also robustness, especially in view of potential future Lake changes</p>",
        "id": 385840733,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692342618
    },
    {
        "content": "<p>FWIW, this is what a no-op <code>lake exe cache get</code> consists of on my machine according to Hotspot:<br>\n<a href=\"/user_uploads/3121/cze4zXfHcj_K0wutUe0lqKhd/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/cze4zXfHcj_K0wutUe0lqKhd/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/cze4zXfHcj_K0wutUe0lqKhd/image.png\"></a></div>",
        "id": 385841846,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692343076
    },
    {
        "content": "<p>This is roughly:</p>\n<ul>\n<li>0.6s initialization and <code>loadWorkspace</code>, mostly elaboration it seems</li>\n<li>0.6s <code>materializeDeps</code>, again mostly elaboration</li>\n<li>&lt; 0.1s of trace hashing in Lake (when the many parallel threads in Lake appear)</li>\n<li>1s of cache initializing, most of it spent in <code>String.replace</code> in <code>hashFileContents</code> (oops)</li>\n<li>0.15s of leantar unpacking, which matches its own output (edit: in further runs, it is more like 20ms)</li>\n</ul>",
        "id": 385844860,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692344343
    },
    {
        "content": "<p>So yes, not duplicating effort would help, but also here we can start small by not duplicating a performance bottleneck: <a href=\"https://github.com/leanprover-community/mathlib4/pull/6647\">https://github.com/leanprover-community/mathlib4/pull/6647</a></p>",
        "id": 385847457,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692345105
    },
    {
        "content": "<p>One more data point: with 0.7s, the proofwidgets lakefile is about as costly to elaborate as all other dependencies combined. Could we possibly elaborate them in parallel?</p>",
        "id": 385850046,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692345656
    },
    {
        "content": "<p>Disabling the new compiler, which for lakefiles specifically seems unproblematic, shaves off another 0.3s in total</p>",
        "id": 385850718,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692345783
    },
    {
        "content": "<p>I would guess that proofwidgets does not actually need to do very much when it is running via a cloud release! Can the <code>all</code> target there detect that it is running out of a cloud release and short-circuit something?</p>",
        "id": 385853107,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692346283
    },
    {
        "content": "<p>It's tricky, isn't it: to even know what <code>all</code> is we have to elaborate the file, at least without e.g. restructuring the manifest file (but personally I would think precompiling the lakefile into an olean would be simpler)</p>",
        "id": 385854240,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692346560
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/385840733\">said</a>:</p>\n<blockquote>\n<p>I think any move away from duplicating Lake trace logic in Cache would be a good move - not just for performance but also robustness, especially in view of potential future Lake changes</p>\n</blockquote>\n<p>lean-cache (the <code>cache</code> rewrite) duplicates more of lake trace logic, but this is also the source of a lot of the speedup. There are two main modifications made to the calculation to make it faster:</p>\n<ul>\n<li>Cache the results of <code>lakefile.lean</code> elaboration into a <code>lake-workspace.json</code> file</li>\n<li>Don't hash olean file contents, precompute the result at build time and read it at hash time instead</li>\n</ul>",
        "id": 385971701,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692385224
    },
    {
        "content": "<p>Summing up here:</p>\n<ul>\n<li>The option <code>-KnoCacheGet</code> has been added to this PR, and allows people to opt of the automatic call to <code>cache get</code> on every build.</li>\n<li>Even after <a href=\"https://github.com/leanprover-community/mathlib4/pull/6647\">#6647</a> we're worried that the current <code>cache get</code> is too slow to run every time?</li>\n</ul>\n<p>My inclination is to merge as is, and keep working on speeding things up (through whichever combination of <a href=\"https://github.com/digama0/lean-cache\">https://github.com/digama0/lean-cache</a> and ongoing work on Lake is appropriate). But if we don't want to merge yet, it would be good for those opposed to specify if there is a performance milestone that would be good enough.</p>",
        "id": 386266767,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1692573096
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/386266767\">said</a>:</p>\n<blockquote>\n<p>The option <code>-KnoCacheGet</code> has been added to this PR, and allows people to opt of the automatic call to <code>cache get</code> on every build.</p>\n</blockquote>\n<p>Not that this option is only really use for a CLI <code>lake build</code> invocation. There is currently no easy way to set this option on the server.</p>",
        "id": 386302015,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692592139
    },
    {
        "content": "<p>If one is working on mathlib itself and wants to avoid editing the committed lakefile, the only way to configure the server would be through editor-specific configuration options. Right now, the VSCode extensions does not provide a way to set Lake server options like <code>-K</code> (it only provides for configuring Lean server options like <code>-D</code>).</p>",
        "id": 386302333,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692592358
    },
    {
        "content": "<p>why wouldn't extra <code>-K</code> args just go to lake via <code>lake serve</code>?</p>",
        "id": 386302475,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692592453
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> They can, the problem is there is no current way to configure them (at least with the VSCode extension).</p>",
        "id": 386302614,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692592576
    },
    {
        "content": "<p>I thought there was an extra args vscode option</p>",
        "id": 386302627,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692592595
    },
    {
        "content": "<p><code>Lean4: Server Args</code> looks a bit too structured</p>",
        "id": 386302679,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692592655
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> Yep, but those options are past after the <code>--</code> in the invocation. That is, the VSCode command is:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">lake</span> <span class=\"n\">serve</span> <span class=\"c1\">-- &lt;vscode-options&gt;</span>\n</code></pre></div>\n<p>And the options after the <code>--</code> are the ones forwarded to <code>lean --server</code> not the ones processed by Lake.</p>",
        "id": 386302698,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692592668
    },
    {
        "content": "<p>you might be able to do it from <code>Lean4: toolchain path</code>?</p>",
        "id": 386302704,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692592676
    },
    {
        "content": "<p>or rather <code>Lean4: Lake Path</code></p>",
        "id": 386302854,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1692592792
    },
    {
        "content": "<p>Would this option (currently) not have to go to <code>print-paths</code>, not <code>serve</code>? Note that I proposed a scheme for forwarding such arguments from the editor as part of auto-build customization</p>",
        "id": 386322995,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1692603666
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span> You are correct! I am mistaken. That means the VSCode configuration options would be sufficient if the Lean server followed your proposal.</p>",
        "id": 386481428,
        "sender_full_name": "Mac Malone",
        "timestamp": 1692655223
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/386266767\">said</a>:</p>\n<blockquote>\n<p>But if we don't want to merge yet, it would be good for those opposed to specify if there is a performance milestone that would be good enough.</p>\n</blockquote>\n<p>I ooncur. I would love to hear what to TODOs the mathlib maintains would like to see addressed before this kind of integration is acceptable. Hopefully, the  number of the recent performance improvements in both cache and Lake should get a closer to that position.</p>",
        "id": 389968094,
        "sender_full_name": "Mac Malone",
        "timestamp": 1694219471
    },
    {
        "content": "<p>How about <code>lake exe cache get</code> (or replacement) under a second?</p>\n<p>We are still a long way from that.</p>\n<p>On <code>nightly-testing</code> I see:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">%</span> <span class=\"n\">time</span> <span class=\"n\">lake</span> <span class=\"n\">env</span>\n<span class=\"bp\">...</span>\n<span class=\"n\">lake</span> <span class=\"n\">env</span>  <span class=\"mi\">0</span><span class=\"bp\">.</span><span class=\"mi\">67</span><span class=\"n\">s</span> <span class=\"n\">user</span> <span class=\"mi\">0</span><span class=\"bp\">.</span><span class=\"mi\">22</span><span class=\"n\">s</span> <span class=\"n\">system</span> <span class=\"mi\">98</span><span class=\"bp\">%</span> <span class=\"n\">cpu</span> <span class=\"mi\">0</span><span class=\"bp\">.</span><span class=\"mi\">910</span> <span class=\"n\">total</span>\n</code></pre></div>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">%</span> <span class=\"n\">time</span> <span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span>\n<span class=\"bp\">...</span>\n<span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span>  <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">34</span><span class=\"n\">s</span> <span class=\"n\">user</span> <span class=\"mi\">0</span><span class=\"bp\">.</span><span class=\"mi\">50</span><span class=\"n\">s</span> <span class=\"n\">system</span> <span class=\"mi\">109</span><span class=\"bp\">%</span> <span class=\"n\">cpu</span> <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">692</span> <span class=\"n\">total</span>\n</code></pre></div>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">%</span> <span class=\"n\">time</span> <span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span> <span class=\"n\">get</span>\n<span class=\"n\">No</span> <span class=\"n\">files</span> <span class=\"n\">to</span> <span class=\"n\">download</span>\n<span class=\"n\">Decompressing</span> <span class=\"mi\">3725</span> <span class=\"n\">file</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">)</span>\n<span class=\"n\">unpacked</span> <span class=\"k\">in</span> <span class=\"mi\">3227</span> <span class=\"n\">ms</span>\n<span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span> <span class=\"n\">get</span>  <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">45</span><span class=\"n\">s</span> <span class=\"n\">user</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">10</span><span class=\"n\">s</span> <span class=\"n\">system</span> <span class=\"mi\">250</span><span class=\"bp\">%</span> <span class=\"n\">cpu</span> <span class=\"mi\">5</span><span class=\"bp\">.</span><span class=\"mi\">022</span> <span class=\"n\">total</span>\n</code></pre></div>",
        "id": 389990699,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694233669
    },
    {
        "content": "<p>I just tried running <code>lean-cache</code>, which fails with </p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">thread</span> <span class=\"bp\">'</span><span class=\"n\">main'</span> <span class=\"n\">panicked</span> <span class=\"n\">at</span> <span class=\"bp\">'</span><span class=\"n\">workspace</span><span class=\"bp\">-</span><span class=\"n\">manifest</span> <span class=\"n\">hash</span> <span class=\"n\">does</span> <span class=\"n\">not</span> <span class=\"k\">match</span><span class=\"o\">,</span> <span class=\"n\">run</span> <span class=\"bp\">'</span><span class=\"n\">lake</span><span class=\"bp\">-</span><span class=\"n\">ext''</span><span class=\"o\">,</span> <span class=\"n\">src</span><span class=\"bp\">/</span><span class=\"n\">main.rs</span><span class=\"o\">:</span><span class=\"mi\">532</span><span class=\"o\">:</span><span class=\"mi\">5</span>\n<span class=\"n\">note</span><span class=\"o\">:</span> <span class=\"n\">run</span> <span class=\"k\">with</span> <span class=\"bp\">`</span><span class=\"n\">RUST_BACKTRACE</span><span class=\"bp\">=</span><span class=\"mi\">1</span><span class=\"bp\">`</span> <span class=\"n\">environment</span> <span class=\"kd\">variable</span> <span class=\"n\">to</span> <span class=\"n\">display</span> <span class=\"n\">a</span> <span class=\"n\">backtrace</span>\n</code></pre></div>\n<p>Running <code>lake-ext</code> fails with</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">zsh</span><span class=\"o\">:</span> <span class=\"n\">segmentation</span> <span class=\"n\">fault</span>  <span class=\"bp\">../</span><span class=\"n\">lean</span><span class=\"bp\">-</span><span class=\"n\">cache</span><span class=\"bp\">/</span><span class=\"n\">lake</span><span class=\"bp\">-</span><span class=\"n\">ext</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">bin</span><span class=\"bp\">/</span><span class=\"n\">lake</span><span class=\"bp\">-</span><span class=\"n\">ext</span>\n</code></pre></div>",
        "id": 389991204,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694234153
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>, do you have some advice on how to get the new cache working on <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span>'s machine?</p>",
        "id": 389992902,
        "sender_full_name": "Mac Malone",
        "timestamp": 1694235424
    },
    {
        "content": "<p>I'm guessing lake has changed its data structures in a recent version</p>",
        "id": 390008834,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694247166
    },
    {
        "content": "<p>lake-ext needs to stay up to date with lake (part of why I want to upstream it)</p>",
        "id": 390008857,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694247185
    },
    {
        "content": "<p>actually maybe not, it was last built for rc4 and it looks like nothing changed between that and v4.0.0</p>",
        "id": 390009110,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694247406
    },
    {
        "content": "<p>oh wait, this is on nightly - indeed <a href=\"https://github.com/leanprover/lean4/compare/v4.0.0...13ca443f058b0e33e44a9fdba347a53330463348\">https://github.com/leanprover/lean4/compare/v4.0.0...13ca443f058b0e33e44a9fdba347a53330463348</a> shows some lake changes</p>",
        "id": 390009285,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694247579
    },
    {
        "content": "<p>Yes, I wanted to test on <code>nightly</code> because that includes the <code>lakefile.olean</code> commit.</p>",
        "id": 390017702,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694253969
    },
    {
        "content": "<p>you will at least need to set the lean-toolchain in <code>lake-ext</code> and recompile it, I will see whether code changes are necessary</p>",
        "id": 390034916,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694265832
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/389990699\">said</a>:</p>\n<blockquote>\n<p>How about <code>lake exe cache get</code> (or replacement) under a second?</p>\n</blockquote>\n<p>On nightly-testing with warm mathlib cache and file cache for me:</p>\n<div class=\"codehilite\" data-code-language=\"Bash Session\"><pre><span></span><code><span class=\"gp\">$ </span><span class=\"nb\">time</span><span class=\"w\"> </span>lake<span class=\"w\"> </span>exe<span class=\"w\"> </span>cache<span class=\"w\"> </span>get\n<span class=\"go\">No files to download</span>\n<span class=\"go\">Decompressing 3733 file(s)</span>\n<span class=\"go\">unpacked in 26 ms</span>\n<span class=\"go\">lake exe cache get  0.70s user 0.45s system 116% cpu 0.994 total</span>\n</code></pre></div>",
        "id": 390385704,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694465249
    },
    {
        "content": "<p>It is unfortunate that because of the lakefile.olean optimization, we have to initialize five separate <code>Environment</code>s, which adds measurable time in <code>finalizeImports</code> as well as MT-marking and freeing time. But I don't see an easy way to avoid that.<br>\n<a href=\"/user_uploads/3121/lH6mxyV6VjSO_vt0S7fa2MqW/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/lH6mxyV6VjSO_vt0S7fa2MqW/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/lH6mxyV6VjSO_vt0S7fa2MqW/image.png\"></a></div>",
        "id": 390386198,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694465491
    },
    {
        "content": "<p>Hmm... today I get (also on <code>nightly-testing</code>)</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">%</span> <span class=\"n\">time</span> <span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span> <span class=\"n\">get</span>\n<span class=\"n\">No</span> <span class=\"n\">files</span> <span class=\"n\">to</span> <span class=\"n\">download</span>\n<span class=\"n\">Decompressing</span> <span class=\"mi\">3740</span> <span class=\"n\">file</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">)</span>\n<span class=\"n\">unpacked</span> <span class=\"k\">in</span> <span class=\"mi\">634</span> <span class=\"n\">ms</span>\n<span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span> <span class=\"n\">get</span>  <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">48</span><span class=\"n\">s</span> <span class=\"n\">user</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">02</span><span class=\"n\">s</span> <span class=\"n\">system</span> <span class=\"mi\">507</span><span class=\"bp\">%</span> <span class=\"n\">cpu</span> <span class=\"mi\">2</span><span class=\"bp\">.</span><span class=\"mi\">464</span> <span class=\"n\">total</span>\n</code></pre></div>\n<p>Which is both significantly slower than what you are getting, but also significantly faster than I was getting three days ago (5.022s, see above).</p>",
        "id": 390412022,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694477426
    },
    {
        "content": "<p>I suspect what has changed is that I noticed my <code>.cache/mathlib</code> directory had become enormous, and deleted it (even deleting it took a long time) in the intervening time.</p>",
        "id": 390412116,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694477477
    },
    {
        "content": "<p>Deleting my <code>.cache/mathlib</code> again brings it down to <code>2.2s</code>, so I think there is still a significant effect here.</p>",
        "id": 390412501,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694477663
    },
    {
        "content": "<p>what is your OS / file system?</p>",
        "id": 390412800,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694477787
    },
    {
        "content": "<p>IIRC <code>lake exe cache get</code> no longer does directory traversals so it shouldn't make a big difference even if there are a ton of files</p>",
        "id": 390412866,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694477816
    },
    {
        "content": "<p>mac os (big sur), apfs filesystem</p>",
        "id": 390412939,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694477846
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/390386198\">said</a>:</p>\n<blockquote>\n<p>It is unfortunate that because of the lakefile.olean optimization, we have to initialize five separate <code>Environment</code>s, which adds measurable time in <code>finalizeImports</code> as well as MT-marking and freeing time. But I don't see an easy way to avoid that.</p>\n</blockquote>\n<p>A potential long-term way to avoid this is to import all of the lakefiles at once. This would require making all definitions in each module private (or package namespaced) to avoid name clashes.</p>",
        "id": 390416779,
        "sender_full_name": "Mac Malone",
        "timestamp": 1694479384
    },
    {
        "content": "<p>Also, <span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span>, Do you have performance / hotspot break of were the major time sinks are in lake and/or cache other than finalizing the environment?</p>",
        "id": 390417182,
        "sender_full_name": "Mac Malone",
        "timestamp": 1694479523
    },
    {
        "content": "<p>What are these five <code>Environment</code>s?</p>",
        "id": 390417574,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694479655
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> The environment of the mathlib configuration and each of its dependencies.</p>",
        "id": 390417746,
        "sender_full_name": "Mac Malone",
        "timestamp": 1694479701
    },
    {
        "content": "<p>that is, the <code>lakefile.olean</code> for mathlib and each dependency?</p>",
        "id": 390417806,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694479726
    },
    {
        "content": "<p>or is this <code>lakefile.lean</code> elaboration stuff?</p>",
        "id": 390418173,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694479842
    },
    {
        "content": "<p>It is the combination of <code>Lake</code> closure and specific <code>lakefile.olean</code> for each package</p>",
        "id": 390454559,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694500812
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"315577\">Mac Malone</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/390417182\">said</a>:</p>\n<blockquote>\n<p>Also, <span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span>, Do you have performance / hotspot break of were the major time sinks are in lake and/or cache other than finalizing the environment?</p>\n</blockquote>\n<p>For Lake, it really seems to be only that. For <code>cache</code>, it's<br>\n<a href=\"/user_uploads/3121/PNSDlwGdT0CH3Uu3e1ury2OQ/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/PNSDlwGdT0CH3Uu3e1ury2OQ/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/PNSDlwGdT0CH3Uu3e1ury2OQ/image.png\"></a></div>",
        "id": 390455293,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694501234
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"315577\">Mac Malone</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/390416779\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/390386198\">said</a>:</p>\n<blockquote>\n<p>It is unfortunate that because of the lakefile.olean optimization, we have to initialize five separate <code>Environment</code>s, which adds measurable time in <code>finalizeImports</code> as well as MT-marking and freeing time. But I don't see an easy way to avoid that.</p>\n</blockquote>\n<p>A potential long-term way to avoid this is to import all of the lakefiles at once. This would require making all definitions in each module private (or package namespaced) to avoid name clashes.</p>\n</blockquote>\n<p>Alternatively, you could not use declarations at all and instead use environment extensions for the main data structures (<code>Package</code>, <code>LeanLib</code> etc). That wouldn't help with user declarations though, which could still conflict, although it wouldn't be too hard to institute conventions to avoid clashes here (like putting everything in a suitable namespace).</p>",
        "id": 390457150,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694502186
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/390386198\">said</a>:</p>\n<blockquote>\n<p>as well as MT-marking</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"315577\">@Mac Malone</span> For what it's worth, job parallelism probably only slows us down in the no-op case. I noticed that the (I assume) <code>Environment</code> MT-marking is triggered by <code>Package.fetchRelease</code>, which it shouldn't do when the release is already fetched. But I don't know if that is the only <code>async</code> use.</p>",
        "id": 390474549,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694508947
    },
    {
        "content": "<p>Why does anything have to be marked MT if it's loaded from oleans?</p>",
        "id": 390474719,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694508990
    },
    {
        "content": "<p>those are already persistent</p>",
        "id": 390474735,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694508996
    },
    {
        "content": "<p>I assume it is the extra state created by <code>finalizeImports</code>. The big hash table, for example.</p>",
        "id": 390475048,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694509113
    },
    {
        "content": "<p>you mean the token table? I recently had a look at doing less at import time for that</p>",
        "id": 390475161,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694509156
    },
    {
        "content": "<p>No, the map of all imported declarations in the environment</p>",
        "id": 390475200,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694509172
    },
    {
        "content": "<p>Hm, right, hard to do much about that</p>",
        "id": 390475400,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694509228
    },
    {
        "content": "<p>although maybe if there was a way to set a global flag to MT everything...?</p>",
        "id": 390475500,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694509262
    },
    {
        "content": "<p>Only to import less, which is also hard without a module system</p>",
        "id": 390475528,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694509272
    },
    {
        "content": "<p>That's definitely possible for lakefiles, we could curate something that is barely more than init, some macros, and bootstrapping definitions</p>",
        "id": 390475641,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694509316
    },
    {
        "content": "<p>the tricky part is that unless it is a bootstrap command you have to import a lot of lean just to define an <code>elab</code></p>",
        "id": 390475876,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694509401
    },
    {
        "content": "<p>but maybe lake can get by just using macros?</p>",
        "id": 390475953,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694509436
    },
    {
        "content": "<p>For what it's worth, here is the profile in Firefox Profiler, which may even be a tad better than Hotspot: <a href=\"https://share.firefox.dev/3PyO1JE\">https://share.firefox.dev/3PyO1JE</a></p>",
        "id": 390483604,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694512146
    },
    {
        "content": "<p>how did you capture this? I haven't managed multi-process captures before</p>",
        "id": 390485652,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1694512879
    },
    {
        "content": "<p><code>perf record --call-graph dwarf lake exe cache get</code>, it just works for me</p>",
        "id": 390486591,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694513175
    },
    {
        "content": "<p>Some usage notes: the \"tracks\" for individual threads are hidden by default. The dark blue bars at the bottom of each track seem to show actual CPU use, i.e. time not spent waiting on locks or I/O.</p>",
        "id": 390487757,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694513674
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/287929-mathlib4/topic/running.20.60cache.20get.60.20as.20part.20of.20every.20.60lake.20build.60/near/390412022\">said</a>:</p>\n<blockquote>\n<p>Hmm... today I get (also on <code>nightly-testing</code>)</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">%</span> <span class=\"n\">time</span> <span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span> <span class=\"n\">get</span>\n<span class=\"n\">No</span> <span class=\"n\">files</span> <span class=\"n\">to</span> <span class=\"n\">download</span>\n<span class=\"n\">Decompressing</span> <span class=\"mi\">3740</span> <span class=\"n\">file</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">)</span>\n<span class=\"n\">unpacked</span> <span class=\"k\">in</span> <span class=\"mi\">634</span> <span class=\"n\">ms</span>\n<span class=\"n\">lake</span> <span class=\"n\">exe</span> <span class=\"n\">cache</span> <span class=\"n\">get</span>  <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">48</span><span class=\"n\">s</span> <span class=\"n\">user</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">02</span><span class=\"n\">s</span> <span class=\"n\">system</span> <span class=\"mi\">507</span><span class=\"bp\">%</span> <span class=\"n\">cpu</span> <span class=\"mi\">2</span><span class=\"bp\">.</span><span class=\"mi\">464</span> <span class=\"n\">total</span>\n</code></pre></div>\n<p>Which is both significantly slower than what you are getting, but also significantly faster than I was getting three days ago (5.022s, see above).</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> I forgot to say, that is a surprising platform divergence but also very good to know. Is it a repeated run, i.e. with a warm file cache? It's at least clear that the surprisingly high system time must be in one of the parallel parts, though in the Linux profile these seem very much insignificant.</p>",
        "id": 390488439,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694513988
    },
    {
        "content": "<p>Yes, warmed up cache. I think we consistently see much higher system times for lean on macos than other system.</p>",
        "id": 390498485,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1694518020
    },
    {
        "content": "<p>Ah yes, that is true. But here it is such an outsized part of the total time that it might be worth investigating further. Do we have anyone adept at macOS profiling? :)</p>",
        "id": 390501169,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1694519005
    },
    {
        "content": "<p>I may regain access to a mac next week, in the meantime I'll just mention that I have found the builtin \"Instruments\" application in OSX useful for profiling Lean in the past.</p>",
        "id": 390564360,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1694539239
    }
]