[
    {
        "content": "<p>If I have an <code>ite (i=j) 1 0</code> built up from using <code>classical</code> to get decidability on the type of <code>i</code>, is there a way I can compare it to an <code>ite (i=j) 1 0</code> built from using an instance like <code>[decidable (i=j)]</code>?</p>",
        "id": 272273921,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645110938
    },
    {
        "content": "<p>The <a href=\"https://leanprover-community.github.io/mathlib_docs/tactics.html#congr\">tactic#congr</a> family might help here</p>",
        "id": 272274182,
        "sender_full_name": "Anne Baanen",
        "timestamp": 1645111035
    },
    {
        "content": "<p>Nice! That did it. Thanks a ton!</p>",
        "id": 272274222,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645111055
    },
    {
        "content": "<p>usually this means that a lemma statement went wrong somewhere</p>",
        "id": 272276252,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645111929
    },
    {
        "content": "<p>so, if you don't mind me asking, does the <code>classical</code> ite come from your code or mathlib?</p>",
        "id": 272276315,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645111961
    },
    {
        "content": "<p>It came from my code. I introduced a <code>open_locale classical</code> because the linter told me to replace all instances of <code>[decidable_eq]</code> with <code>classical</code> in the proof</p>",
        "id": 272279945,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645113353
    },
    {
        "content": "<p>Okay, I figured out what was probably wrong. I had first inserted <code>classical</code>s in the tactics of proofs on an as-needed basis, but later decided to just <code>open_locale classical</code>, but in this one proof I had forgotten to take out the <code>classical</code> tactic.</p>",
        "id": 272280689,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645113607
    },
    {
        "content": "<p>the two are not interchangeable. open_locale classical changes the statements of all your definitions and lemmas. classical as the tactic only changes the proofs.</p>",
        "id": 272313375,
        "sender_full_name": "Yakov Pechersky",
        "timestamp": 1645127542
    },
    {
        "content": "<p>You want the latter, not the former. The former restricts your definitions and lemmas to be only valid over things that are classically defined, instead of things that are either classically or decidably defined.</p>",
        "id": 272313553,
        "sender_full_name": "Yakov Pechersky",
        "timestamp": 1645127626
    },
    {
        "content": "<p>Whether or not you rely on decidability inside the proof doesn't matter for the proof itself, which is what the \"classical\" tactic does.</p>",
        "id": 272313647,
        "sender_full_name": "Yakov Pechersky",
        "timestamp": 1645127659
    },
    {
        "content": "<p>Ah, I see. So if I have a term-mode proof, should I turn it into a tactic-mode proof so that I can hit it with the <code>classical</code> tactic?</p>",
        "id": 272316474,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645128728
    },
    {
        "content": "<p>Separately, doing this breaks the statements of some theorems with <code>ite</code> in them, is there a way to fix that without putting in <code>[decidable_eq]</code>s into the hypothesis?</p>",
        "id": 272316833,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645128943
    },
    {
        "content": "<p>The way to fix them <em>is</em> to add <code>decidable_eq</code> hypotheses.</p>",
        "id": 272317002,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645129031
    },
    {
        "content": "<p>Okay, previously having <code>decidable_eq</code> in the hypothesis got the mathlib linter mad at me. Is there a way to tell which <code>decidable_eq</code>s are okay to have? Is it exactly when you need it to make the theorem statement work?</p>",
        "id": 272317189,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645129129
    },
    {
        "content": "<p>Yes, exactly! <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span></p>",
        "id": 272318637,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645129919
    },
    {
        "content": "<p>The idea is that the decidable instances appearing in the type should be general while the ones appearing in the proof can be whatever. And you want to have as few decidability hypotheses as possible, so any that doesn't appear in the type should be declared in the proof.</p>",
        "id": 272318911,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645130057
    },
    {
        "content": "<p>Note the difference between <code>def</code> and <code>lemma</code> here. Because <code>lemma</code> forgets the proof as soon as it's done, you can poison it with <code>classical</code> as much as you want. If you do that in a definition however, you'll make it noncomputable.</p>",
        "id": 272319050,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645130143
    },
    {
        "content": "<p>More details: an if-then-else that branches on a condition P must have that P is decidable! Otherwise you can't actually consider the two branches</p>",
        "id": 272319123,
        "sender_full_name": "Yakov Pechersky",
        "timestamp": 1645130169
    },
    {
        "content": "<p>Ah nice! This makes so much sense. Thank you all for the explanation.</p>",
        "id": 272319170,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645130199
    },
    {
        "content": "<p>I should also add that usually some goals within a <code>def</code> are proofs, and those behave just as in a <code>lemma</code>, so you can classical-poison them as much as you want even though they are \"part of a <code>def</code>\".</p>",
        "id": 272319358,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645130301
    },
    {
        "content": "<p>The important distinction is Type-valued vs Prop-valued goal.</p>",
        "id": 272319406,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645130332
    },
    {
        "content": "<p>Right! So if I want a Prop, then I do the <code>classical</code> tactic, and if I want to make a (computable) definition, then I should use <code>decidable_eq</code>.<br>\n(And I should never use <code>open_locale classical</code>?)</p>",
        "id": 272319559,
        "sender_full_name": "Daniel Packer",
        "timestamp": 1645130414
    },
    {
        "content": "<p><code>open_locale classical</code> is the mathematician's cheat code. Short term gain, long-earned pain.</p>",
        "id": 272319895,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645130624
    },
    {
        "content": "<p>I wish finsupp could be fixed :(</p>",
        "id": 272320459,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645130932
    },
    {
        "content": "<p>Couldn't we follow the same idea as for <code>dfinsupp</code>? There it seems to work well.</p>",
        "id": 272321999,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645131699
    },
    {
        "content": "<p>What’s wrong with <code>finsupp</code>? Is it just that the use of <code>open_locale classical</code> would be a big chore to unwind?</p>",
        "id": 272388533,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1645182595
    },
    {
        "content": "<p>No, not quite. It would indeed be a big chore to unwind, but mostly the definition of <code>finsupp</code> is such that it requires an awful lot of decidability. <code>dfinsupp</code> circumvents the problem by having a more flexible representation of the support.</p>",
        "id": 272399443,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645189894
    },
    {
        "content": "<p>What's wrong with <code>finsupp</code> is that it's hard for mathematicians to use because it's constructive and mathematicians have no idea what decidable equality means because in maths it's an axiom. In fact nobody likes <code>finsupp</code> because it's too constructive for the classical people and somehow not constructive enough for the constructive people, is my understanding of it (I remember Reid bashing it from a constructivist point of view at some point in the past). One day I'll write a purely classical finsupp :-) It's just f : X -&gt; Y and the hypothesis that the preimage of univ - 0 is set.finite.</p>",
        "id": 272409676,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645195421
    },
    {
        "content": "<p>the main reason I care about it is that currently we have to do <a href=\"https://github.com/leanprover-community/mathlib/blob/3e6439cd7e04a883c6f71605e3eb333d7623af07/src/data/nat/factorization.lean#L257\">ugly hacks</a> to make the <code>nat</code> multiplicative inductions computable; I don't much care mathematically but it just seems to me that they should be computable in principle</p>",
        "id": 272409877,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645195532
    },
    {
        "content": "<p>So there in your link is the claim that <code>finsupp is noncomputable</code>, and yet it uses <code>finset</code> instead of <code>set.finite</code> so it's not classical either.</p>",
        "id": 272410227,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645195714
    },
    {
        "content": "<p>Yeah <code>finsupp</code> is actually wrong in multiple ways, if you're expecting <code>finsupp X R</code> to be the free <code>R</code>-module on <code>X</code>.</p>",
        "id": 272412350,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645196686
    },
    {
        "content": "<p><code>dfinsupp</code> is also wrong, but less so.</p>",
        "id": 272412991,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645196993
    },
    {
        "content": "<p>So what would it take to make <code>finsupp</code> computable (or to make a computable replacement for <code>finsupp</code>)?  As <span class=\"user-mention\" data-user-id=\"284160\">@Eric Rodriguez</span>  notes (if I understand correctly), the noncomputability of finsupp is infecting things like <code>nat.factorization</code>, which in turn makes that less appealing than it might be.  For example, it seems reasonable to want to use <code>factorization</code> instead of <code>padic_val_nat</code> in <a href=\"https://github.com/leanprover-community/mathlib/pull/12254\">#12254</a>, but we don't want to force other definitions in the file to be needlessly marked <code>noncomputable</code> just because of this design choice.</p>",
        "id": 273154544,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1645739659
    },
    {
        "content": "<p>Are these problems that go away if you write <code>noncomputable theory</code> at the top of your files?</p>",
        "id": 273154786,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645739812
    },
    {
        "content": "<p>lol I was about to ping you, Reid <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 273154810,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645739825
    },
    {
        "content": "<p>Yeah, I expect that would work for what's in this PR, but I'm not sure if that's compatible with the plans <span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span> and <span class=\"user-mention\" data-user-id=\"479997\">@Sean Golinski</span> have for this file.</p>",
        "id": 273155611,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1645740208
    },
    {
        "content": "<p>I don't see how computability of <code>nat.factorization</code> could be relevant since it's just a thin wrapper around the computable <code>nat.factors</code> (and that uses an algorithm that you would never want to run anyways)</p>",
        "id": 273157581,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645741228
    },
    {
        "content": "<p>But anyways <code>finsupp</code> is in a strange place that makes nobody happy, I think.</p>",
        "id": 273157648,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645741260
    },
    {
        "content": "<p><code>multiset.to_finsupp</code> (used in nat.factorization) is noncomputable, but interestingly enough Eric W tells me this is due to the fact that it is an add-equiv - the equiv itself is computable</p>",
        "id": 273159283,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645742268
    },
    {
        "content": "<p>You can get around this but it's mildly painful - see the proof of <a href=\"https://leanprover-community.github.io/mathlib_docs/find/nat.rec_on_prime_pow\">docs#nat.rec_on_prime_pow</a> for details</p>",
        "id": 273159363,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645742294
    },
    {
        "content": "<p>That's really weird. <code>add_equiv</code> = <code>equiv</code> + Prop-fields</p>",
        "id": 273159387,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645742306
    },
    {
        "content": "<p>The reason I like the idea of using <code>nat.factorization</code> is really that I just find it more readable than <code>padic_val_nat</code>, but I guess the algorithm I want to run is the <code>padic_val_nat</code> one. Perhaps a less painful way would be to simply redefine <code>nat.factorization</code> in terms of <code>padic_val_nat</code>, but perhaps there are other considerations at play.</p>",
        "id": 273162674,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1645744149
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"387244\">@Yaël Dillies</span>, the problem is the <code>has_add</code> argument to <code>@add_equiv.mk</code>, and the fact that <a href=\"https://leanprover-community.github.io/mathlib_docs/find/finsupp.has_add\">docs#finsupp.has_add</a> is noncomputable</p>",
        "id": 273162798,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1645744215
    },
    {
        "content": "<p>Ah okay, makes more sense</p>",
        "id": 273162814,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1645744230
    },
    {
        "content": "<p>Lean doesn't care that that <code>has_add</code> argument \"isn't really part of the data\", it's in the expression so the whole thing is poisoned</p>",
        "id": 273162853,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1645744258
    },
    {
        "content": "<p>can we not split this into a normal equiv and then an add equiv? and I guess we can make the normal equiv have a <code>map_add</code>, but that's kinda yucky...</p>",
        "id": 273211231,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1645784518
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110032\">@Reid Barton</span> <a href=\"#narrow/stream/113489-new-members/topic/.60ite.60.20with.20multiple.20decidable.20instances/near/273157581\">said</a>:</p>\n<blockquote>\n<p>I don't see how computability of <code>nat.factorization</code> could be relevant since it's just a thin wrapper around the computable <code>nat.factors</code> (and that uses an algorithm that you would never want to run anyways)</p>\n</blockquote>\n<p>Not sure what you mean by this. The <code>nat.factors</code> algorithm uses trial division but besides that, for the range of numbers where trial division works well, the algorithm is reasonably computationally efficient and I don't know any easy wins to make it faster</p>",
        "id": 273386161,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645943246
    },
    {
        "content": "<p>I think that number theorists would classify trial division as \"an algorithm you would never want to run\" but I'm not sure lean 3 is the place to be experimenting with better ones</p>",
        "id": 273392430,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645952979
    },
    {
        "content": "<p>Trial division is the fastest algorithm for proving the primality of small primes. I implemented a more complicated and efficient algorithm in metamath but I realized it was a waste of time when the largest prime I needed was 4001 and a trial division proof was 10x smaller</p>",
        "id": 273392630,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645953331
    },
    {
        "content": "<p>This is the same reason why <code>norm_num</code> uses the basic n^2 multiplication algorithm instead of FFT multiplication or Karatsuba multiplication</p>",
        "id": 273392714,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645953443
    },
    {
        "content": "<p>you need stupidly large numbers for these algorithms to actually be worthwhile, numbers so large that many other things in lean would go wrong first</p>",
        "id": 273392722,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645953478
    },
    {
        "content": "<p>I think the largest prime you needed might not be representative of the largest prime some of my friends will need if they want to do their kinds of maths in lean but I totally agree that trial division works fine for the kind of things we want to do (eg it enabled us to prove Bertrand's postulate)</p>",
        "id": 273392789,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645953573
    },
    {
        "content": "<p>I think lean isn't ready for your friends' work for other reasons besides this. Lean 4 will be more prepared for a lot of this, not least because it uses GMP powered multiplication anyway</p>",
        "id": 273392879,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645953676
    },
    {
        "content": "<p>Once mathlib4 is up and running, I'm sure we can port <a href=\"https://openresearch-repository.anu.edu.au/bitstream/1885/177195/1/thesis.pdf\">the formalization of the AKS algorithm in HOL4</a> to lean as part of someone's bachelor thesis or similar</p>",
        "id": 273393003,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645953861
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/113489-new-members/topic/.60ite.60.20with.20multiple.20decidable.20instances/near/273392789\">said</a>:</p>\n<blockquote>\n<p>I think the largest prime you needed might not be representative of the largest prime some of my friends will need if they want to do their kinds of maths in lean but I totally agree that trial division works fine for the kind of things we want to do (eg it enabled us to prove Bertrand's postulate)</p>\n</blockquote>\n<p>Actually, I am a bit curious about this. Do you have any concrete examples where large primes are needed in pure maths? (Cue joke about 57)</p>",
        "id": 273393110,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645954031
    },
    {
        "content": "<p>Starting over from 2 each time you find a factor isn't great and you can get another constant factor by knowing primes mod 30 say but yes, it's not as bad as I expected.</p>",
        "id": 273398942,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645962070
    },
    {
        "content": "<p>Oh, it isn't supposed to start over from 2</p>",
        "id": 273399476,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645962954
    },
    {
        "content": "<p>it can be fixed, but I will wait for someone to complain about it</p>",
        "id": 273399955,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645963397
    },
    {
        "content": "<p>I did something similar in the squarefree norm_num extension which does avoid starting over every time it finds a prime</p>",
        "id": 273400033,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645963490
    },
    {
        "content": "<p>Mario, I am not sure if this counts as large, but in <a href=\"https://arxiv.org/abs/1110.1738#:~:text=We%20show%20that%20transcendental%20elements,can%20obstruct%20the%20Hasse%20principle.\">this paper</a>, the authors use the factors of a 318 digit number to get their result.  Searching for <code>factor</code> yields some results in that paper and also see Section 5.1.</p>",
        "id": 273401886,
        "sender_full_name": "Damiano Testa",
        "timestamp": 1645966138
    },
    {
        "content": "<p>The weak Goldbach work needs to verify quickly whether numbers with 30 digits are prime or not.</p>",
        "id": 273402045,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645966352
    },
    {
        "content": "<p>Anyways, the point is that an awful algorithm is just as <code>computable</code> to Lean as a good one. I don't think having potentially awful algorithms to compute as many things in mathlib as possible is a useful goal, and I suspect that nobody would really be trying to add them if <code>noncomputable theory</code> was the default.</p>",
        "id": 273402710,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645967203
    },
    {
        "content": "<p>\"making <code>finsupp</code> computable\" is not the right kind of goal, the correct goal should be \"add data structure X which supports operations Y in time complexity Z, for use in algorithm W\"</p>",
        "id": 273402730,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645967249
    },
    {
        "content": "<p>And the definition of <code>nat.factorization</code> is going to (or at least should!) differ totally depending on whether it returns something computationally list-like or function-like--that is why I want to emphasize that computability is not a yes or no question.</p>",
        "id": 273402934,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645967549
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110032\">Reid Barton</span> <a href=\"#narrow/stream/113489-new-members/topic/.60ite.60.20with.20multiple.20decidable.20instances/near/273402710\">said</a>:</p>\n<blockquote>\n<p>Anyways, the point is that an awful algorithm is just as <code>computable</code> to Lean as a good one. I don't think having potentially awful algorithms to compute as many things in mathlib as possible is a useful goal, and I suspect that nobody would really be trying to add them if <code>noncomputable theory</code> was the default.</p>\n</blockquote>\n<p>While I agree with you for the most part, having awful algorithms to compute lots of things is actually helpful if you want to use <code>dec_trivial</code> to prove trivial things</p>",
        "id": 273403623,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645968513
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"321459\">Damiano Testa</span> <a href=\"#narrow/stream/113489-new-members/topic/.60ite.60.20with.20multiple.20decidable.20instances/near/273401886\">said</a>:</p>\n<blockquote>\n<p>Mario, I am not sure if this counts as large, but in <a href=\"https://arxiv.org/abs/1110.1738#:~:text=We%20show%20that%20transcendental%20elements,can%20obstruct%20the%20Hasse%20principle.\">this paper</a>, the authors use the factors of a 318 digit number to get their result.  Searching for <code>factor</code> yields some results in that paper and also see Section 5.1.</p>\n</blockquote>\n<p>For a theorem like that, I would first reduce to the problem of proving primality of the (given) factors (so don't bother to compute <code>nat.factors</code> directly). The more advanced method for primality proving I used in metamath is <a href=\"https://en.wikipedia.org/wiki/Pocklington_primality_test\">Pocklington certificates</a>, although it is hard to guess in advance whether a given prime number will be broken down into small primes by this method. Once the primes are about 6 digits or so I would use the usual trial division approach to finish it off.</p>",
        "id": 273403990,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645968929
    },
    {
        "content": "<p>In the specific paper above they show that 809147864157687938441948148614369785987783654943839689121548451<br>\n788111145202992792430023470932052297439515068068797124401938255<br>\n799311490342451172887433057574480263654457987109316488649107 is prime by using elliptic curve primality testing.  This should yield a verifiable certificate, though verifying the certificate might also be computationally intensive, in this case, I'm not sure.</p>",
        "id": 273420590,
        "sender_full_name": "Damiano Testa",
        "timestamp": 1645989846
    },
    {
        "content": "<p>Primality algorithms generally perform worse than <a href=\"https://en.wikipedia.org/wiki/Primality_certificate\">primality certificates</a> when the goal is certification. Some primality certificates require prime factorizations of smaller numbers though, so it can be considerably harder to generate a certificate than to check primality directly</p>",
        "id": 273420979,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645990278
    }
]