[
    {
        "content": "<p>Hi! I am a postdoc doing theoretical computer science. In the last few days I formalized in Lean a linear algebra result we had in a recent paper: Theorem 4.1 in <a href=\"https://arxiv.org/pdf/2508.02825\">this paper</a>; the Lean proof is <a href=\"https://gist.github.com/raresbuhai/3d99d272ff1601024797660825d1a725\">here</a>; the main theorem is large_bottom_rank_implies_large_top_rank. (Lean 4.21.0)</p>\n<p>It was all vibe coding — I was curious to try it given the <a href=\"https://arxiv.org/pdf/2510.19804\">recent paper</a> of Alexeev and Mixon.</p>\n<p>My question: In cases like this, what should one be careful about to avoid fooling themselves? I made sure the code compiles and that there are no sorry/admit/axiom in it (also checked with #print axioms). The main theorem statement involves two custom definitions, so I also read them in detail to ensure they match my intentions. Are these the main pitfalls?</p>",
        "id": 560449400,
        "sender_full_name": "Rares Buhai",
        "timestamp": 1764180087
    },
    {
        "content": "<p>Hello Rares also curious in the answer to this for my own use case to prevent AI model reward hacking. What I currently have banned are the following: \"The following tokens are strictly prohibited to prevent reward hacking:</p>\n<p><code>sorry``admit``admit?``unsafe``#eval``IO``IO.FS``System``open System``Lean.Elab``Lean.Meta``Lean.Compiler``Lake</code>\"</p>",
        "id": 560465659,
        "sender_full_name": "Austin Hatfield",
        "timestamp": 1764186770
    },
    {
        "content": "<p>My guess would be that the main pitfalls are the statements and definitions not saying what you would want them to say. They should be checked thoroughly if not written by the user; also, check the types in the statements if they are not explicit (as recalled in other recent threads, for example, subtration in the naturals is not the same as in the integers, so some constants being read as naturals might significantly change the meaning of your statements).</p>",
        "id": 560494790,
        "sender_full_name": "Philippe Duchon",
        "timestamp": 1764201064
    },
    {
        "content": "<p>also, to be safe, avoid tactics like csimp, native_decide (checking for uses of the axiom <code>ofReduceBool</code> should suffice)</p>",
        "id": 560498272,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1764203713
    },
    {
        "content": "<p>Beyond safety, you also want to write the proof in a clean way. Some models using RL can go in circles of unnecessary proof steps.</p>",
        "id": 560498444,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1764203874
    },
    {
        "content": "<p>These are great pointers, thanks!</p>",
        "id": 560548171,
        "sender_full_name": "Rares Buhai",
        "timestamp": 1764235184
    },
    {
        "content": "<p>I'd proofread the output anyway just to make sure the AI system isn't exploiting any unknown Lean bug. Sometimes it happens on hyper-optimized AI systems: <a href=\"https://youtu.be/Lu56xVlZ40M?si=1uISdMwE4fdc92N_&amp;t=254\">https://youtu.be/Lu56xVlZ40M?si=1uISdMwE4fdc92N_&amp;t=254</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"Lu56xVlZ40M\" href=\"https://youtu.be/Lu56xVlZ40M?si=1uISdMwE4fdc92N_&amp;t=254\"><img src=\"https://uploads.zulipusercontent.net/0b39014cb2b0a8fedb1355a9abc6151c12fa04d2/68747470733a2f2f692e7974696d672e636f6d2f76692f4c75353678566c5a34304d2f6d7164656661756c742e6a7067\"></a></div>",
        "id": 560570272,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1764241550
    },
    {
        "content": "<p>BTW, it is better to create a Github repo than to create a Github gist, as the former makes you fix which version of Lean and libraries you depend on.</p>",
        "id": 560640030,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1764262305
    }
]