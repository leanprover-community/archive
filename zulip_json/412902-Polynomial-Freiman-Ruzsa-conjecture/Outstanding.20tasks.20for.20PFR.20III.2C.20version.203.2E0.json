[
    {
        "content": "<p>Time to restart the third phase of the PFR project, in which we formalize the m-torsion variant, as well as the improved constant from 11 to 9!</p>\n<p>Here is the current state of play. (See also <a href=\"https://teorth.github.io/pfr/blueprint/dep_graph_document.html\">the blueprint dependency graph</a>, where I manually count about 46 blue bubbles that need filling in.)</p>\n<p>Ongoing tasks:</p>\n<ul>\n<li><a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#compare-sums\">Comparing sums</a> Should be a straightforward consequence of <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#klm-2\">Kaimonovich–Vershik–Madiman inequality, II</a>, existing entropy facts, and the finite sum API. <strong>Claimed by <span class=\"user-mention\" data-user-id=\"376152\">@Paul Lezeau</span>  </strong></li>\n<li><a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multidist-indep\">Multidistance of independent variables</a> <strong>Claimed by <span class=\"user-mention\" data-user-id=\"638899\">@Lorenzo Luccioli</span> and <span class=\"user-mention\" data-user-id=\"556875\">@Pietro Monticone</span> </strong></li>\n</ul>\n<p>Outstanding tasks for the m-torsion project:</p>\n<p>A.1. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multidist-nonneg\">Nonnegativity</a> Should follow quickly from <a href=\"https://teorth.github.io/pfr/docs/find/?pattern=max_entropy_le_entropy_add#doc\">pfr#max_entropy_le_entropy_add</a>.  <strong>Completed by <span class=\"user-mention\" data-user-id=\"585783\">@Arend Mellendijk</span> </strong><br>\nA.2. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multidist-ruzsa-I\">Multidistance and Ruzsa distance I</a> A relatively straightforward application of existing Ruzsa distance inequalities together with the definition of multidistance.<br>\nA.3. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multidist-ruzsa-II\">Multidistance and Ruzsa distance II</a> A corollary of A.1 and existing Ruzsa distance inequalities.<br>\nA.4. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multidist-ruzsa-III\">Multidistance and Ruzsa distance III</a> A more complicated application of Ruzsa distance inequalities.<br>\nA.5. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multidist-ruzsa-IV\">Multidistance and Ruzsa distance IV</a> Another more complicated application of Ruzsa distance inequalities.<br>\nA.6. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#multi-zero\">Vanishing</a>  Will follow from A.2 and existing Ruzsa distance inequalities.<br>\nA.7. <a href=\"https://teorth.github.io/pfr/blueprint/sect0012.html#cond-multidist-nonneg\"> Nonnegativity of conditional multidistance</a>.  This is almost completed thanks to A.1, but there is one <code>sorry</code> due to the need to handle events of probability zero.  <strong>Completed by <span class=\"user-mention\" data-user-id=\"657719\">@Terence Tao</span> </strong></p>\n<p>Outstanding tasks for the 2-torsion project, focusing on finishing the KL divergence material, and introducing the rho family of functionals:<br>\nB.1. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#kl-sums\">Kullback–Leibler and sums</a> Should be routine from already established facts about KL divergence.  <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.2. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#kl-cond\">Kullback–Leibler and conditioning</a>  Should follow easily from the relationships between conditional and unconditional entropy and KL divergence. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.3. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#Conditional-Gibbs\">Conditional Gibbs inequality</a> Should follow easily from the already proven Gibbs inequality and definition. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.4. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho_minus_nonneg\">Rho minus nonnegative</a>  Should follow from Gibbs inequality and definitions. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.5. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rhominus-subgroup\">Rho minus of subgroup</a>  Mostly just calculation. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.6. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rhoplus-subgroup\">Rho plus of subgroup</a> Corollary of B.5. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.7.  <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho-subgroup\">Rho of subgroup</a>  Follows from B.5 and B.6. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.8.  <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho-invariant\">Rho invariant</a>  This will follow from translation-invariance of entropy and KL divergence. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.9. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho_of_uniform\">Rho of uniform</a>  This follows quickly from previous facts about rho.<strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.10. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho-sums\">Rho and sums</a> Should follow from B.1. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.11. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho-cond-relabeled\">Conditional rho and relabeling</a>  Should be a straightforward consequence of definitions and known facts about entropy. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.12. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#rho-cts\">Continuity of rho</a>  Potentially tricky, but <a href=\"https://teorth.github.io/pfr/docs/PFR/TauFunctional.html#continuous_tau_restrict_probabilityMeasure\"><code>continuous_tau_restrict_probabilityMeasure</code></a> could be a guide. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.13. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#phi-min-exist\">φ-minimizers exist</a>  Should be somewhat similar to [<code>tau_minimizer_exists</code>] <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong>(<a href=\"https://teorth.github.io/pfr/docs/PFR/TauFunctional.html#tau_minimizer_exists\">https://teorth.github.io/pfr/docs/PFR/TauFunctional.html#tau_minimizer_exists</a>) and use B.12.<br>\nB.14. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#phi-first-estimate\">Bound on I_1</a>  This is similar to <a href=\"https://teorth.github.io/pfr/blueprint/sect0006.html#first-estimate\">the corresponding bound for the original PFR argument.</a> <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong><br>\nB.15. <a href=\"https://teorth.github.io/pfr/blueprint/sect0013.html#I1-I2-diff\">Difference between I_1 and I_2</a>   Follows from already established fibring identities. <strong>Established by <span class=\"user-mention\" data-user-id=\"110050\">@Sébastien Gouëzel</span> </strong></p>",
        "id": 480247656,
        "sender_full_name": "Terence Tao",
        "timestamp": 1730578885
    },
    {
        "content": "<p>Can I claim B.1 to B.8?</p>",
        "id": 480249241,
        "sender_full_name": "Sébastien Gouëzel",
        "timestamp": 1730580513
    },
    {
        "content": "<p>PR at <a href=\"https://github.com/teorth/pfr/pull/228\">https://github.com/teorth/pfr/pull/228</a>. It was fun, but now I should go back to real life for some time :-)</p>",
        "id": 480434052,
        "sender_full_name": "Sébastien Gouëzel",
        "timestamp": 1730723693
    },
    {
        "content": "<p>Here's task A.1: <a href=\"https://github.com/teorth/pfr/pull/229\">https://github.com/teorth/pfr/pull/229</a></p>",
        "id": 480468821,
        "sender_full_name": "Arend Mellendijk",
        "timestamp": 1730733839
    },
    {
        "content": "<p>I have a referee report to finish, so it gave me an excuse to formalize instead some more rho functional lemmas in <a href=\"https://github.com/teorth/pfr/pull/230\">https://github.com/teorth/pfr/pull/230</a>. Structured procrastination works really well :-)</p>",
        "id": 481381868,
        "sender_full_name": "Sébastien Gouëzel",
        "timestamp": 1731097731
    },
    {
        "content": "<p>I've completed the proof that the constant 9 works in PFR instead of 11, in <a href=\"https://github.com/teorth/pfr/pull/231\">https://github.com/teorth/pfr/pull/231</a>. It went smoothly, for the most part. </p>\n<p>I've found the proof for continuity of rho in the blueprint a little bit too optimistic: instead of <code>Clear from definition</code>, it would be more fair to say this is an easy exercise, because the lack of continuity of <code>(x, y) ↦ log (x / y)</code> has to be dealt with somehow.</p>\n<p>Also, once I was done and I did <code>#printaxioms better_PFR_conjecture'</code>, I had the bad surprise to see that it depended on <code>sorryAx</code>. In fact, it's been the case of all mains theorems in the project, since the commit <a href=\"https://github.com/teorth/pfr/commit/ebd4d5babe62b9ae162c2667c05d774269afd4b1#diff-3ea916cc29c2e35fe3f117b8e69973be0c8c26d1da802976094fb9caf6a5e8d9R72\">https://github.com/teorth/pfr/commit/ebd4d5babe62b9ae162c2667c05d774269afd4b1#diff-3ea916cc29c2e35fe3f117b8e69973be0c8c26d1da802976094fb9caf6a5e8d9R72</a> introduced a sorry in <code>measure_compl_support</code>. Anyway, it wasn't hard to fix once I had located the source of the sorry. By the way, is there a better command than <code>#printaxioms</code> that would have told me where the sorry was coming from? I had to do a bit of detective work to find it by hand...</p>",
        "id": 482965985,
        "sender_full_name": "Sébastien Gouëzel",
        "timestamp": 1731920931
    },
    {
        "content": "<p>Yes, sorry about <code>measure_compl_support</code>. I was stuck in the middle of a bigger refactor and thought it would make an easy exercise for someone else <span aria-label=\"grinning\" class=\"emoji emoji-1f600\" role=\"img\" title=\"grinning\">:grinning:</span></p>",
        "id": 482966397,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1731921055
    },
    {
        "content": "<p>For axiom-tracking, see <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/Tracking.20axioms\">#general &gt; Tracking axioms</a></p>",
        "id": 482966539,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1731921102
    },
    {
        "content": "<p>Wow, that's amazing!  I thought it would take a much more painstaking and slow approach, but perhaps having a working model of formalized PFR for the previous constants helped.</p>\n<p>Ah, yes, you're right, a bit of massaging is needed to make it self-evident that Kullback-Liebler divergence is in fact a continuous operation.</p>",
        "id": 483112470,
        "sender_full_name": "Terence Tao",
        "timestamp": 1731960194
    },
    {
        "content": "<p>Clearly, the fact that we already had a formalization for exponent 11 made a big difference, as the approach is pretty similar. I could copy and paste lemmas, modify a little bit the statements and fix the proofs wherever Lean complained. Or, at least, take a strong inspiration from the statements, and look at which lemma was useful at which place. A huge speedup overall! </p>\n<p>There's maybe a lesson to be learned here. In analysis, often it's not so much that one uses a big black box, but rather that one uses a standard technique with minor modifications here or there. It looks like formalization works quite nicely also in this kind of situation -- and it's also the kind of situation where I would expect AI to have a good potential.</p>",
        "id": 483120886,
        "sender_full_name": "Sébastien Gouëzel",
        "timestamp": 1731963073
    },
    {
        "content": "<p>I certainly noticed something similar when we moved from C=12 to C=11... often one could copy and paste the code, change a 12 to an 11 at the top, and fix the few statements where Lean actually complained.  It also makes me think that for doing formalized analysis it is more convenient to work with explicit constants everywhere rather than with the more standard asymptotic notation like O() and o() because maintaining the constants as the argument improves over time is actually relatively painless compared with other formalizatio tasks, and of course Lean guarantees that one is not propagating an arithmetic error or something all over the place.</p>",
        "id": 483121861,
        "sender_full_name": "Terence Tao",
        "timestamp": 1731963428
    },
    {
        "content": "<p>A style Bhavik and I are experimenting with is making <em>all</em> constants appearing in statement into separate <code>def</code>s. Something like</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">constLemmaA</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">...</span>\n\n<span class=\"kn\">lemma</span><span class=\"w\"> </span><span class=\"n\">lemmaA</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">...</span><span class=\"w\"> </span><span class=\"bp\">≤</span><span class=\"w\"> </span><span class=\"n\">constLemmaA</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">...</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">constLemmaB</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">ℝ</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"bp\">-</span><span class=\"w\"> </span><span class=\"bp\">*</span><span class=\"w\"> </span><span class=\"n\">constLemmaA</span>\n\n<span class=\"kn\">lemma</span><span class=\"w\"> </span><span class=\"n\">lemmaB</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"bp\">...</span><span class=\"w\"> </span><span class=\"bp\">≤</span><span class=\"w\"> </span><span class=\"n\">constLemmaB</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">...</span>\n</code></pre></div>",
        "id": 483126091,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1731965015
    },
    {
        "content": "<p>This way, whenever a constant changes, the arithmetic error only propagates to the other constants/theorems that directly mention it, instead of long range fixes being needed (as in Sébastien's <a href=\"https://teorth.github.io/pfr/docs/find/?pattern=232#doc\">pfr#232</a>)</p>",
        "id": 483126422,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1731965138
    },
    {
        "content": "<p>This comes at the cost of being less explicit with what the constant is, but that can be fixed in the final theorem by unfolding all constants present</p>",
        "id": 483126569,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1731965195
    },
    {
        "content": "<p>Here's a true story and a problem with mathematics which are related to this.</p>\n<p>The true story: Ken Ribet proved in the late 80s that FLT followed from (what was called then) the Taniyama-Shimura-Weil conjecture; this was the paper which made Wiles drop everything and start working on FLT. Ken's paper (after the FLT-related intro) begins with the line \"let p be an odd prime\", an assumption which runs through the entire 35-or-so-page paper, and he then goes on to prove a level-lowering result for mod p modular forms and mod p Galois representations. </p>\n<p>A few years after the Wiles breakthrough, Richard Taylor drew up a strategy for proving new cases of an old conjecture of Artin on L-functions and one of the ingredients in the strategy was a strengthening of Ken's result so that it included the case p=2. I started working on this in about 2000, and it was a phenomenally painful task, because not only did I have to read Ken's paper very carefully, I also had to check every reference, because p=2 is a famously thorny prime in this area of number theory (not only does GL_2(Z) have elements of order 2, but GL_1(Z) has elements of order 2 and there are even some questions about GL_1 (for example the Iwasawa main conjecture on p-adic L-functions for totally real fields) which have not been satisfactorily resolved for p=2 yet). You really never knew what you were going to hit; Ribet would refer to SGA7 and so you'd have to read for these gigantic tomes of Grothendieck et al and really check hard to make sure that the results he was quoting didn't assume p&gt;2, or find your way around them if they did. I knew some tricks (which I'd learnt from Ken and others) about how to prove various intermediate lemmas for p=2, but I also knew that the experts seemed convinced that proving the full level-lowering theorem was hard. I still remember clear as day where I was (the library in Oberwohlfach) when I realised that in fact all of the issues were surmountable and the paper worked fine for p=2 (as long as you inserted a few tweaks here and there). I emailed Ken and Richard to say \"what is all the fuss about? level-lowering works fine for p=2?\" and I got emails back from them simultaneously, both saying \"I thought that the problem was here\", for two completely different values of \"here\". So in fact the theorem had been known to the union of the experts, it was just that no single expert happened to know it. I wrote a paper \"level-lowering for p=2\" which contained none of my own ideas but was extremely influential because this was the last ingredient Taylor needed for his Artin strategy so I also got a multi-author Duke paper out of it proving new cases of an old conjecture. And all because removing the assumption p&gt;2 was easy, but nobody had noticed it was easy. Terry has pointed out how formalization makes this kind of exercise much easier; I would have loved to have had a formalization of Ken's result back then (ironically, the strategy I'm following for FLT doesn't need Ken's result!)</p>\n<p>The problem with mathematics is a version of this phenomenon which is a couple of orders of magnitude worse, and is explained in the second half of Michael Harris' blog post <a href=\"https://siliconreckoner.substack.com/p/is-it-time-to-sell-out\">here</a>. If you skip over his usual anti-formalization/AI rant in the first half, you'll run into a section \"Dreams of an AI scribe (or, my secret plan to sell out)\" accompanied by a picture of the front cover of a 727-page green book called \"Stabilisation de la formule des traces tordue, Vol 2\". The game here is that this book, and volume 1, prove stabilization of the twisted trace formula in the number field case, but Michael wants it in the function field case, to finish the proof of another theorem he's working on. There is an analogy between number fields and function fields, for example their integer rings are both Dedekind domains with finite residue fields and finite class groups etc, but there are also differences (for example the function fields are characteristic p, so one prime p becomes a \"special\" prime that you can't divide by, and this screws some stuff up). Michael points out that going from the number field theorem to the function field theorem will be hard, because either you write a paper saying \"here is the diff between the green books and the function field theorem\" (which will be kind of hard to publish, he conjectures, although I managed to publish my p=2 paper by writing just the diff, but that was only 8 pages), or you publish another 1000 pages (which will be kind of hard to publish, because 90% of it will be plagiarised). This means (according to Michael, and I suspect he will have evidence for this claim) that humans are reluctant to work on the problem, which leaves him stuck. If AI could do it however...</p>",
        "id": 483133952,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1731968017
    }
]