[
    {
        "content": "<p>There are now a number of experimental add-ons to Lean (e.g.,<a href=\"#narrow/stream/113488-general/topic/Duper\">Duper</a>, <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot\">Lean Copilot</a>, <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/sagredo\">Sagredo</a>... any others?), that could in principle be used to close simple goals that show up in the PFR project.  Would there be interest (particularly among potential code contributors) for installing one or more of these add-ons to PFR as an experiment?  Given that the primary aims of this project have already been successfully completed, and the current pace of contributions has moderated from its peak, I think we can afford the risk that one of these tools may cause some temporary build issues (I assume that in the very worst case we can roll back to a version of the repository without the tool).</p>",
        "id": 408649789,
        "sender_full_name": "Terence Tao",
        "timestamp": 1702918220
    },
    {
        "content": "<p>I'd be open to trying out new tools!</p>",
        "id": 408650430,
        "sender_full_name": "Paul Lezeau",
        "timestamp": 1702918474
    },
    {
        "content": "<p>There is also <a href=\"https://github.com/wellecks/llmstep\">llmstep</a>, which is similar to Lean Copilot.</p>",
        "id": 408653848,
        "sender_full_name": "RÃ©my Degenne",
        "timestamp": 1702919546
    },
    {
        "content": "<p>Ideally I would want to experiment with a tool where a participant who doesn't want to use the tool sees no change in his or her workflow (and no big CPU spikes when trying to compile the code), but that participants who do want to try out the tool can do so.</p>",
        "id": 408654208,
        "sender_full_name": "Terence Tao",
        "timestamp": 1702919683
    },
    {
        "content": "<p>As someone who works on similar (but different) tools, I'd love all your thoughts on the current tools both in terms of usability and power.</p>",
        "id": 408655675,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702920191
    },
    {
        "content": "<p>I think except for duper, the others are just suggest and proof search tools that (as long as you don't leave them in your code) won't effect other users.  After running them, you replace the call with the suggested tactic or proof.  <span class=\"user-mention\" data-user-id=\"436568\">@Josh Clune</span>  <span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> <span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span> <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> could say more for sure.</p>",
        "id": 408656030,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702920306
    },
    {
        "content": "<p>Duper also has a similar suggestion mode <code>duper?</code> as well, right?</p>",
        "id": 408656240,
        "sender_full_name": "Terence Tao",
        "timestamp": 1702920373
    },
    {
        "content": "<p>If the tool is invisible to all other users then I don't really see any downside to adding such a tool (other than the one-time cost of actually installing the tool and propagating it to all the local copies of the repo - I assume it isn't an absolutely massive set of files).</p>",
        "id": 408656869,
        "sender_full_name": "Terence Tao",
        "timestamp": 1702920627
    },
    {
        "content": "<p>I also assume that these tools can be maintained independently of mathlib, so that updating one won't break the other.</p>",
        "id": 408657303,
        "sender_full_name": "Terence Tao",
        "timestamp": 1702920800
    },
    {
        "content": "<p>The only potential issue I see is that <code>duper</code> depends on <code>lean_auto</code> which depends on <code>std</code>, which is also a dependency of <code>mathlib</code>. So there may be a clash if two different versions of <code>std</code> are used.</p>",
        "id": 408683971,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1702931884
    },
    {
        "content": "<p>I believe lake is not clever enough to do version resolution, so it won't actually cause a failure necessarily. As long as lean_auto and duper are not using unstable APIs I would not expect them to break very often due to std changes.</p>",
        "id": 408728849,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702953062
    },
    {
        "content": "<p>And if they do, the fix would be to either PR / merge updates to lean_auto / duper or have a branch in a fork and depend on that instead</p>",
        "id": 408728989,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702953119
    },
    {
        "content": "<p>If/when duper becomes a dependency of mathlib, managing this will become the responsibility of the maintainer team, we will make sure that they are always on compatible versions</p>",
        "id": 408729196,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702953175
    },
    {
        "content": "<blockquote>\n<p>Ideally I would want to experiment with a tool where a participant who doesn't want to use the tool sees no change in his or her workflow (and no big CPU spikes when trying to compile the code), but that participants who do want to try out the tool can do so.</p>\n</blockquote>\n<p>LeanCopilot uses Lake's cloud release mechanism. Users don't have to compile any file from LeanCopilot when running <code>lake build</code> in a downstream package (e.g., pfr). The compiled LeanCopilot files are retrieved from GitHub. I don't see any additional burden for users who don't want to use LeanCopilot. Another possibility is to add it as an optional dependency, using the same trick for doc-gen4.</p>\n<blockquote>\n<p>the others are just suggest and proof search tools that (as long as you don't leave them in your code) won't effect other users. After running them, you replace the call with the suggested tactic or proof.</p>\n</blockquote>\n<p>That's correct. You shouldn't leave LeanCopilot's tactics (<code>suggest_tactics</code>, <code>search_proof</code>, etc.) in the code. They are used for generating suggestions. When they succeed, you should click on the suggestions to use them in the proof.  Otherwise, there is no guarantee it will succeed next time (and it can be slow).</p>",
        "id": 409428481,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1703177804
    }
]