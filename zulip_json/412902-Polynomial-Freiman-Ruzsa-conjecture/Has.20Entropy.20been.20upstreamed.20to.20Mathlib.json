[
    {
        "content": "<p>Hello, I am looking for the definition of entropy of a distribution which I found here. I wanted to ask if this definition has already reached Mathlib, and if so, under what name?</p>",
        "id": 520917573,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748445259
    },
    {
        "content": "<p>I really only need basic facts about entropy such as concavity. I just need it on an arbitrary distribution on a finite set.</p>",
        "id": 520917702,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748445306
    },
    {
        "content": "<p>Alternatively, if I use this project's definitions and theorems on entropy, is it advisable to use it as a dependency of just borrow the relevant definitions and attribute them in a doc comment?</p>",
        "id": 520917962,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748445392
    },
    {
        "content": "<p>Specifically this definition : <a href=\"https://github.com/teorth/pfr/blob/0d9c5da9388d34d3b4c591f28f4e206e678817b3/PFR/ForMathlib/Entropy/Basic.lean#L43-L45\">https://github.com/teorth/pfr/blob/0d9c5da9388d34d3b4c591f28f4e206e678817b3/PFR/ForMathlib/Entropy/Basic.lean#L43-L45</a></p>",
        "id": 520919824,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748445953
    },
    {
        "content": "<p>It's not in Mathlib. However we have the Kullback-Leibler divergence in Mathlib (<a href=\"https://leanprover-community.github.io/mathlib4_docs/Mathlib/InformationTheory/KullbackLeibler/Basic.html#InformationTheory.klDiv\">InformationTheory.klDiv</a>), from which you can build the entropy if you want. Not that it's much easier than defining an entropy directly, but you could reuse a lemma or two.</p>",
        "id": 520944778,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1748454144
    },
    {
        "content": "<p>In my case I want to work on purely discrete probability spaces.</p>",
        "id": 520977806,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748468110
    },
    {
        "content": "<p>KL divergence seems one step removed from entropy. Is it convenient to use for proofs where I would probably not compare distributions?</p>",
        "id": 520977955,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748468179
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"350992\">Rémy Degenne</span> <a href=\"#narrow/stream/412902-Polynomial-Freiman-Ruzsa-conjecture/topic/Has.20Entropy.20been.20upstreamed.20to.20Mathlib/near/520944778\">said</a>:</p>\n<blockquote>\n<p>It's not in Mathlib. However we have the Kullback-Leibler divergence in Mathlib (<a href=\"https://leanprover-community.github.io/mathlib4_docs/Mathlib/InformationTheory/KullbackLeibler/Basic.html#InformationTheory.klDiv\">InformationTheory.klDiv</a>), from which you can build the entropy if you want. Not that it's much easier than defining an entropy directly, but you could reuse a lemma or two.</p>\n</blockquote>\n<p>I am wondering about that for basically most probability theory. On the one hand, reinventing the wheel seems tedious. On the other hand I want to keep the measure theoretic probability ideas as deeply hidden as I can, even in proofs. For example, if I need to prove a fresh concentration bound, should I set it up in measure theoretic lingo or should I set it up in the discrete form that I am more fluent with by a lot (I work in CS theory largely).</p>",
        "id": 520978488,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748468476
    },
    {
        "content": "<p>I might end up learning/re-learning enough measure theory to not worry about this if I stick with higher abstraction, but I don’t know if that is the optimal way to get the job done.</p>",
        "id": 520978656,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1748468552
    }
]