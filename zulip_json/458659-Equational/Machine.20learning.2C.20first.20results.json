[
    {
        "content": "<p>Today we managed to produce our first deep learning model for the implications. This is joint work with <strong>Fernando Vaquerizo</strong> (who for the moment being prefers to let the Zulip interaction, etc. to me).</p>\n<p>Our first ideas were on the line of applying natural language processing (NLP) to the problem, what would imply tokenization + vectorization before training some network, since we think that using a pretrained network for some natural language would give a poor response.</p>\n<p>But to calibrate the difficulty we have started with just tokenization (no vectorization) and a small convolutional neural network with few training epochs.</p>\n<p>The results have been surprisingly good! With a 20% of the data for testing, the accuracy is 99.44% (meaning that the model only mis-classifies around 123 thousand of the 22 million implications).</p>\n<p>This has even made us doubt the training data (perhaps the file I generated from the Github output was wrong?), but as far as I can tell it seems correct.</p>\n<p>We will not be able to keep working on this until Monday. Since there still is some room for improvement in this model, at least we have an optimization running.</p>\n<p>Next steps to improve the accuracy even more can be using a deeper CNN, using vectorization, and tweaking some parameters like the reduction of dimensionality. We will try them next week. I'm also curious to try decision trees and see how they fare against CNNs.</p>\n<p>After we find the best possible model, we will use explainable AI (XAI) in order to try to see what patterns is the model seeing, and hopefully extract some metatheorems from them. This may take some time.</p>\n<p>In case you are wondering: we haven't asked the model how does it classify our most interesting questions (Astérix/Obélix, Dupont/Dupond, 5105 is Austin...), will do on Monday, hopefully also in a better model.</p>\n<p>(I end with an anecdote. This morning, a department colleague asked me:<br>\n-- Machine learning, what do you want that for?<br>\n-- With it I will get my last paycheck. And the rest of you will never get paid again!<br>\n(Half joking? <span aria-label=\"scream\" class=\"emoji emoji-1f631\" role=\"img\" title=\"scream\">:scream:</span>))</p>",
        "id": 477739141,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729288237
    },
    {
        "content": "<p>Nice! I should mention that I also did something similar over the last couple of days, except that I built and trained a transformer, not a CNN. My model was trained on 70% of the available implication data, with the other 30% for testing. No training was done on conjectures. The model itself is still training, and I plan to do some more experimentation. I plan to share more once that's done.</p>\n<p>The current version does predict all of the conjectures from the repo to be nonimplications (as expected). I'm still evaluating the accuracy on the test set, but it seems to be around 97% accurate on that. (Of, course, it's probably the case that if one \"knows\" some random collection of 70% of the implications, then 97% or more of the remaining implications are \"obvious\".)</p>\n<p>As mentioned above, I don't expect much for the unknown implications, but I am attaching the current predictions (see below) for those to this message, just in case. I would be interested to see what percentage of these happen to be correct! The data came from a slightly outdated copy of the repo, and since I extracted the data two conjectures were resolved (as non-implications, and hence correctly predicted by the model).</p>\n<p><a href=\"/user_uploads/3121/Mh_5EMs891tvSYBDIgM8x9CN/unknown_predictions\">unknown_predictions</a></p>",
        "id": 477740486,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729288967
    },
    {
        "content": "<p>So, are these models only predicting whether an implication is true/false or do the also cook up a proof/counterexample?</p>",
        "id": 477770629,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1729310933
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477770629\">ha dicho</a>:</p>\n<blockquote>\n<p>So, are these models only predicting whether an implication is true/false or do the also cook up a proof/counterexample?</p>\n</blockquote>\n<p>In our case, only classifying the implication status. To produce a proof we would presumably need a LLM with high dimension, so many space and time resources.</p>",
        "id": 477776894,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729317573
    },
    {
        "content": "<p>Johan, what I did was actually train an encoder transformer to give vector representations associated to any equation -- I did this with about 2000000 randomly generated equations. Those vector embeddings were then fine-tuned to classify whether one implies the other using the implication data, but in principle those embeddings could be used for other tasks as well (whether or not they end up being useful is another question altogether).</p>",
        "id": 477800779,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729339921
    },
    {
        "content": "<p>Given the slogan that machine learning is a form of compression, I am curious as to how large your model is compared to the 22 million bit size of the raw implication graph.</p>",
        "id": 477811343,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729347757
    },
    {
        "content": "<p>How many bits of information are needed to encode the equations themselves? Do we know that?</p>",
        "id": 477812038,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729348423
    },
    {
        "content": "<p><a href=\"https://github.com/teorth/equational_theories/blob/main/data/equations.txt\">https://github.com/teorth/equational_theories/blob/main/data/equations.txt</a> is 158KB.  The Python script <a href=\"https://github.com/teorth/equational_theories/blob/main/scripts/generate_eqs_list.py\">https://github.com/teorth/equational_theories/blob/main/scripts/generate_eqs_list.py</a> to generate it is 4.46KB.</p>",
        "id": 477812516,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729348842
    },
    {
        "content": "<p>We could post a question at <a href=\"https://codegolf.stackexchange.com/\">https://codegolf.stackexchange.com/</a> to see how much bits are really needed</p>",
        "id": 477812549,
        "sender_full_name": "Daniel Weber",
        "timestamp": 1729348900
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690858\">Daniel Weber</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477812549\">said</a>:</p>\n<blockquote>\n<p>We could post a question at <a href=\"https://codegolf.stackexchange.com/\">https://codegolf.stackexchange.com/</a> to see how much bits are really needed</p>\n</blockquote>\n<p>Out of curiosity which <a href=\"https://codegolf.stackexchange.com/tags\">tag</a> would you use? I did not find one that seemed appropriate but then again do not do golfing regularly.</p>",
        "id": 477813506,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1729349669
    },
    {
        "content": "<p>[code-golf] [kolmogorov-complexity] [string] [binary-tree] [abstract-algebra] seem good</p>",
        "id": 477814063,
        "sender_full_name": "Daniel Weber",
        "timestamp": 1729350092
    },
    {
        "content": "<p><a href=\"/user_uploads/3121/jFQqf_L49dBTAY_sv9Z1Wj0g/generate_eqs_list.py\">generate_eqs_list.py</a></p>\n<p>Here's a somewhat smaller version, that only generates exactly <code>equations.txt</code>, with dead code removed. (No change in the algorithm to remove wasteful comparisons)</p>",
        "id": 477815914,
        "sender_full_name": "Amir Livne Bar-on",
        "timestamp": 1729351556
    },
    {
        "content": "<p>the complexity of the implications matrix is much more interesting tho</p>",
        "id": 477816201,
        "sender_full_name": "Amir Livne Bar-on",
        "timestamp": 1729351747
    },
    {
        "content": "<p>Lol I'd love to see how the codegolf community would handle the entire EquationalTheories codebase needed to generate the final implication graph</p>",
        "id": 477816309,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729351810
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690858\">Daniel Weber</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477814063\">said</a>:</p>\n<blockquote>\n<p>[code-golf] [kolmogorov-complexity] [string] [binary-tree] [abstract-algebra] seem good</p>\n</blockquote>\n<p>Ahh</p>\n<p>Correct me if this is wrong. Your question would be to generate the txt file but with as little code as possible.</p>\n<p>I was thinking of compressing the size of the data into another format that could be converted back into the txt file. </p>\n<p>For example a long time ago I asked this Code Golf question</p>\n<p><a href=\"https://codegolf.stackexchange.com/questions/112874/enumerate-binary-trees\">https://codegolf.stackexchange.com/questions/112874/enumerate-binary-trees</a></p>\n<p>and was happy to see the idea by  @xnor which only used various brackets to encode the data.</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">BNF</span><span class=\"w\">             </span><span class=\"n\">xnor</span><span class=\"w\">       </span><span class=\"n\">Christian</span><span class=\"w\">   </span><span class=\"n\">Ben</span>\n<span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"o\">))</span><span class=\"w\">   </span><span class=\"o\">[{}{{}{}}]</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">(</span><span class=\"mi\">00</span><span class=\"o\">))</span><span class=\"w\">     </span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">)))</span><span class=\"w\">   </span><span class=\"o\">[{}{(())}]</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">((</span><span class=\"mi\">0</span><span class=\"o\">)))</span><span class=\"w\">    </span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span>\n<span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">),</span><span class=\"w\"> </span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">))</span><span class=\"w\">   </span><span class=\"o\">[{()}{()}]</span><span class=\"w\"> </span><span class=\"o\">((</span><span class=\"mi\">0</span><span class=\"o\">)(</span><span class=\"mi\">0</span><span class=\"o\">))</span><span class=\"w\">    </span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span>\n<span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"o\">),</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"o\">)</span><span class=\"w\">   </span><span class=\"o\">[{{}{}}{}]</span><span class=\"w\"> </span><span class=\"o\">((</span><span class=\"mi\">00</span><span class=\"o\">)</span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\">     </span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">)),</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"o\">)</span><span class=\"w\">   </span><span class=\"o\">[{(())}{}]</span><span class=\"w\"> </span><span class=\"o\">(((</span><span class=\"mi\">0</span><span class=\"o\">))</span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\">    </span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">)))</span><span class=\"w\">   </span><span class=\"o\">[({}{()})]</span><span class=\"w\"> </span><span class=\"o\">((</span><span class=\"mi\">0</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">)))</span><span class=\"w\">    </span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span>\n<span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">),</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"o\">))</span><span class=\"w\">   </span><span class=\"o\">[({()}{})]</span><span class=\"w\"> </span><span class=\"o\">(((</span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"mi\">0</span><span class=\"o\">))</span><span class=\"w\">    </span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">b</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"o\">)))</span><span class=\"w\">   </span><span class=\"o\">[(({}{}))]</span><span class=\"w\"> </span><span class=\"o\">(((</span><span class=\"mi\">00</span><span class=\"o\">)))</span><span class=\"w\">    </span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"bp\">-</span><span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">u</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">))))</span><span class=\"w\">   </span><span class=\"o\">[(((())))]</span><span class=\"w\"> </span><span class=\"o\">((((</span><span class=\"mi\">0</span><span class=\"o\">))))</span><span class=\"w\">   </span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span>\n</code></pre></div>",
        "id": 477818395,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1729353369
    },
    {
        "content": "<p>Techically \"code to generate the text file\" <em>is</em> a format to compress the data.</p>",
        "id": 477818991,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729353931
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477818991\">said</a>:</p>\n<blockquote>\n<p>Techically \"code to generate the text file\" <em>is</em> a format to compress the data.</p>\n</blockquote>\n<p>Yes, understood.</p>\n<p>The reason I noted it is that the code golfers go to some amazing lengths to save a byte or character when counting the size of their code and allowing them to change the representation as @xnor did can get them a lower number. </p>\n<p>To put it another way, would asking this as a Code Golf question allow for other representations that can be shown to be equivalent or should all results generate the txt file? Also, should it be posed as two seperate questions, one for only txt output and one that allows another representation? These questions are for <span class=\"user-mention\" data-user-id=\"690858\">@Daniel Weber</span> as he suggested the idea.</p>\n<hr>\n<p>EDIT</p>\n<p>The Code Golf task: </p>\n<blockquote>\n<p>Take a single positive integer <code>n</code> as input and output all of the distinct binary trees with <code>n</code> nodes. </p>\n</blockquote>\n<p>The <a href=\"https://codegolf.stackexchange.com/a/112911\">current</a> shortest posted code is at 19 characters/bytes using <a href=\"https://github.com/isaacg1/pyth\">Pyth</a></p>\n<blockquote>\n<p><code>f!|sTf&lt;sY0._T^}1_1t</code></p>\n</blockquote>",
        "id": 477819680,
        "sender_full_name": "Eric Taucher",
        "timestamp": 1729354459
    },
    {
        "content": "<p>I guess if you really want some ML method that would compress this, one could just train some embedding for the ~5000 equations. It would take <code>C * 5000 + D</code> bits to store this for some constants <code>C</code> and <code>D</code>. Such a model wouldn’t be able to say anything about equations that are not part of the list of 5000 though.</p>",
        "id": 477820693,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729355353
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"366057\">Eric Taucher</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477819680\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477818991\">said</a>:</p>\n<blockquote>\n<p>Techically \"code to generate the text file\" <em>is</em> a format to compress the data.</p>\n</blockquote>\n<p>Yes, understood.</p>\n<p>The reason I noted it is that the code golfers go to some amazing lengths to save a byte or character when counting the size of their code and allowing them to change the representation as @xnor did can get them a lower number. </p>\n<p>To put it another way, would asking this as a Code Golf question allow for other representations that can be shown to be equivalent or should all results generate the txt file? Also, should it be posed as two seperate questions, one for only txt output and one that allows another representation? These questions are for <span class=\"user-mention silent\" data-user-id=\"690858\">Daniel Weber</span> as he suggested the idea.</p>\n</blockquote>\n<p>I think it would be best to ask for exactly one equation from each equivalence class (where the relation consists of renaming and swapping LHS and RHS), and allowing any reasonable format (which could be strings like <code>a * (b * c) = (a * b) * c</code>, prefix/postfix, any kind of built-in binary tree representation with labelled leaves, etc.)</p>",
        "id": 477821920,
        "sender_full_name": "Daniel Weber",
        "timestamp": 1729356454
    },
    {
        "content": "<p>It occurs to me that the neural network may largely be reacting to the number of variables present in each law, see the discussion at <a href=\"https://github.com/teorth/equational_theories/pull/278\">equational#278</a> and associated links.  As a general rule of thumb, we do not expect equations with fewer variables to imply equations with more variables, for instance.</p>",
        "id": 477822034,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729356563
    },
    {
        "content": "<p>(except when the new variables are part of a function application to each side of the antecedent)</p>",
        "id": 477822432,
        "sender_full_name": "Jared green",
        "timestamp": 1729356911
    },
    {
        "content": "<p>If we want to learn new math from the models, it may be worth to over-train them until they reach the \"grokking\" regime. There's evidence that grokking works by finding approximate group structures in the training set.</p>",
        "id": 477824933,
        "sender_full_name": "Amir Livne Bar-on",
        "timestamp": 1729359130
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"302984\">Amir Livne Bar-on</span> <a href=\"#narrow/stream/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477824933\">said</a>:</p>\n<blockquote>\n<p>If we want to learn new math from the models, it may be worth to over-train them until they reach the \"grokking\" regime. There's evidence that grokking works by finding approximate group structures in the training set.</p>\n</blockquote>\n<p>Do you have a reference for this? Sounds interesting!</p>",
        "id": 477830313,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729364502
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477830313\">schrieb</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"302984\">Amir Livne Bar-on</span> <a href=\"#narrow/stream/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477824933\">said</a>:</p>\n<blockquote>\n<p>If we want to learn new math from the models, it may be worth to over-train them until they reach the \"grokking\" regime. There's evidence that grokking works by finding approximate group structures in the training set.</p>\n</blockquote>\n<p>Do you have a reference for this? Sounds interesting!</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/pdf/2201.02177\">https://arxiv.org/pdf/2201.02177</a></p>",
        "id": 477830393,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729364573
    },
    {
        "content": "<p>Yes, that's the paper where they found this phenomenon. There was a blog post after that, that dug into the matrices in the network, and found that the embedding uses characters of the group:<br>\n<a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\">https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking</a></p>\n<p>EDIT: also on arxiv<br>\n<a href=\"https://arxiv.org/abs/2301.05217\">https://arxiv.org/abs/2301.05217</a></p>",
        "id": 477830504,
        "sender_full_name": "Amir Livne Bar-on",
        "timestamp": 1729364683
    },
    {
        "content": "<p>Thanks. So in this case they explicitly learned a group structure, right? I interpreted your message as saying that there is evidence that grokking <em>in general</em> works by (somehow) finding (approx) group structures.</p>",
        "id": 477830739,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729364876
    },
    {
        "content": "<p>They only investigated in detail what happens with modular addition. But the original paper showed that the same phenomenon happens with no-as-nice operations. We can guess that modular division also uses characters. And operations such as x^2+y^2 can conceivably use a similar approach to arrive at sparse matrices. So I'm extrapolating that this happens. I don't know how it handles multiplication in S5, maybe using some higher-dimensional representation? It's all guesswork. But I believen in it, since such embedding has very low regularization loss.</p>",
        "id": 477831081,
        "sender_full_name": "Amir Livne Bar-on",
        "timestamp": 1729365207
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477811343\">ha dicho</a>:</p>\n<blockquote>\n<p>Given the slogan that machine learning is a form of compression, I am curious as to how large your model is compared to the 22 million bit size of the raw implication graph.</p>\n</blockquote>\n<p>Nice question! Our CNN file without any compression weights 1.43MB (compare with the 22 million bit table, 2.75MB). It is just a 4-layer CNN.</p>\n<p>As a first approximation and when the accuracy is high, it is sound to say that a model is a form of data compression. But the comparison is not completely fair, in three fronts:<br>\n1) In a way we are mistaking the output for the machine generating it: the model can generate the data, not just store it (compare the factors of a number with the algorithm to find them). In this sense it would be more accurate to compare with the Kolmogorov complexity as already mentioned above. As a rough measure, the zipped file with standard compression for our CNN weights 737kB.<br>\n2) The model does not provide lossless compression: some implications are incorrectly derived, so we can think of those as corrupted data in our table. So it would be needed to compare with machines giving approximate outputs up to a confidence degree.<br>\n3) The model a priori is useful to extrapolate (e.g. for n=5 it could be a guidance for, given any implication, if one should run first Prover9 or Mace4 on the input in order to reduce computing time).</p>",
        "id": 477893376,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729428999
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477822034\">ha dicho</a>:</p>\n<blockquote>\n<p>It occurs to me that the neural network may largely be reacting to the number of variables present in each law, see the discussion at <a href=\"https://github.com/teorth/equational_theories/pull/278\">equational#278</a> and associated links.  As a general rule of thumb, we do not expect equations with fewer variables to imply equations with more variables, for instance.</p>\n</blockquote>\n<p>This kind of question is what XAI (explainable AI) should be able to settle. With it we should be able to extract the reasons why the CNN is returning YES or NO on a given pair with a local method, and on clusters of pairs with a global method. Then we may be able to decide if each pattern established by the model is an actual theorem, or just a heuristic which is better than a random guess for some reason (which should be some kind of probabilistic theorem). But I think there should be several kinds of patterns, anything that the human eye can catch, is catchable by a NN, plus subtler things (a priori, it is true that our CNN has only 4 layers, but also true that there are only 120 thousand mistakes).</p>",
        "id": 477893804,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729429384
    },
    {
        "content": "<p>In fact, now we have optimized the CNN (run for more epochs), and the accuracy has gone up to 99.70%, so only 66 thousand wrong implications.</p>",
        "id": 477893823,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729429404
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477740486\">said</a>:</p>\n<blockquote>\n<p>My model was trained on 70% of the available implication data, with the other 30% for testing.</p>\n</blockquote>\n<p>How were the 70%/30% split? Is it totally randomized or is there some other method? The implication graph is transitive, so if in the training data, the model can see that equation A implies equations 1-100, and equation B implies 200-300, if it's asked if equation A implies B in the testing set, it has a pretty good bet that it doesn't. Conversely the implication is likely to be true if all/most of equation B's implications are a subset of equation As. I think the way to structure this to make the two sets independent of each other is to take a cut of the graph, put all nodes on one side of the cut into the training set, and the edges in the cut and on the other side into the testing set, to make them totally independent of each other. Otherwise the model may just be inferring what transitivity means.</p>",
        "id": 477911127,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729443941
    },
    {
        "content": "<p>Also most of the implication data is extremely redundant. The largest equivalence class (Equation 2) is ~1500 nodes, so since each one implies every other, the edges in the closure of that equivalence class is ~1500^2=2.2m, or about a quarter of the total implications. Furthermore, that equivalence class implies all others, so that's another ~1500*(4694-~1500)=4.8m edges. That's 85% of all implications right there. I'm not super familiar with AI/ML and how to structure experiments, so I don't know if that's an issue or not.</p>",
        "id": 477911465,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729444249
    },
    {
        "content": "<p>I guess could try to generate a random poset of the same size and equivalence class structure as a comparison (but i actually dont have a reasonable random poset model to propose here )</p>",
        "id": 477919038,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729451505
    },
    {
        "content": "<p>Not sure I follow, what is that comparison used for?</p>",
        "id": 477919231,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729451675
    },
    {
        "content": "<p>Also had an idea that is probably a bit of a silly oversimplification of how NNs really work, but given that an NN can probably easily learn the basic rewriting rules, I wonder how well the performance of an x-level NN matches the reachability of implications using x rewrites.</p>",
        "id": 477919426,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729451864
    },
    {
        "content": "<p>I’m doing another training run now training on implications only involving a fixed randomly chosen set of 75% of the equations (also with more aggressive regularization).</p>",
        "id": 477919551,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729451958
    },
    {
        "content": "<p>Curious to see if it performs just as well! Would also be interesting to measure the success rate of guessing all 'implication'/'non-implication' for the remaining 25% as a baseline measure for the most naive strategy.</p>",
        "id": 477919724,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729452107
    },
    {
        "content": "<p>Note that the testing implication set will now include about 50% of all implications.</p>",
        "id": 477919792,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729452161
    },
    {
        "content": "<p>Its to see if the neural network is really learning specific features of the equational poset or it is giving the same performance it would give on a random poset.</p>\n<p>another option is just to randomly permute the vertices and equation labels of the equational poset and see if the accuracy of the neural net training on this shuffled model performs noticeably worse</p>",
        "id": 477919918,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729452254
    },
    {
        "content": "<p>(EDITED because of wrong answer)</p>\n<p>For now we have generated a random label (yes/no)  for each pair of equations, then trained the CNN with 60/20/20 percentages (training, validation, test) just as before. After 80 epochs (early stopping), the result is 49.99% accuracy.</p>",
        "id": 478019332,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729507218
    },
    {
        "content": "<p>It should be a poset, not completely random</p>",
        "id": 478019961,
        "sender_full_name": "Daniel Weber",
        "timestamp": 1729507415
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"690858\">Daniel Weber</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478019961\">ha dicho</a>:</p>\n<blockquote>\n<p>It should be a poset, not completely random</p>\n</blockquote>\n<p>You are completely right, will fix soon.</p>",
        "id": 478021528,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729507898
    },
    {
        "content": "<p>Related to transitivity, etc. Since we have only 600k explicit implications (2.7% of the total), if the CNN is learning transitivity it should perform well with a really small training dataset. According to that, we have taken 5/5/90 percentages (training, validation, test), with resulting 99.6% accuracy on test.</p>\n<p>Next we will try 1/1/98 (so less implications that we have needed to build the whole poset).</p>",
        "id": 478025529,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729509109
    },
    {
        "content": "<p>With 1% of the implications for the training set (1/1/98),  the accuracy of the CNN with 4 layers is 99.3%.</p>",
        "id": 478028189,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729509905
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span>  <span class=\"user-mention silent\" data-user-id=\"690858\">Daniel Weber</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478019961\">ha dicho</a>:</p>\n<blockquote>\n<p>It should be a poset, not completely random</p>\n</blockquote>\n<p>What if we just use numbers for the labels? We maintain exactly the same poset, just with random numbers as labels (not the equation numbers, which may carry some information because of the order in which the equations are generated). This is easy to generate from the project files. This would be enough, right?</p>",
        "id": 478033831,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729511505
    },
    {
        "content": "<p>For graphs, there's a wide variety of models used in practice for random graphs that capture real world scenarios better than the Erdos-Renyi random graph. Like \"sample vertices as points in R^n from a mixture of k Gaussians, then add edges with a probability based on their distance\". A popular question then is finding a model that seems to fit a given dataset well. (Ideally this would be maximizing the likelihood of the dataset under the model, but in practice it's looking for something that reproduces a few select statistics well.)</p>\n<p>I doubt there's established models like this for posets. (Of course we can talk about taking just the minimal edges and forgetting directedness, but this also loses a lot of information.) But we could probably come up with something.</p>",
        "id": 478073671,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729520435
    },
    {
        "content": "<p>For example: how large of an \"n\" do we need to embed the poset in R^n, equipped with the elementwise ≥ relation?</p>",
        "id": 478073915,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729520501
    },
    {
        "content": "<p>And, if we built a random poset by sampling points randomly in [0,1]^n, does our poset look \"typical\"?</p>",
        "id": 478074158,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729520553
    },
    {
        "content": "<p>Claim: n is at least 3, otherwise Graphiti could give a planar presentation <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 478074436,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729520626
    },
    {
        "content": "<p>Seems easier to just segment the implication data set and see how well it performs, unless there's some additional insight we get by sampling random graphs that we don't from looking at the one we have.</p>",
        "id": 478076729,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729521220
    },
    {
        "content": "<p>Continuing on the earlier thoughts, there are (distinct from the above) ideas of <a href=\"https://en.wikipedia.org/wiki/Order_dimension\">Order Dimension</a>, k-Dimension (same article), and <a href=\"https://en.wikipedia.org/wiki/Interval_dimension\">Interval dimension</a> which are also good measures how 'complicated' a poset is. It would be great to try to calculate such an embedding. (Maybe start with the sublattice for just 3-application equations first.)</p>",
        "id": 478092799,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729525278
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"448405\">Alex Meiburg</span> <a href=\"#narrow/stream/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478073915\">said</a>:</p>\n<blockquote>\n<p>For example: how large of an \"n\" do we need to embed the poset in R^n, equipped with the elementwise ≥ relation?</p>\n</blockquote>\n<p>The order dimension is this, right?</p>",
        "id": 478093106,
        "sender_full_name": "Daniel Weber",
        "timestamp": 1729525353
    },
    {
        "content": "<p>(I've been trying to read more about it since I sent the link) I think so, I think I initially misunderstood it. It's kind of surprising that it's NP-complete to test for order dimension 3, since the \"left-right algorithm\" sounds so much it like it should generalize immediately to higher dimensions.</p>",
        "id": 478094593,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729525788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"761203\">Vlad Tsyrklevich</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/477919724\">said</a>:</p>\n<blockquote>\n<p>Curious to see if it performs just as well! Would also be interesting to measure the success rate of guessing all 'implication'/'non-implication' for the remaining 25% as a baseline measure for the most naive strategy.</p>\n</blockquote>\n<p>This seems to again have about 97% accuracy on the testing set, after training for only one epoch. In this case I chose a random set of 75% of the equations, and trained only on the implications whose lhs and rhs are both contained in that fixed set of equations. I'm surprised that the accuracy is that high, to be honest. The model seems to be learning <em>something</em>. I'm going to try again with only 25% of the equations to see what happens.</p>",
        "id": 478121925,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729534455
    },
    {
        "content": "<p>Interesting</p>",
        "id": 478128248,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1729536783
    },
    {
        "content": "<p>To clarify, it ended up being ~97% accuracy both on the implications which involve some equation from the remaining 25%, and also on the implications which <em>only</em> involve equations from the remaining 25%.</p>",
        "id": 478129338,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729537161
    },
    {
        "content": "<p>The fact that it scored a similar amount on each is impressive! That makes it sound like it really didn't overfit much at all.<br>\nSo this is a transformer that you trained to give meaningful embeddings (= embeddings that could give good predictions), and it produced meaningful embeddings on the remaining equations too?</p>\n<p>How big is the embedding dimension? Can you give us an example of what are some \"maximally distinct\" equations (by some metric of, far-apart embeddings)?</p>",
        "id": 478172756,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1729554839
    },
    {
        "content": "<p>Hi all. I uploaded the data I have been using to huggingface. It can be found here: <a href=\"https://huggingface.co/datasets/adamtopaz/equational_dataset\">https://huggingface.co/datasets/adamtopaz/equational_dataset</a></p>\n<p>The <code>README.md</code> has a description of how this data was generated, and how the training/validation/testing split was created.</p>",
        "id": 478996144,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729894998
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478121925\">ha dicho</a>:</p>\n<blockquote>\n<p>In this case I chose a random set of 75% of the equations, and trained only on the implications whose lhs and rhs are both contained in that fixed set of equations.</p>\n</blockquote>\n<p>How do you compare your results with ours? With just 1% of the pairs one can get 99.3% accuracy, without embedding.</p>",
        "id": 478998851,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729896694
    },
    {
        "content": "<p>Can you say how you chose that 1%? Is it just a random 1% across all implications?</p>",
        "id": 478998950,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729896768
    },
    {
        "content": "<p>If so, then the data seems to be fundamentally different</p>",
        "id": 478998965,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729896785
    },
    {
        "content": "<p>I would be curious to see how your CNN model does on the split in terms of equations as opposed to randomly choosing one implications</p>",
        "id": 478999057,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729896847
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478998965\">ha dicho</a>:</p>\n<blockquote>\n<p>If so, then the data seems to be fundamentally different</p>\n</blockquote>\n<p>Yes, perhaps you had already thought of a way of comparison.</p>",
        "id": 478999100,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729896878
    },
    {
        "content": "<p>No, I haven’t. I don’t see any reasonable way to compare the two models</p>",
        "id": 478999143,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729896913
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478999057\">ha dicho</a>:</p>\n<blockquote>\n<p>I would be curious to see how your CNN model does on the split in terms of equations as opposed to randomly choosing one implications</p>\n</blockquote>\n<p>We will try to follow your recipe in our model (I will convert your files to our format). Thank you for sharing your data!</p>",
        "id": 478999855,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729897342
    },
    {
        "content": "<p>I’m also working on cleaning up my model code and making it more user friendly. I’ll share that as well when it’s ready.</p>",
        "id": 479000088,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729897499
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479000088\">schrieb</a>:</p>\n<blockquote>\n<p>I’m also working on cleaning up my model code and making it more user friendly. I’ll share that as well when it’s ready.</p>\n</blockquote>\n<p>Are you fine tuning an existing one? (if yes, which one) Or training from scratch?</p>",
        "id": 479026089,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729924097
    },
    {
        "content": "<p>No, I didn’t fine tune anything. I just made a transformer in PyTorch. It’s quite small (only about 1mb to store the weights).</p>",
        "id": 479060554,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729958497
    },
    {
        "content": "<p>(And I trained it on my laptop, which has an underpowered 4gb discrete gpu.)</p>",
        "id": 479067100,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1729965117
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479067100\">schrieb</a>:</p>\n<blockquote>\n<p>(And I trained it on my laptop, which has an underpowered 4gb discrete gpu.)</p>\n</blockquote>\n<p>I know this pain. Looking forward to testing it. <br>\nIn the meantime, I have a prompt-completion implication dataset, but I was looking into fine tuning o1-mini, which requires the chat format. Well, perhaps I'll just generate a synthetic dataset for that.</p>",
        "id": 479073797,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729971049
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479060554\">said</a>:</p>\n<blockquote>\n<p>No, I didn’t fine tune anything. I just made a transformer in PyTorch. It’s quite small (only about 1mb to store the weights).</p>\n</blockquote>\n<p>How did you encode the Magma?</p>",
        "id": 479109608,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1730011203
    },
    {
        "content": "<p>Does it matter if the test/validation overlap? Also, this is just the data set, there's no way to run inference on the model ourselves, right?</p>",
        "id": 479109957,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1730011587
    },
    {
        "content": "<p>In my case, if I fine tune something, I'll make the dataset open. Will just share it here.<br>\nBtw. we could also use something like W&amp;B for managing the ml lifecycle (or use tools like Vertex or Sagemaker). We'd then have all such experiments very well documented and structured.</p>",
        "id": 479120553,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730022797
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"266304\">Siddhartha Gadgil</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479109608\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479060554\">said</a>:</p>\n<blockquote>\n<p>No, I didn’t fine tune anything. I just made a transformer in PyTorch. It’s quite small (only about 1mb to store the weights).</p>\n</blockquote>\n<p>How did you encode the Magma?</p>\n</blockquote>\n<p>This is roughly explained in the dataset readme. Here are some more details: Magma expressions are serialized using prefix notation. The LHS and RHS of an equation are tokenized, appended, padded as needed, and the model gets the padded tokens, it computes (learned, absolute) positional embeddings.  It also gets information about which tokens correspond to the LHS and which to the RHS (and which to padding) -- I call these \"kinds\".</p>",
        "id": 479130470,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730032723
    },
    {
        "content": "<p>Also, for training, there are some transformations being done: variable names are randomly shuffled, and the LHS/RHS of an equation are flipped randomly with 0.5 probability.</p>",
        "id": 479130597,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730032840
    },
    {
        "content": "<p>Anyway, I tried to make the code as usable as possible and put it here: <a href=\"https://github.com/adamtopaz/equational_transformer\">https://github.com/adamtopaz/equational_transformer</a><br>\nNote that this was still only ever tested on my own hardware, so if you try to run this, and run into issues, please do let me know (by DM, so that this channel doesn't get too noisy).</p>",
        "id": 479130731,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730032981
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"243562\">@Adam Topaz</span> will try it out. Thank you for the sharing the repo.</p>",
        "id": 479131082,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730033353
    },
    {
        "content": "<p>If you just want to use the model, I suggest looking at the <code>play.ipynb</code> notebook first.</p>",
        "id": 479131121,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730033390
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479131121\">schrieb</a>:</p>\n<blockquote>\n<p>If you just want to use the model, I suggest looking at the <code>play.ipynb</code> notebook first.</p>\n</blockquote>\n<p>Yeah, will skip the pretraining for now. Great stuff!</p>",
        "id": 479131230,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730033458
    },
    {
        "content": "<p>Didn't know about <code>lake exe tokenized_data generate -h</code>. That's pretty cool. <br>\nChecked out the notebook, the dataset on HF, etc. Was thinking of training a transformer too, but I guess there's no need to do double work.<br>\nAre you planning to upload the model to HF, too? We could have a public colab.</p>",
        "id": 479158116,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730056931
    },
    {
        "content": "<p>The <code>tokenized_data</code> exe is only in the fork of the repo ( I could PR it to the main repo, but it’s not clear there is enough interest.)</p>",
        "id": 479158584,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730057349
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479158584\">schrieb</a>:</p>\n<blockquote>\n<p>The <code>tokenized_data</code> exe is only in the fork of the repo ( I could PR it to the main repo, but it’s not clear there is enough interest.)</p>\n</blockquote>\n<p>Since you already wrote the code, and pre-training works, I think it's not necessary. We can use your pretrained model. <br>\nBut it'd be really cool if you could upload the model to HF. <br>\nThat'd make many things easier (we could deploy it to Spaces, SageMaker or Azure ML). And have a very simple, no-installation-required colab for testers.</p>",
        "id": 479159563,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730058229
    },
    {
        "content": "<p>Ok. I’ll try to do that at some point soon.</p>",
        "id": 479160036,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730058616
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"448405\">Alex Meiburg</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478172756\">said</a>:</p>\n<blockquote>\n<p>The fact that it scored a similar amount on each is impressive! That makes it sound like it really didn't overfit much at all.<br>\nSo this is a transformer that you trained to give meaningful embeddings (= embeddings that could give good predictions), and it produced meaningful embeddings on the remaining equations too?</p>\n<p>How big is the embedding dimension? Can you give us an example of what are some \"maximally distinct\" equations (by some metric of, far-apart embeddings)?</p>\n</blockquote>\n<p>To follow up on this, I computed the top three principal components of the embeddings, and plotted them. It seems that there are four distinct \"chunks\" but some additional structure within two of the chunks. If you want to see the visualization, here it is: <br>\n<a href=\"/user_uploads/3121/2pN43__dZNC8ON9y7HkxAn6p/equation_embeddings.html\">equation_embeddings.html</a><br>\nAnd if you want to play with the embeddings on your own, here they are: <br>\n<a href=\"/user_uploads/3121/blPsMq-39pAZXY1LpL8Z7QMe/embeddings.pickle\">embeddings.pickle</a> (this is a dictionary with keys of the form \"EquatuionN\" and value a numpy vector of the 64-dimensional embedding)</p>",
        "id": 479724771,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730306798
    },
    {
        "content": "<p>(A direct URL if you don't want to download the html file directly: <a href=\"https://sites.ualberta.ca/~topaz/equation_embeddings.html\">https://sites.ualberta.ca/~topaz/equation_embeddings.html</a> )</p>",
        "id": 479725222,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730306971
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/479724771\">schrieb</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"448405\">Alex Meiburg</span> <a href=\"#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results/near/478172756\">said</a>:</p>\n<blockquote>\n<p>The fact that it scored a similar amount on each is impressive! That makes it sound like it really didn't overfit much at all.<br>\nSo this is a transformer that you trained to give meaningful embeddings (= embeddings that could give good predictions), and it produced meaningful embeddings on the remaining equations too?</p>\n<p>How big is the embedding dimension? Can you give us an example of what are some \"maximally distinct\" equations (by some metric of, far-apart embeddings)?</p>\n</blockquote>\n<p>To follow up on this, I computed the top three principal components of the embeddings, and plotted them. It seems that there are four distinct \"chunks\" but some additional structure within two of the chunks. If you want to see the visualization, here it is: <br>\n<a href=\"/user_uploads/3121/2pN43__dZNC8ON9y7HkxAn6p/equation_embeddings.html\">equation_embeddings.html</a><br>\nAnd if you want to play with the embeddings on your own, here they are: <br>\n<a href=\"/user_uploads/3121/blPsMq-39pAZXY1LpL8Z7QMe/embeddings.pickle\">embeddings.pickle</a> (this is a dictionary with keys of the form \"EquatuionN\" and value a numpy vector of the 64-dimensional embedding)</p>\n</blockquote>\n<p>Awesome! Based on your pickle file- </p>\n<p><a href=\"/user_uploads/3121/A3G2qtboZj-fX69SiniFjJnT/pickle1.png\">embeddings</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/A3G2qtboZj-fX69SiniFjJnT/pickle1.png\" title=\"embeddings\"><img data-original-dimensions=\"1693x1409\" src=\"/user_uploads/thumbnail/3121/A3G2qtboZj-fX69SiniFjJnT/pickle1.png/840x560.webp\"></a></div>",
        "id": 479726376,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730307369
    },
    {
        "content": "<p>yeah, that's the projection onto the x-y axis of the 3d plot above.</p>",
        "id": 479726459,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1730307408
    },
    {
        "content": "<p>Btw. the old umap also in 3d: <a href=\"#narrow/channel/458659-Equational/topic/3.20categories.20in.20UMAP.20after.20tokenization.20of.20theorems/near/476903869\">https://leanprover.zulipchat.com/#narrow/channel/458659-Equational/topic/3.20categories.20in.20UMAP.20after.20tokenization.20of.20theorems/near/476903869</a></p>\n<p>2d looks perhaps clearer?</p>",
        "id": 479727209,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730307648
    },
    {
        "content": "<p>And here's the one more.<br>\n<a href=\"/user_uploads/3121/1tk3FB_Ryg7PPm3m6nlNnBIc/pickle2.png\">pickle2.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/1tk3FB_Ryg7PPm3m6nlNnBIc/pickle2.png\" title=\"pickle2.png\"><img data-original-dimensions=\"1909x1722\" src=\"/user_uploads/thumbnail/3121/1tk3FB_Ryg7PPm3m6nlNnBIc/pickle2.png/840x560.webp\"></a></div>",
        "id": 479727337,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1730307699
    },
    {
        "content": "<p>Might be of interest here too <a class=\"stream-topic\" data-stream-id=\"458659\" href=\"/#narrow/channel/458659-Equational/topic/Graph.20ML.3A.20Directed.20link.20prediction.20on.20the.20implication.20graph\">#Equational &gt; Graph ML: Directed link prediction on the implication graph</a></p>",
        "id": 481586539,
        "sender_full_name": "Pietro Monticone",
        "timestamp": 1731264248
    }
]