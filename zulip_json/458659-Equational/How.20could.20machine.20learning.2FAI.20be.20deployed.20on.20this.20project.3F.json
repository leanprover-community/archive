[
    {
        "content": "<p>In my recent blog post on this project, I noted that thus far machine learning/modern AI tools have only made peripheral contributions to the project, e.g., in writing supporting code.  After talking a bit with <span class=\"user-mention\" data-user-id=\"556875\">@Pietro Monticone</span> about this, we decided to open a discussion on possible ways in which these methods might be useful for this project.  Some preliminary suggestions we came up with were</p>\n<ul>\n<li>Requesting access to APIs to advanced AI tools </li>\n<li>Testing out ML-based Lean-compatible tools such as LeanCopilot, ImProver, or LeanAgent (see also the survey at <a href=\"https://arxiv.org/abs/2404.09939\">https://arxiv.org/abs/2404.09939</a> )</li>\n<li>Are there any graph-ML methods that might be relevant?</li>\n<li>Creating a benchmark of implications specifically designed for testing out ML-based tools</li>\n</ul>\n<p>Any other suggestions would be welcome.</p>\n<p>p.s. Is there some way to crosspost this thread to the \"Machine Learning for Theorem Proving\" channel?</p>",
        "id": 476566745,
        "sender_full_name": "Terence Tao",
        "timestamp": 1728784092
    },
    {
        "content": "<p>This is mostly already covered in your point (2), but assistance in converting a standard human-written proof into valid Lean code. Lean has a learning curve as steep as a brick wall, so anything to help someone who might not be super comfortable (or knows nothing at all) contribute could help expand the vision of allowing people with less background help. </p>\n<p>An additional (albeit non-sexy) use case is <a href=\"http://[The%2015%20Top%20AI-Powered%20Tools%20For%20Automated%20Unit%20Testing%20(forbes.com)](https://www.forbes.com/sites/technology/article/unit-testing/)\">unit test writing</a> which can help ensure that as more contributions are made they don't break previous work. This could be critical especially if the code used here is used as a template for future projects</p>",
        "id": 476574998,
        "sender_full_name": "Joe McCann",
        "timestamp": 1728794128
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/stream/458659-Equational/topic/How.20could.20machine.20learning.2FAI.20be.20deployed.20on.20this.20project.3F/near/476566745\">ha dicho</a>:</p>\n<blockquote>\n<p>In my recent blog post on this project, I noted that thus far machine learning/modern AI tools have only made peripheral contributions to the project, e.g., in writing supporting code.  <br>\n[...]<br>\nAny other suggestions would be welcome.</p>\n</blockquote>\n<p>I had some plans for this ( but I didn't realize how fast the project would develop!). I'm mostly interested in the application of ML tools to try to prove implications/anti-implications. What I'd like to do is:<br>\n1) In a first step, to run different kinds of ML methods (decision trees, random forests, neural networks...)  in (a big portion of) the current database of pairs of identities, and test the efficiency of the classification predictions (in a test sample). Then get predictions about pairs yet to solve.<br>\n2) After that, apply methods of explainable AI (XAI) to the different models, to see what patterns are explaining the different outcomes, then try to translate them to theorems. E.g., I expect a deep enough neural network should be able to find our metatheorems about elements at the extreme of an identity, etc. but should also pick up subtler patterns (if they exist). Here it will be important to discard hallucinations.<br>\n3) If possible, try to make the AI produce Lean code from the patterns, and see if it succeeds.</p>\n<p>I have some basic knowledge on how to do 1) and 2) (btw, we could use the Kaggle platform for this), but since I could easily mess up, my plan is to contact tomorrow (Monday) with an expert in XAI I know in my university. I think he will be interested; he is an expert in SHAP (the most successful XAI method at present) and has modified it to apply it in new contexts. I will update you on this (although I don't know if this is really needed; the collective of people in this project has a tremendous range of skills and wit!).</p>\n<p>About 3), I don't know if my colleague will be able to help, or if he knows someone else.</p>",
        "id": 476597058,
        "sender_full_name": "Jose Brox",
        "timestamp": 1728816583
    },
    {
        "content": "<p>If you can embed access to the model as a lean tactic then you could be sure that the resultant proof is correct</p>",
        "id": 476598002,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1728817623
    },
    {
        "content": "<p>If lean says it is. I have added external checkers for good measure</p>",
        "id": 476598054,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1728817664
    },
    {
        "content": "<p>If there would be something like alpha-proof tool chain available but in a more interactive fashion I think that would really help. One could formulate problems in natural language and use alpha-zero to solve this overnight. For that something similar like  Leela Chess Zero would need to exist and maybe some specialized instances of LLMs (which is quite similar to the LeanCopilot problem.) The AlphaZero model would benefit from the distributed computation efforts (lean proofs made by the model).</p>",
        "id": 476799745,
        "sender_full_name": "Tim",
        "timestamp": 1728915809
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 476801120,
        "sender_full_name": "Tim",
        "timestamp": 1728916273
    },
    {
        "content": "<p>(I am using Cursor with Lean and Claude. And O1 in the browser. I've been using LeanCopilot and read the LeanAgent paper.)</p>\n<ul>\n<li>I believe that enabling those lifelong learners through deployment, tooling and UX can make a huge difference (it has already solved 162 unsolved cases and it'll most likely be millions very soon).</li>\n<li>As for graphml, please have a look at: <a href=\"https://huggingface.co/blog/intro-graphml\">https://huggingface.co/blog/intro-graphml</a></li>\n<li>I was looking into graph transformers; check out <a href=\"https://github.com/microsoft/Graphormer/\">https://github.com/microsoft/Graphormer/</a></li>\n</ul>",
        "id": 476805610,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728917780
    },
    {
        "content": "<p>Also, how about turning mathlib into a knowledge base using something like node2vec, and then adding curriculum learning agents on top of it?</p>\n<p>It'd work like this: </p>\n<ol>\n<li>We'd need to turn the mathematical objects / relations into a graph,</li>\n<li>We'd store the graph in a graph db (the edges could come from lean-enabled scanners, or from llms),</li>\n<li>We could then use something like node2vec or graph transformers (see <a href=\"https://arxiv.org/pdf/2407.09777\">https://arxiv.org/pdf/2407.09777</a>)</li>\n</ol>",
        "id": 476806841,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728918150
    },
    {
        "content": "<p>I think it would be interesting if machine learning could be used in coming up with novel examples of magmas that settle open non-implications.</p>",
        "id": 476850037,
        "sender_full_name": "Anand Rao Tadipatri",
        "timestamp": 1728938836
    },
    {
        "content": "<p>One could imagine some adaptation of the \"<a href=\"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/\">FunSearch</a>\" approach (where the magmas would be procedurally generated by LLM-generated Python code or something) possibly being effective.</p>",
        "id": 476851225,
        "sender_full_name": "Terence Tao",
        "timestamp": 1728939528
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"672170\">@Michael Bucko</span> What are the 162 cases solved by LeanAgent? That sounds interesting.</p>",
        "id": 476947890,
        "sender_full_name": "Fan Zheng",
        "timestamp": 1728985239
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"562644\">Fan Zheng</span> <a href=\"#narrow/stream/458659-Equational/topic/How.20could.20machine.20learning.2FAI.20be.20deployed.20on.20this.20project.3F/near/476947890\">schrieb</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"672170\">Michael Bucko</span> What are the 162 cases solved by LeanAgent? That sounds interesting.</p>\n</blockquote>\n<p>From the paper, \" LeanAgent proves 162 sorry theorems across 23 Lean repositories, including from challenging mathematics, highlighting its potential to assist in formalizing complex proofs across multiple domains. For example, LeanAgent successfully proves advanced sorry theorems from the PFR repository and proves challenging theorems in abstract algebra and algebraic topology. It outperforms the ReProver baseline by up to 11×, progressively learning from basic to highly complex mathematical concepts. Moreover, LeanAgent shows significant performance in forgetting measures and backward transfer, achieving a near-perfect composite score of 94%. This explains its continuous generalizability and continuous improvement.\"</p>",
        "id": 476950573,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728986151
    },
    {
        "content": "<p>There is a discussion about LeanAgent here: <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanAgent\">#Machine Learning for Theorem Proving &gt; LeanAgent</a></p>",
        "id": 476951253,
        "sender_full_name": "Rémy Degenne",
        "timestamp": 1728986336
    },
    {
        "content": "<p>Anyone tried out this or its techniques used for search of proofs in the project: <a href=\"https://github.com/deepseek-ai/DeepSeek-Prover-V1.5\">https://github.com/deepseek-ai/DeepSeek-Prover-V1.5</a></p>",
        "id": 476967095,
        "sender_full_name": "Tim",
        "timestamp": 1728991853
    },
    {
        "content": "<p>No, I don’t think so.</p>",
        "id": 476969711,
        "sender_full_name": "Pietro Monticone",
        "timestamp": 1728992764
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"761825\">Jose Brox</span> <a href=\"#narrow/stream/458659-Equational/topic/How.20could.20machine.20learning.2FAI.20be.20deployed.20on.20this.20project.3F/near/476597058\">ha dicho</a>:</p>\n<blockquote>\n<p>My plan is to contact tomorrow (Monday) with an expert in XAI I know in my university. I think he will be interested; he is an expert in SHAP (the most successful XAI method at present) and has modified it to apply it in new contexts. I will update you on this (although I don't know if this is really needed; the collective of people in this project has a tremendous range of skills and wit!).</p>\n</blockquote>\n<p>We have started this morning (I have briefed my colleague). We will start doing the fastest thing we can do, which is to apply already existing models for text recognition (to pairs of equations together with their implication status, not looking at the proofs) and see what happens. Next we will train our own models for this same problem (we expect this shouldn't take too long with this data). These we can do in a few days. If the success ratio is high, we will apply XAI to try to understand the patterns behind the predictions (this will take sensibly more time).</p>\n<p>There are details that worry me, like how much \"obvious knowledge\" we can detract from the database: if we apply transitivity, duality, etc. then we can get biased patterns (e.g. the model doesn't know a priori about invariance under change of variables).</p>",
        "id": 476971417,
        "sender_full_name": "Jose Brox",
        "timestamp": 1728993375
    },
    {
        "content": "<p>We think between the two of us we don't have the skills and resources (including time needed) to apply LLMs for the high-level applications that are being discussed in the project, we could try to form a group in our university, which has an AI Center (and we can access a supercomputer if needed), but perhaps there already are people better suited to do this close to the project. We think that LLMs are needed in order to predict a proof for a new pair from the proof of the training pairs.</p>",
        "id": 476972294,
        "sender_full_name": "Jose Brox",
        "timestamp": 1728993662
    },
    {
        "content": "<p>what's XAI? Explainable AI?</p>",
        "id": 476972437,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1728993706
    },
    {
        "content": "<p>In the context of this project, what would explainability look like?</p>",
        "id": 476972647,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1728993773
    },
    {
        "content": "<p>What would the AI produce as explanations?</p>",
        "id": 476972666,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1728993782
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/458659-Equational/topic/How.20could.20machine.20learning.2FAI.20be.20deployed.20on.20this.20project.3F/near/476972666\">ha dicho</a>:</p>\n<blockquote>\n<p>What would the AI produce as explanations?</p>\n</blockquote>\n<p>For example, for the equations-only (no proofs) models, asking for the explanation of a neural network why a particular pair of equations (a=b,c=d) is an anti-implication, it could highlight the leftmost variables of a,b,c,d and \"show\" that they are equal in a,b but different in c,d. For decision trees it could be different. It is also important that XAI can be applied to different depths, so that for example an intermediate layer of the neural network may discriminate by the number of variables in each term, etc.</p>",
        "id": 476973527,
        "sender_full_name": "Jose Brox",
        "timestamp": 1728994069
    },
    {
        "content": "<p>See <a class=\"stream-topic\" data-stream-id=\"458659\" href=\"/#narrow/channel/458659-Equational/topic/Graph.20ML.3A.20Directed.20link.20prediction.20on.20the.20implication.20graph\">#Equational &gt; Graph ML: Directed link prediction on the implication graph</a></p>",
        "id": 481584719,
        "sender_full_name": "Pietro Monticone",
        "timestamp": 1731262756
    }
]