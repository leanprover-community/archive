[
    {
        "content": "<p>which, if not all of them, of the independent pairs of equations can be proven so by an invariant that one preserves and the other changes?</p>",
        "id": 477257452,
        "sender_full_name": "Jared green",
        "timestamp": 1729096702
    },
    {
        "content": "<p>All of them, if \"being true in a magma\" counts as an \"invariant\". I would argue it kind of does, e.g. \"number of <code>x</code>-es\" is captured by the free-commutative magma.</p>",
        "id": 477259515,
        "sender_full_name": "Cody Roux",
        "timestamp": 1729097502
    },
    {
        "content": "<p>thats not the kind of invariant i mean. i mean some a priori metric with a linear order.</p>",
        "id": 477282305,
        "sender_full_name": "Jared green",
        "timestamp": 1729105615
    },
    {
        "content": "<p>As <span class=\"user-mention\" data-user-id=\"116028\">@Cody Roux</span> pointed out, technically any anti-implication \"EquationX does not imply EquationY\" can be disproven by evaluating both sides of EquationX and both sides of EquationY in the free magma modulo equation X; both sides will necessarily agree for the former and necessarily disagree for the latter, by the universal property.  One can then impose some linear ordering on that free magma to make this an \"a priori metric with a linear order\", though it is admittedly an artificial one in most cases.</p>\n<p>That said, some large files of automatically generated anti-implications using invariants were created in <a href=\"https://github.com/teorth/equational_theories/pull/389\">equational#389</a> (see also <a href=\"https://github.com/teorth/equational_theories/pull/526\">equational#526</a>).  One could possibly make some full runs here to see exactly how many of the ~14 million anti-implications can be picked up by \"simple\" invariants.</p>",
        "id": 477285640,
        "sender_full_name": "Terence Tao",
        "timestamp": 1729106948
    },
    {
        "content": "<p>ordering on terms, not elements.</p>",
        "id": 477291047,
        "sender_full_name": "Jared green",
        "timestamp": 1729108942
    },
    {
        "content": "<p>(they wouldnt be equations if the elements were different, right?)</p>",
        "id": 477291220,
        "sender_full_name": "Jared green",
        "timestamp": 1729109015
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/stream/458659-Equational/topic/independent.20equations.20and.20invariants/near/477285640\">ha dicho</a>:</p>\n<blockquote>\n<p>As <span class=\"user-mention silent\" data-user-id=\"116028\">Cody Roux</span> pointed out, technically any anti-implication \"EquationX does not imply EquationY\" can be disproven by evaluating both sides of EquationX and both sides of EquationY in the free magma modulo equation X; both sides will necessarily agree for the former and necessarily disagree for the latter, by the universal property.  One can then impose some linear ordering on that free magma to make this an \"a priori metric with a linear order\", though it is admittedly an artificial one in most cases.</p>\n<p>That said, some large files of automatically generated anti-implications using invariants were created in <a href=\"https://github.com/teorth/equational_theories/pull/389\">equational#389</a> (see also <a href=\"https://github.com/teorth/equational_theories/pull/526\">equational#526</a>).  One could possibly make some full runs here to see exactly how many of the ~14 million anti-implications can be picked up by \"simple\" invariants.</p>\n</blockquote>\n<p>In addition, if the machine learning models turn out to produce a high success ratio, then they problably will be applying some invariants (specially the neural network ones) which we could later try to extract (we will see, we are on it).</p>",
        "id": 477296688,
        "sender_full_name": "Jose Brox",
        "timestamp": 1729111209
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"761825\">Jose Brox</span> <a href=\"#narrow/stream/458659-Equational/topic/independent.20equations.20and.20invariants/near/477296688\">schrieb</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"657719\">Terence Tao</span> <a href=\"#narrow/stream/458659-Equational/topic/independent.20equations.20and.20invariants/near/477285640\">ha dicho</a>:</p>\n<blockquote>\n<p>As <span class=\"user-mention silent\" data-user-id=\"116028\">Cody Roux</span> pointed out, technically any anti-implication \"EquationX does not imply EquationY\" can be disproven by evaluating both sides of EquationX and both sides of EquationY in the free magma modulo equation X; both sides will necessarily agree for the former and necessarily disagree for the latter, by the universal property.  One can then impose some linear ordering on that free magma to make this an \"a priori metric with a linear order\", though it is admittedly an artificial one in most cases.</p>\n<p>That said, some large files of automatically generated anti-implications using invariants were created in <a href=\"https://github.com/teorth/equational_theories/pull/389\">equational#389</a> (see also <a href=\"https://github.com/teorth/equational_theories/pull/526\">equational#526</a>).  One could possibly make some full runs here to see exactly how many of the ~14 million anti-implications can be picked up by \"simple\" invariants.</p>\n</blockquote>\n<p>In addition, if the machine learning models turn out to produce a high success ratio, then they problably will be applying some invariants (specially the neural network ones) which we could later try to extract (we will see, we are on it).</p>\n</blockquote>\n<p>Two ideas. </p>\n<ol>\n<li>\"Global e-graph-AI loop\": Perhaps LLMs bootstrapped with human input / dataset (we'd feed them with implications and anti-implications, for a given set of equations) and tools could iteratively generate e-graphs the way we train models (perhaps in parallel - think forking them and merging), and them we could use them to further train those LLMs. E-graph (global) saturation would be a function of LLM's ability to explore the problem space. At first, there'll be a few e-graphs for a couple of projects. Eventually, one would be able to merge them the same way we merge ML models.</li>\n<li>\"E-graph diffuser\": A diffusion model could potentially iteratively \"dream\" an e-graph (the same ways it comes up with 3d games that people can play). That's a different approach to the problem, but very inspiring. Diffusion models evolve (different parametrization techniques, incl. the recent energy-based approaches). They've been successful in multiple fields and could be used to develop their own intuition in a slightly different way.</li>\n</ol>",
        "id": 477302766,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729113755
    }
]