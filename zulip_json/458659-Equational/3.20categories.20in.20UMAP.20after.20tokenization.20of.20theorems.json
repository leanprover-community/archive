[
    {
        "content": "<p>I tokenized the theorems (as of Oct 14th) (sentences truncated to 512, used bert-base-uncased), and then used UMAP to visualize the result. </p>\n<p>I see three categories (points: 245, dimension: 768).<br>\n<a href=\"/user_uploads/3121/SCVENezmxHm2edSKchTgA1CY/Bildschirmfoto-2024-10-15-um-07.46.07.png\">Bildschirmfoto 2024-10-15 um 07.46.07.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/SCVENezmxHm2edSKchTgA1CY/Bildschirmfoto-2024-10-15-um-07.46.07.png\" title=\"Bildschirmfoto 2024-10-15 um 07.46.07.png\"><img data-original-dimensions=\"1394x1120\" src=\"/user_uploads/thumbnail/3121/SCVENezmxHm2edSKchTgA1CY/Bildschirmfoto-2024-10-15-um-07.46.07.png/840x560.webp\"></a></div><p>Do we know how to interpret these categories? I'd be happy to create a PR for this.</p>",
        "id": 476903869,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728972006
    },
    {
        "content": "<p>I think you're talking looking at the actual theorem bodies in text? Without a mapping of the points to particular theorems there is no way to interpret if it's interesting or just an artifact. Most theorems are auto-generated by only a few methods so it would make sense that they cluster.</p>",
        "id": 476905295,
        "sender_full_name": "Vlad Tsyrklevich",
        "timestamp": 1728972477
    },
    {
        "content": "<p>Yes, the actual theorem bodies. 245 points in 768 dimensions. </p>\n<p>I am just wondering why 3 clusters and not 20 (esp given their sizes). I was trying to explore some unexpected patterns. Perhaps there're some hidden ways, or higher-order concepts in the way we're proving things.</p>\n<p>I once did a similar analysis based on k-prime numbers and got this (only computed for the first 10,000 numbers -- the compute would not allow more back in the day).<br>\n<a href=\"/user_uploads/3121/TlRGgJHQCYnhp_kc3Q7KucaV/Bildschirmfoto-2024-10-15-um-08.13.47.png\">Bildschirmfoto 2024-10-15 um 08.13.47.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/TlRGgJHQCYnhp_kc3Q7KucaV/Bildschirmfoto-2024-10-15-um-08.13.47.png\" title=\"Bildschirmfoto 2024-10-15 um 08.13.47.png\"><img data-original-dimensions=\"976x942\" src=\"/user_uploads/thumbnail/3121/TlRGgJHQCYnhp_kc3Q7KucaV/Bildschirmfoto-2024-10-15-um-08.13.47.png/840x560.webp\"></a></div>",
        "id": 476906925,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728973028
    },
    {
        "content": "<p>Why is there a 0 in the label?</p>",
        "id": 476966688,
        "sender_full_name": "Fan Zheng",
        "timestamp": 1728991697
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"562644\">Fan Zheng</span> <a href=\"#narrow/stream/458659-Equational/topic/3.20categories.20in.20UMAP.20after.20tokenization.20of.20theorems/near/476966688\">schrieb</a>:</p>\n<blockquote>\n<p>Why is there a 0 in the label?</p>\n</blockquote>\n<p>I'm getting all the token embeddings from the last hidden layer. If the input text contains the digit 0, it will be included as a token. Then I am aligning bert and spacy tokens. The script currently considers 0 as one of the words for which embeddings are calculated.</p>",
        "id": 476969617,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728992739
    },
    {
        "content": "<p>Instead of theorems and their text, one could use a similarity metric on the finite magmas and plot those. I think that might make for a picture with interesting features.<br>\nA potential similarity metric: cosine similarity between the ordered sets of satisfied equations. I guess one could also come up with some difference metric on the operation tables that accounts for isomorphism.</p>",
        "id": 476971604,
        "sender_full_name": "Zoltan A. Kocsis (Z.A.K.)",
        "timestamp": 1728993443
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"763356\">Zoltan A. Kocsis (Z.A.K.)</span> <a href=\"#narrow/stream/458659-Equational/topic/3.20categories.20in.20UMAP.20after.20tokenization.20of.20theorems/near/476971604\">schrieb</a>:</p>\n<blockquote>\n<p>Instead of theorems and their text, one could use a similarity metric on the finite magmas and plot those. I think that might make for a picture with interesting features.<br>\nA potential similarity metric: cosine similarity between the ordered sets of satisfied equations. I guess one could also come up with some difference metric on the operation tables that accounts for isomorphism.</p>\n</blockquote>\n<p>I am already trying to run the script on the zipped implications from Vlad, but not sure if my compute will be enough.</p>",
        "id": 476979652,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1728995920
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"562644\">Fan Zheng</span> <a href=\"#narrow/stream/458659-Equational/topic/3.20categories.20in.20UMAP.20after.20tokenization.20of.20theorems/near/476966688\">schrieb</a>:</p>\n<blockquote>\n<p>Why is there a 0 in the label?</p>\n</blockquote>\n<p>Here's the (very short) code for this:</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"err\">!</span><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">spacy</span> <span class=\"n\">transformers</span> <span class=\"n\">torch</span>\n<span class=\"err\">!</span><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">chardet</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"kn\">import</span> <span class=\"nn\">spacy</span>\n<span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">BertTokenizer</span><span class=\"p\">,</span> <span class=\"n\">BertModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">chardet</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n\n    <span class=\"c1\">#Spacy and BERT</span>\n    <span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'en_core_web_sm'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">BertModel</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">,</span> <span class=\"n\">output_hidden_states</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># I set it up in Colab</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">google.colab</span> <span class=\"kn\">import</span> <span class=\"n\">files</span>\n    <span class=\"n\">uploaded</span> <span class=\"o\">=</span> <span class=\"n\">files</span><span class=\"o\">.</span><span class=\"n\">upload</span><span class=\"p\">()</span>\n\n    <span class=\"n\">input_file</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">uploaded</span><span class=\"p\">))</span>\n\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">input_file</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">raw_data</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span>\n\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">chardet</span><span class=\"o\">.</span><span class=\"n\">detect</span><span class=\"p\">(</span><span class=\"n\">raw_data</span><span class=\"p\">)</span>\n    <span class=\"n\">encoding</span> <span class=\"o\">=</span> <span class=\"n\">result</span><span class=\"p\">[</span><span class=\"s1\">'encoding'</span><span class=\"p\">]</span>\n    <span class=\"n\">confidence</span> <span class=\"o\">=</span> <span class=\"n\">result</span><span class=\"p\">[</span><span class=\"s1\">'confidence'</span><span class=\"p\">]</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">confidence</span> <span class=\"o\">&gt;=</span> <span class=\"mf\">0.5</span> <span class=\"ow\">and</span> <span class=\"n\">encoding</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">raw_data</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">encoding</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Defaulting to 'utf-8' with errors handled.\"</span><span class=\"p\">)</span>\n        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">raw_data</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'utf-8'</span><span class=\"p\">,</span> <span class=\"n\">errors</span><span class=\"o\">=</span><span class=\"s1\">'replace'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n    <span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">sent</span><span class=\"o\">.</span><span class=\"n\">text</span> <span class=\"k\">for</span> <span class=\"n\">sent</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">sents</span><span class=\"p\">]</span>\n\n    <span class=\"n\">word_embeddings</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">word_counts</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n\n        <span class=\"n\">doc_sent</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n        <span class=\"n\">words_in_sent</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">text</span> <span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">doc_sent</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">is_punct</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">is_space</span><span class=\"p\">]</span>\n\n        <span class=\"c1\"># Tokenize the sentence with BERT tokenizer</span>\n        <span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span>\n            <span class=\"n\">sentence</span><span class=\"p\">,</span>\n            <span class=\"n\">return_tensors</span><span class=\"o\">=</span><span class=\"s1\">'pt'</span><span class=\"p\">,</span>\n            <span class=\"n\">add_special_tokens</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">max_length</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span>        <span class=\"c1\"># Set max length to 512 tokens</span>\n            <span class=\"n\">truncation</span><span class=\"o\">=</span><span class=\"kc\">True</span>        <span class=\"c1\"># Truncate</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"c1\"># Check if the sentence was truncated</span>\n        <span class=\"k\">if</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">model_max_length</span> <span class=\"ow\">and</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">[</span><span class=\"s1\">'input_ids'</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"o\">&gt;=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">model_max_length</span><span class=\"p\">:</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Warning: Sentence truncated to </span><span class=\"si\">{</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">model_max_length</span><span class=\"si\">}</span><span class=\"s2\"> tokens.\"</span><span class=\"p\">)</span>\n\n        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n            <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n            <span class=\"n\">hidden_states</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"o\">.</span><span class=\"n\">hidden_states</span>  <span class=\"c1\"># Tuple of 13 layers</span>\n\n        <span class=\"n\">token_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">hidden_states</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># Shape: [seq_len, hidden_size]</span>\n        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">convert_ids_to_tokens</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">[</span><span class=\"s1\">'input_ids'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>\n\n        <span class=\"n\">spacy_idx</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"n\">bert_word</span> <span class=\"o\">=</span> <span class=\"s1\">''</span>\n        <span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">):</span>\n            <span class=\"k\">if</span> <span class=\"n\">token</span> <span class=\"o\">==</span> <span class=\"s1\">'[CLS]'</span> <span class=\"ow\">or</span> <span class=\"n\">token</span> <span class=\"o\">==</span> <span class=\"s1\">'[SEP]'</span><span class=\"p\">:</span>\n                <span class=\"k\">continue</span>\n            <span class=\"k\">if</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">startswith</span><span class=\"p\">(</span><span class=\"s1\">'##'</span><span class=\"p\">):</span>\n                <span class=\"n\">bert_word</span> <span class=\"o\">+=</span> <span class=\"n\">token</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">:]</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"k\">if</span> <span class=\"n\">bert_word</span><span class=\"p\">:</span>\n                    <span class=\"n\">spacy_idx</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n                <span class=\"n\">bert_word</span> <span class=\"o\">=</span> <span class=\"n\">token</span>\n            <span class=\"k\">if</span> <span class=\"n\">spacy_idx</span> <span class=\"o\">&lt;</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">words_in_sent</span><span class=\"p\">):</span>\n                <span class=\"n\">word</span> <span class=\"o\">=</span> <span class=\"n\">words_in_sent</span><span class=\"p\">[</span><span class=\"n\">spacy_idx</span><span class=\"p\">]</span>\n                <span class=\"c1\"># Remove special characters and lowercase for comparison</span>\n                <span class=\"n\">cleaned_bert_word</span> <span class=\"o\">=</span> <span class=\"n\">bert_word</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n                <span class=\"n\">cleaned_word</span> <span class=\"o\">=</span> <span class=\"n\">word</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n                <span class=\"k\">if</span> <span class=\"n\">cleaned_bert_word</span> <span class=\"o\">==</span> <span class=\"n\">cleaned_word</span><span class=\"p\">:</span>\n                    <span class=\"n\">vector</span> <span class=\"o\">=</span> <span class=\"n\">token_embeddings</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n                    <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">word_embeddings</span><span class=\"p\">:</span>\n                        <span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">vector</span>\n                        <span class=\"n\">word_counts</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n                    <span class=\"k\">else</span><span class=\"p\">:</span>\n                        <span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">vector</span>\n                        <span class=\"n\">word_counts</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n    <span class=\"c1\"># Average the embeddings for each word</span>\n    <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">word_embeddings</span><span class=\"p\">:</span>\n        <span class=\"n\">word_embeddings</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"o\">/=</span> <span class=\"n\">word_counts</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Convert the embeddings to a TSV file for word2vec processing</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'output.tsv'</span><span class=\"p\">,</span> <span class=\"s1\">'w'</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'utf-8'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">tsv_file</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">word</span><span class=\"p\">,</span> <span class=\"n\">vector</span> <span class=\"ow\">in</span> <span class=\"n\">word_embeddings</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n            <span class=\"n\">vector_str</span> <span class=\"o\">=</span> <span class=\"s1\">'</span><span class=\"se\">\\t</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"p\">))</span>\n            <span class=\"n\">tsv_file</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'</span><span class=\"si\">{</span><span class=\"n\">word</span><span class=\"si\">}</span><span class=\"se\">\\t</span><span class=\"si\">{</span><span class=\"n\">vector_str</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Done\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">\"__main__\"</span><span class=\"p\">:</span>\n    <span class=\"n\">main</span><span class=\"p\">()</span>\n</code></pre></div>",
        "id": 477015719,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729006251
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"562644\">@Fan Zheng</span> btw. here's the colab: <a href=\"https://colab.research.google.com/drive/1GToDglCGggXKnFuIchz3SlASAVN2zmkA?usp=sharing\">https://colab.research.google.com/drive/1GToDglCGggXKnFuIchz3SlASAVN2zmkA?usp=sharing</a></p>",
        "id": 477017392,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729006683
    },
    {
        "content": "<p>Can't really run it in the colab env. Turned out to be too heavy. But I could re-run it (and optimize) with more compute.</p>",
        "id": 477471360,
        "sender_full_name": "Michael Bucko",
        "timestamp": 1729179092
    }
]