[
    {
        "content": "<p>I am experimenting with using Lean to represent the computations performed by potentially large programs originally written in other languages. In the process, I've come across what I think is an unnecessary inefficiency in elaborating and doing type class inference for large monadic blocks. I'm wondering whether anyone knows whether a) there's a way to write such things that's more efficient, or b) whether making this more efficient by optimizing Lean itself is likely to be feasible. To be more specific, I've been testing with the following program:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">set_option</span><span class=\"w\"> </span><span class=\"n\">maxRecDepth</span><span class=\"w\"> </span><span class=\"mi\">10000</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">AddALot</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">StateM</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">do</span>\n<span class=\"w\">  </span><span class=\"n\">set</span><span class=\"w\"> </span><span class=\"n\">x</span>\n<span class=\"w\">  </span><span class=\"n\">modifyGet</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"bp\">Î»</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"o\">((),</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"o\">))</span>\n<span class=\"w\">  </span><span class=\"bp\">&lt;</span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"n\">few</span><span class=\"w\"> </span><span class=\"n\">hundred</span><span class=\"w\"> </span><span class=\"n\">copies</span><span class=\"w\"> </span><span class=\"n\">of</span><span class=\"w\"> </span><span class=\"n\">the</span><span class=\"w\"> </span><span class=\"n\">previous</span><span class=\"bp\">&gt;</span>\n<span class=\"w\">    </span><span class=\"k\">let</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">get</span>\n<span class=\"w\">  </span><span class=\"n\">pure</span><span class=\"w\"> </span><span class=\"n\">y</span>\n\n<span class=\"bp\">#</span><span class=\"n\">eval</span><span class=\"w\"> </span><span class=\"n\">StateT</span><span class=\"bp\">.</span><span class=\"n\">run'</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">AddALot</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"mi\">0</span>\n</code></pre></div>\n<p>This is very much an artificial example. Real instances would have different statements. But it serves as an interesting benchmark. With Lean's default recursion depth limit, I first encounter an error about reaching the maximum recursion depth. If I increase it to the limit in the code above, I first get an error from macOS about exceeding the maximum stack depth. If I also increase that, it does successfully terminate, but uses more time and memory than I'd expect it should.</p>\n<p>Specifically:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>$ lake env time -l lean --profile BigDo.lean\ncompilation of AddALot took 616ms\nelaboration took 1.84s\n1026\ncumulative profiling times:\n    attribute application 0.00641ms\n    compilation 622ms\n    elaboration 1.84s\n    import 45.8ms\n    initialization 30.8ms\n    interpretation 9.98ms\n    linting 46.4ms\n    parsing 55.2ms\n    type checking 6.83ms\n    typeclass inference 712ms\n        3.46 real         3.28 user         0.12 sys\n           269041664  maximum resident set size\n          &lt;snip&gt;\n         13268910529  cycles elapsed\n           225353728  peak memory footprint\n</code></pre></div>\n<p>As you can see, elaboration and type class inference take the bulk of the time.</p>\n<p>I experimented with this after trying something similar in Coq, but using the <a href=\"https://github.com/DeepSpec/InteractionTrees/blob/master/theories/Core/ITreeDefinition.v#L30-L42\">itree monad</a>, which is substantially more complex, instead of the state monad. The Coq version, with an even larger number of monadic actions, checks in a fraction of a second.</p>\n<p>Does anyone here have thoughts about whether this is likely to be resolvable with a reasonable amount of effort?</p>",
        "id": 465264794,
        "sender_full_name": "Aaron Tomb",
        "timestamp": 1724703260
    },
    {
        "content": "<p><code>do</code> is a complex elaborator in Lean. From your link I'm assuming you're using neither <code>do</code> nor typeclasses in Coq, I would suggest to do the same in Lean if you have such large programs. You could define your own much simpler <code>do</code> notation on top if desired.</p>",
        "id": 465269393,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1724705437
    },
    {
        "content": "<p>True, that library defines its own custom syntax rather than using a generic <code>do</code>. I'm not sure why that hadn't already occurred to me, honestly!</p>",
        "id": 465272257,
        "sender_full_name": "Aaron Tomb",
        "timestamp": 1724706733
    }
]