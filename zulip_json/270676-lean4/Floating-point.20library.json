[
    {
        "content": "<p>Hello, I am trying to find a floating-point library in lean4, similar to the Flocq library in Coq. Could someone please point me to the library?</p>",
        "id": 401254285,
        "sender_full_name": "Mohit Tekriwal",
        "timestamp": 1699573177
    },
    {
        "content": "<p>Such a library does not currently exist to my knowledge. We do have built-in floating point numbers for program development at <a href=\"https://leanprover-community.github.io/mathlib4_docs/find/?pattern=Float#doc\">docs#Float</a> but not yet verified them.</p>",
        "id": 401254374,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1699573228
    },
    {
        "content": "<p>okay, thanks!</p>",
        "id": 401255196,
        "sender_full_name": "Mohit Tekriwal",
        "timestamp": 1699573873
    },
    {
        "content": "<p>Is someone in the LEAN community already working on it though?</p>",
        "id": 401434555,
        "sender_full_name": "Mohit Tekriwal",
        "timestamp": 1699648321
    },
    {
        "content": "<p>I am at least not aware of anything being publicly announced on here but that doesn't mean it doesn't exist.</p>",
        "id": 401434987,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1699648612
    },
    {
        "content": "<p>Not the same, but I’m working on fixed point intervals arithmetic based on UInt64 (so no new axioms).  It would be better to have IEEE floating point for my purposes, though, so +1 to floats being a great thing to do.</p>",
        "id": 401437278,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1699650088
    },
    {
        "content": "<p>(this is just me being ignorant, but are modern FPUs shown to be bug-free so we can actually assert that the hardware float instructions do always correspond with the IEEE definition?)</p>",
        "id": 401441084,
        "sender_full_name": "James Gallicchio",
        "timestamp": 1699653029
    },
    {
        "content": "<p>Hardware folk do a lot of verification these days, but (1) I don’t know if these extend to full formal SAT/SMT proofs or just lots of model checking and (2) unfortunately the circuit designs are proprietary so we don’t get to see those proofs.  However, we would have a choice about what instructions to assume compliant: the safest would be to go with just ring operations and not trust even division, though it’s not very ergonomic.</p>\n<p>One thing I find super annoying is that if one wants to do interval arithmetic, modern CPUs handle rounding modes via processor flags, and modern compilers proudly distain these flags and say they consider themselves free to reorder the flag changes across arithmetic operations.  This means that doing interval arithmetic safely requires nasty hacks.  Thus, if we ever want to use floats to do certified approximate computing, ball arithmetic using round-to-nearest is the better way to go.</p>\n<p>Nvidia GPUs are better here: they provide sane “round up / round down” versions of instructions.  But other GPUs including Apple’s do not, so there is no portability.</p>",
        "id": 401441617,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1699653507
    },
    {
        "content": "<p>I think given the above the right compromise for most math proof purposes is UInt64 (thus what I am doing).  But it would be nice to have the float theory for code that wants to be faster, and for program verification in float contexts.</p>",
        "id": 401441681,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1699653591
    },
    {
        "content": "<p>That's brutal :/ thanks for the detailed response</p>",
        "id": 401442570,
        "sender_full_name": "James Gallicchio",
        "timestamp": 1699654108
    },
    {
        "content": "<p>The typical move is to set the flag uniformly to “round up”, and store <code>[a,b]</code> as the tuple <code>(-a,b)</code> so that both directions of rounding are handled with the same processor mode (held fixed for a long period of time).  But this is not friendly at all for library code which has to interoperate with other stuff arbitrarily.</p>",
        "id": 401442824,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1699654333
    },
    {
        "content": "<p>I have some lemmas <a href=\"https://github.com/bustercopley/lean-float\">here</a>. The main result is <code>u + v = (u ⊝ (u ⊕ v)) ⊕ v</code>[1][2][3]. The assumptions are probably not in a particularly useful form, and there was never any intention to be well-structured, complete or suitable for any particular purpose. Moreover everything is in <code>ℕ</code>, so the results have to be stated and proved in several forms. Still, it kept me busy.</p>\n<p>[1] On Properties of Floating Point Arithmetics: Numerical Stability and the Cost of Accurate Computations, Douglas M. Priest, Ph.D. thesis, 1992 (pp. 14-19)<br>\n[2] A Floating-Point Technique for Extending the Available Precision, T. J. Dekker, Numerische Mathematik 18:224-242, 1971 (equation 4.4)<br>\n[3] The Art Of Computer Programming - Volume 2 (Seminumerical Algorithms), Donald E. Knuth, Third edition, 1997, Addison Wesley Longman (section 4.2.2)</p>\n<p>Priest's thesis is great. My results are not as general as his (I assume that, e.g., <code>u ⊕ v = f (u + v)</code> for some unspecified function <code>f</code> satisfying certain assumptions, where Priest makes no such assumption). I think the assumption holds for IEEE (in any rounding mode).</p>",
        "id": 401442839,
        "sender_full_name": "Richard Copley",
        "timestamp": 1699654363
    }
]