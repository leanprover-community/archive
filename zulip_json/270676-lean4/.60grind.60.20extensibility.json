[
    {
        "content": "<p>I'm a (very) amateur who likes checking in on Lean occasionally, and I've just discovered the work on <code>grind</code>, which looks very cool! I think I've read everything in the docs, but I'm trying to understand a few architectural things.</p>\n<ul>\n<li>It's stated that <code>grind</code> works with a fixed list of engines. Is it intended that we can add new engines in user code?</li>\n<li>One of the listed engines is \"a suite of satellite theory solvers\". Are these just small engines, or are satellite solvers some separate concept with a separate interface? Can these be added in user code?</li>\n</ul>\n<p>The language in the introduction in the docs might be taken as implying this set is fixed, but it also says that \"<code>[grind](https://leanprover-community.github.io/mathlib4_docs/Init/Grind/Tactics.html#Lean.Parser.Tactic.grind)</code> ships with an algebraic solver nick-named <strong><code>[ring](https://leanprover-community.github.io/mathlib4_docs/Init/Grind/Tactics.html#Lean.Grind.Config.ring)</code></strong>`, where \"ships\" suggests extensibility.</p>\n<p>Additionally, I'm curious whether the whiteboard can handle conjectures? That is to say, if some engine were to add an unproven conjecture to the whiteboard which it thinks is maybe true and maybe interesting, is it possible for <code>grind</code> to attempt to prove this conjecture either true or false (hopefully producing a lemma), and then utilise the result if it succeeds?</p>\n<p>This is cool stuff!</p>",
        "id": 535334383,
        "sender_full_name": "Sam",
        "timestamp": 1755702166
    },
    {
        "content": "<p>As far as I understand, <code>grind</code> is supposed to be extensible, so that we can add new engines/sattelite solvers ourselves. But since <code>grind</code> is very new, we are discouraged from doing this now already.</p>\n<p>For your question about the whiteboard, <code>grind</code> only puts things on the whiteboard by proving it, not by guessing that it is true. Although I guess you could say that when <code>grind</code> does a case-split, it \"adds the conjecture that the one case holds and adds the conjecture that the other case holds\" and then tries to prove the two cases?</p>",
        "id": 535338276,
        "sender_full_name": "Jovan Gerbscheid",
        "timestamp": 1755703155
    },
    {
        "content": "<p>While theoretically possible <code>grind</code> is currently not open for extension by users that are not comitting in core. I have seen the idea you are proposing be implemented by effectively adding <code>P or ~P</code> to the whiteboard equivalent and then letting the solver do a case split on that in another SMT solver called CVC4 so this is also possble in principle with grind.</p>",
        "id": 535338665,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1755703255
    },
    {
        "content": "<p>Whether it's a good idea is an entirely different question though, grind is not a high performance case distinction based solver, the idea is that it tries only very little case distinction because problems that have a lot of case distinction can suffer from proof stability. So even if this works in other tools that doesnt mean it should be applied to grind</p>",
        "id": 535339288,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1755703408
    },
    {
        "content": "<p>Interesting, thanks.</p>\n<p>I had the thought of adding the <code>P or ~P</code> but I wasn't sure whether any of the engines would perform the case split on that and attempt to prove and utilise one of the cases.</p>\n<p>My interest in this relates to machine learning based proving. I've thought for a while that a reasonable approach to that problem would be a whiteboard based system like <code>grind</code>, with classical solvers for all the common cases, and an \"ML model of last resort\" to add things to the whiteboard, such that the model does the minimal amount of work only when classical solvers can't make progress.</p>\n<p>A reasonable core operation for such a model might be to add a conjecture to the whiteboard. That operation would be \"easy\" (it doesn't have to produce a proof, just a syntactically/semantically correct expression) and non-destructive (it's fine for conjectures to be wrong).</p>\n<p>So I'm wondering whether it would be possible to add an engine which can be used to interface with such a model, as <code>grind</code> seems like a potentially good base to build from here.</p>",
        "id": 535341451,
        "sender_full_name": "Sam",
        "timestamp": 1755704018
    },
    {
        "content": "<p>Why would such an approach have to be directly integrated with grind? You could let that thing call grind, then add a conjecture etc no?</p>",
        "id": 535350027,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1755706626
    },
    {
        "content": "<p>That would work for prototyping, but for harder cases which bounce back and forth between <code>grind</code> and the model you would be repeating a lot of <code>grind</code> work. Good enough for research, but not how you'd want to build a more production quality version of it. Unless <code>grind</code> can do some whiteboard caching between invocations somehow?</p>",
        "id": 535353386,
        "sender_full_name": "Sam",
        "timestamp": 1755707757
    },
    {
        "content": "<p>I also imagine a good version of this may, primarily for efficiency purposes, take top-K suggested conjectures from the model and try them all simultaneously. Each of those would have to redo all of the <code>grind</code> work so far, I think. Then you'd have to redo that work again when you insert all the true ones.</p>\n<p>Having this as an engine running on one continuous whiteboard just seems the more proper way to do it.</p>",
        "id": 535353880,
        "sender_full_name": "Sam",
        "timestamp": 1755707944
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"428422\">Sam</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535353386\">said</a>:</p>\n<blockquote>\n<p>That would work for prototyping, but for harder cases which bounce back and forth between <code>grind</code> and the model you would be repeating a lot of <code>grind</code> work. Good enough for research, but not how you'd want to build a more production quality version of it. Unless <code>grind</code> can do some whiteboard caching between invocations somehow?</p>\n</blockquote>\n<p>Doing these multi bounces between a suggestion oracle and grind itself again sounds somewhat contradictory to the goal of having stable proofs available through grind.</p>",
        "id": 535365141,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1755712811
    },
    {
        "content": "<p>Reusing work from a previous run of <code>grind</code> after adding a new fact sounds very difficult to arrange! Just knowing \"such and such a prefix of the computation can't have been affected by this new fact\" would be very hard, and supporting the ability to know such things would be far too constraining at this point in grind's development (and quite possibly ever).</p>",
        "id": 535399726,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1755729425
    },
    {
        "content": "<p>Interesting! I've been thinking of <code>grind</code> and its whiteboard like an e-graph++, enhanced with rules contributed by engines which call out to arbitrary theory solvers. Based on <span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span>'s response I think I must have some misunderstanding here, because e-graphs happily accept new expressions.</p>",
        "id": 535430174,
        "sender_full_name": "Sam",
        "timestamp": 1755752609
    },
    {
        "content": "<p>My assumption was that what all engines were doing was repeatedly querying the whiteboard for information and adding new facts found by the engine.</p>",
        "id": 535430253,
        "sender_full_name": "Sam",
        "timestamp": 1755752669
    },
    {
        "content": "<p>You can add new expressions, but it matters when you add it. If it is in the initial context, and then grind runs, you may reach a different state than if grind runs, and then you manually add it to the context (and let grind continue). So this was specifically about the stability of results when attempting to reuse previous work.</p>",
        "id": 535663631,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1755862120
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Kim Morrison</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535663631\">said</a>:</p>\n<blockquote>\n<p>You can add new expressions, but it matters when you add it. If it is in the initial context, and then grind runs, you may reach a different state than if grind runs, and then you manually add it to the context (and let grind continue). So this was specifically about the stability of results when attempting to reuse previous work.</p>\n</blockquote>\n<p>Say that I have a term elaborator that constructs some expression from the initial local context. If there a way to indicate to grind that it should start by binding this?</p>",
        "id": 535668973,
        "sender_full_name": "Chris Henson",
        "timestamp": 1755864221
    },
    {
        "content": "<p>Ah! I think I understand the stability point now. I can see how that might pose a challenge, although I think it would matter less to the types of use cases I'm personally interested in (using Lean + <code>grind</code> as a tool in a larger pipeline). I'd be happy running it once and then just extracting terms or similar as a proof certificate.</p>",
        "id": 535716387,
        "sender_full_name": "Sam",
        "timestamp": 1755879584
    },
    {
        "content": "<p>I've also noted that the docs stress that <code>grind</code> isn't intended to handle large decision problems. Is it possible that an engine could be added at some point which would identify good SAT candidates and utilise <code>bv_decide</code> within <code>grind</code>?</p>",
        "id": 535717581,
        "sender_full_name": "Sam",
        "timestamp": 1755880007
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535338665\">said</a>:</p>\n<blockquote>\n<p>While theoretically possible <code>grind</code> is currently not open for extension by users that are not comitting in core. I have seen the idea you are proposing be implemented by effectively adding <code>P or ~P</code> to the whiteboard equivalent and then letting the solver do a case split on that in another SMT solver called CVC4 so this is also possble in principle with grind.</p>\n</blockquote>\n<p>Well what if I just want to mess with adding stuff and I’m none of that? How could I just do it locally? I’m not expecting it to be stable or supported. I just want to.</p>",
        "id": 535731581,
        "sender_full_name": "Alok Singh",
        "timestamp": 1755885812
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"337670\">Alok Singh</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535731581\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535338665\">said</a>:</p>\n<blockquote>\n<p>While theoretically possible <code>grind</code> is currently not open for extension by users that are not comitting in core. I have seen the idea you are proposing be implemented by effectively adding <code>P or ~P</code> to the whiteboard equivalent and then letting the solver do a case split on that in another SMT solver called CVC4 so this is also possble in principle with grind.</p>\n</blockquote>\n<p>Well what if I just want to mess with adding stuff and I’m none of that? How could I just do it locally? I’m not expecting it to be stable or supported. I just want to.</p>\n</blockquote>\n<p>You fork Lean and edit grind code.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"428422\">Sam</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535717581\">said</a>:</p>\n<blockquote>\n<p>I've also noted that the docs stress that <code>grind</code> isn't intended to handle large decision problems. Is it possible that an engine could be added at some point which would identify good SAT candidates and utilise <code>bv_decide</code> within <code>grind</code>?</p>\n</blockquote>\n<p>Also theoretically possible yes</p>",
        "id": 535754091,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1755895770
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"337670\">Alok Singh</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535731581\">said</a>:</p>\n<blockquote>\n<p>Well what if I just want to mess with adding stuff and I’m none of that? How could I just do it locally? I’m not expecting it to be stable or supported. I just want to.</p>\n</blockquote>\n<p>Depending on what you actually want to do, <code>have</code> might already suffice for your purposes. :-)</p>",
        "id": 535766258,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1755903853
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/.60grind.60.20extensibility/near/535754091\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>and utilise <code>bv_decide</code> within <code>grind</code>?</p>\n</blockquote>\n</blockquote>\n<p>Our hesitation here (not necessarily permanent!) is that we'd prefer the engines to be \"incremental\", i.e. so relevant facts can be shipped to them as they are discovered, and the engine maintains internal state and reports back if it finds a contradiction (or \"good enough\" fact to go on the whiteboard).</p>\n<p>For <code>bv_decide</code> we'd have to have a heuristic that decide when we had enough stuff to pass to <code>bv_decide</code> (and moreover accept the expense of restarting <code>bv_decide</code> if it failed but we wanted to try again later with more facts).</p>",
        "id": 535766389,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1755904002
    }
]