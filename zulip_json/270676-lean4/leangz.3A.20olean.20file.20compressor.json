[
    {
        "content": "<p>As advertised <a href=\"#narrow/stream/270676-lean4/topic/olean.20dump.20tool/near/368355271\">earlier</a>, I've implemented <a href=\"https://github.com/digama0/leangz\">https://github.com/digama0/leangz</a> , a tool to compress and decompress olean files into a new file format <code>.lgz</code>. Because it is performance-sensitive, I wrote it in Rust instead of Lean, with the idea being that it would replace the current <code>tgz</code> call in <code>lake exe cache</code>.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">$</span> <span class=\"n\">cargo</span> <span class=\"n\">build</span> <span class=\"c1\">--release</span>\n<span class=\"bp\">$</span> <span class=\"n\">time</span> <span class=\"n\">target</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">leangz</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span>\n<span class=\"n\">________________________________________________________</span>\n<span class=\"n\">Executed</span> <span class=\"k\">in</span>   <span class=\"mi\">88</span><span class=\"bp\">.</span><span class=\"mi\">34</span> <span class=\"n\">millis</span>    <span class=\"n\">fish</span>           <span class=\"n\">external</span>\n   <span class=\"n\">usr</span> <span class=\"n\">time</span>   <span class=\"mi\">84</span><span class=\"bp\">.</span><span class=\"mi\">27</span> <span class=\"n\">millis</span>    <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">58</span> <span class=\"n\">millis</span>   <span class=\"mi\">82</span><span class=\"bp\">.</span><span class=\"mi\">69</span> <span class=\"n\">millis</span>\n   <span class=\"n\">sys</span> <span class=\"n\">time</span>    <span class=\"mi\">4</span><span class=\"bp\">.</span><span class=\"mi\">34</span> <span class=\"n\">millis</span>    <span class=\"mi\">0</span><span class=\"bp\">.</span><span class=\"mi\">40</span> <span class=\"n\">millis</span>    <span class=\"mi\">3</span><span class=\"bp\">.</span><span class=\"mi\">94</span> <span class=\"n\">millis</span>\n<span class=\"bp\">$</span> <span class=\"n\">time</span> <span class=\"n\">target</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">leangz</span> <span class=\"bp\">-</span><span class=\"n\">x</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz</span>\n<span class=\"n\">________________________________________________________</span>\n<span class=\"n\">Executed</span> <span class=\"k\">in</span>   <span class=\"mi\">25</span><span class=\"bp\">.</span><span class=\"mi\">72</span> <span class=\"n\">millis</span>    <span class=\"n\">fish</span>           <span class=\"n\">external</span>\n   <span class=\"n\">usr</span> <span class=\"n\">time</span>   <span class=\"mi\">18</span><span class=\"bp\">.</span><span class=\"mi\">91</span> <span class=\"n\">millis</span>    <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">15</span> <span class=\"n\">millis</span>   <span class=\"mi\">17</span><span class=\"bp\">.</span><span class=\"mi\">75</span> <span class=\"n\">millis</span>\n   <span class=\"n\">sys</span> <span class=\"n\">time</span>    <span class=\"mi\">7</span><span class=\"bp\">.</span><span class=\"mi\">10</span> <span class=\"n\">millis</span>    <span class=\"mi\">0</span><span class=\"bp\">.</span><span class=\"mi\">00</span> <span class=\"n\">millis</span>    <span class=\"mi\">7</span><span class=\"bp\">.</span><span class=\"mi\">10</span> <span class=\"n\">millis</span>\n<span class=\"bp\">$</span> <span class=\"n\">target</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">leangz</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz.olean</span>\n<span class=\"bp\">$</span> <span class=\"n\">gzip</span> <span class=\"bp\">-</span><span class=\"n\">k</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span>\n<span class=\"bp\">$</span> <span class=\"n\">ls</span> <span class=\"bp\">-</span><span class=\"n\">al</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span><span class=\"bp\">*</span>\n<span class=\"n\">ls</span> <span class=\"bp\">-</span><span class=\"n\">al</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span><span class=\"bp\">*</span>\n<span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">r</span><span class=\"c1\">-- 1 mario mario 1772440 Jun 22 21:53 ../lean4/build/release/stage1/lib/lean/Init/Core.olean</span>\n<span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">r</span><span class=\"c1\">-- 1 mario mario  554831 Jun 22 21:53 ../lean4/build/release/stage1/lib/lean/Init/Core.olean.gz</span>\n<span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">r</span><span class=\"c1\">-- 1 mario mario  361867 Jun 23 09:09 ../lean4/build/release/stage1/lib/lean/Init/Core.olean.lgz</span>\n<span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">r</span><span class=\"c1\">-- 1 mario mario 1772440 Jun 23 09:09 ../lean4/build/release/stage1/lib/lean/Init/Core.olean.lgz.olean</span>\n<span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">rw</span><span class=\"bp\">-</span><span class=\"n\">r</span><span class=\"c1\">-- 1 mario mario  361867 Jun 23 09:12 ../lean4/build/release/stage1/lib/lean/Init/Core.olean.lgz.olean.lgz</span>\n<span class=\"bp\">$</span> <span class=\"n\">diff</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz.olean.lgz</span>\n<span class=\"bp\">$</span> <span class=\"n\">diff</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz.olean</span>\n<span class=\"n\">Binary</span> <span class=\"n\">files</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span> <span class=\"n\">and</span> <span class=\"bp\">../</span><span class=\"n\">lean4</span><span class=\"bp\">/</span><span class=\"n\">build</span><span class=\"bp\">/</span><span class=\"n\">release</span><span class=\"bp\">/</span><span class=\"n\">stage1</span><span class=\"bp\">/</span><span class=\"n\">lib</span><span class=\"bp\">/</span><span class=\"n\">lean</span><span class=\"bp\">/</span><span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz.olean</span> <span class=\"n\">differ</span>\n</code></pre></div>\n<p>As you can see, the <code>olean.lgz</code> file is 20% the size of the original <code>olean</code>, and 34% smaller than the gzipped olean file. (Note that lgz includes a gzip step internally; the lgz file without compression is 757152 bytes, or  42% of the original olean.)</p>\n<p>One thing that has not yet been solved is whether it is possible to get bit-for-bit identical oleans back. As you can see, the olean -&gt; lgz -&gt; olean process does not yield exactly the same result, although lgz -&gt; olean -&gt; lgz does. I believe this is because the olean compactor in lean core uses a different traversal strategy which yields a different ordering for the nodes and hence different pointer values. This also causes a problem for using it in <code>lake exe cache</code>, since either lake needs to run an extra olean -&gt; lgz -&gt; olean roundtrip before calculating the trace, or we need to calculate new traces, or else lake will refuse to use the unpacked olean files. And even if it does use them because we overrode the trace, any locally compiled oleans will not match that trace and cause a bunch of recompiles.</p>",
        "id": 368896955,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687526909
    },
    {
        "content": "<p>Why do you have a custom traversal order, can't you traverse the objects in the order as in the file, which already is topologically sorted? And have you thought about compressing n-ary applications yet?</p>",
        "id": 368903667,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687528145
    },
    {
        "content": "<p>If we have the full library ecosystem of Rust available, we can also move on to zstd from gzip</p>",
        "id": 368904748,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687528359
    },
    {
        "content": "<p>The original traversal order is lost because lgz stores the nodes in a different way which avoids allocating names for nodes which are used only once. It should still be possible to recover the olean traversal order though since decompression is much like the original compaction stage. It is a bit annoying that the order isn't what I am generating now though, a very canonical postorder DFS</p>",
        "id": 368917692,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687530815
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/368903667\">said</a>:</p>\n<blockquote>\n<p>And have you thought about compressing n-ary applications yet?</p>\n</blockquote>\n<p>So far it is only doing the type oblivious thing, traversing the object graph, which means it can't do anything expr-specific. I would like to give it type info at least enough to identify exprs, because you can do a lot better with types: you don't need to say how many arguments each constructor has, and most importantly you can remove the hash field, which is aggressively bad for compression.</p>",
        "id": 368920128,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687531224
    },
    {
        "content": "<p>From my previous numbers, objects with constructor tag 5, which are presumably almost entirely applications, are really all that matters. So if adding type info is nontrivial, you could simply check for any such objects whether they have an <code>app</code> shape and whether the redundant data checks out and in that case discard the data and pack together the spine</p>",
        "id": 368936874,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687534002
    },
    {
        "content": "<p>The olean reproducibility issue is now fixed (although I cheated a bit by changing lean instead of leangz: <a href=\"https://github.com/leanprover/lean4/pull/2286\">lean4#2286</a>)</p>",
        "id": 369095764,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687591276
    },
    {
        "content": "<p>Did you compare <code>gz</code> to, e.g., <code>xz</code> for the generic compression step?</p>",
        "id": 369171208,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1687609928
    },
    {
        "content": "<p>BTW, it seems that th e<code>lgz</code> extension is taken on Windows by MS Windows Application Log.</p>",
        "id": 369171755,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1687610090
    },
    {
        "content": "<p>The <code>flate2</code> library I am using has three available compression formats (DEFLATE, gzip and zlib), but a basic test didn't show up any major differences in compression quality. I have not tried a large scale test though. There is also a quality knob (fast &lt;-&gt; best) which can be tweaked depending on how much we think it is worth squeezing extra bytes out</p>",
        "id": 369175117,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687610809
    },
    {
        "content": "<p>What if you run an external <code>xz</code> to compare? I remember that moving from <code>tar.gz</code> to <code>tar.xz</code> saved us some space (hence, download time) with Lean 3 cache.</p>",
        "id": 369240387,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1687629331
    },
    {
        "content": "<p>I also wonder if zlib with a preconfigured dictionary would do a good job, after all we are serving individual files now right, so a dictionary might be more relevant than before</p>",
        "id": 369241374,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1687629735
    },
    {
        "content": "<p>Woohoo, I implemented <span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span> 's idea to heuristically identify Name / Level / Expr constructors in the olean files, and the resulting file is 2.7x smaller than the old implementation and <strong>13.3x</strong> smaller than the original .olean file (or 4.1x smaller than the olean.gz files we are using in <code>lake exe cache</code>). At this point I think it is competitive with the old lean 3 .xz files.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"mi\">1772440</span> <span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean</span>\n <span class=\"mi\">554831</span> <span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.gz</span>\n <span class=\"mi\">361866</span> <span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz</span>\n <span class=\"mi\">133153</span> <span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz2</span>\n<span class=\"mi\">1772440</span> <span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz2.olean</span>\n<span class=\"mi\">1772440</span> <span class=\"n\">Init</span><span class=\"bp\">/</span><span class=\"n\">Core.olean.lgz.olean</span>\n</code></pre></div>\n<p>The reason I didn't use the type-aware version, at least for now, is because types are not always available, for example in dynamically typed environment extensions, and it is good to be able to compress exprs that might appear in those terms as well.</p>",
        "id": 369388445,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687694557
    },
    {
        "content": "<p>That's fantastic!</p>",
        "id": 369388849,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687694651
    },
    {
        "content": "<p>After a bit more tweaking, I managed to get the compression ratio up to <strong>14.8x</strong> on <code>Lean</code>. On <code>Mathlib</code> the compression ratio is even higher, a <strong>16.9x</strong> reduction from 2.72GB to 160MB. Does anyone know how that compares to mathlib3?</p>",
        "id": 369446240,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687707663
    },
    {
        "content": "<p>The choice of compression algorithm doesn't seem to make too big of a difference, compared to the wins I was getting with just adjusting the encoding. These are the numbers for compressing <code>Lean</code> with various algorithms and quality settings: (I've been using <code>gz 7</code> for most of the numbers in this thread.)</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">deflate</span> <span class=\"mi\">1</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">50503772</span> <span class=\"bp\">=</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">878758</span>\n<span class=\"n\">zlib</span>    <span class=\"mi\">1</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">50508068</span> <span class=\"bp\">=</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">877748</span>\n<span class=\"n\">gz</span>      <span class=\"mi\">1</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">50516660</span> <span class=\"bp\">=</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">875728</span>\n<span class=\"n\">deflate</span> <span class=\"mi\">7</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">40508075</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">809939</span>\n<span class=\"n\">zlib</span>    <span class=\"mi\">7</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">40512371</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">808368</span>\n<span class=\"n\">gz</span>      <span class=\"mi\">7</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">40520963</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">805228</span>\n<span class=\"n\">deflate</span> <span class=\"mi\">9</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">40460111</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">827495</span>\n<span class=\"n\">zlib</span>    <span class=\"mi\">9</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">40464407</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">825921</span>\n<span class=\"n\">gz</span>      <span class=\"mi\">9</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">40472999</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">822774</span>\n</code></pre></div>",
        "id": 369448620,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687708327
    },
    {
        "content": "<p>Unfortunately, Patrick is right that this doesn't really solve many of the problems associated to lean's large disk usage: nightlies will still be big (although they can be shipped to you smaller), so you still have to wipe your elan cache regularly, and copies of mathlib in your local projects will still be big. The only thing that is directly impacted is that the <code>.mathlib</code> cache will be smaller, and also the cache download times will be smaller.</p>",
        "id": 369449896,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687708666
    },
    {
        "content": "<p>To go the last mile would need lean itself to support reading these zipped files in memory, which unfortunately kills a lot of the advantage of mmap instant load times. Maybe that's a tradeoff users are willing to make, although I'm not sure how to expose that as an option. Maybe it would be lake's responsibility?</p>",
        "id": 369450600,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687708821
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369446240\">said</a>:</p>\n<blockquote>\n<p>After a bit more tweaking, I managed to get the compression ratio up to <strong>14.8x</strong> on <code>Lean</code>. On <code>Mathlib</code> the compression ratio is even higher, a <strong>16.9x</strong> reduction from 2.72GB to 160MB. Does anyone know how that compares to mathlib3?</p>\n</blockquote>\n<p>I just checked, and recent mathlib3 <code>tar.xz</code> files are 92MB, so I guess there is still some ways to go.</p>",
        "id": 369452899,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687709334
    },
    {
        "content": "<p>This isnt quite a fair comparision due to the extra size to the original oleans that the new code generator adds though, if you want an actual comparision to mathlib3 you should probably add the option that disables it to the lakefile and compile with that</p>",
        "id": 369465247,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1687712899
    },
    {
        "content": "<p>the code generator only adds about 5% to the oleans though</p>",
        "id": 369466064,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687713128
    },
    {
        "content": "<p>at least in mathlib</p>",
        "id": 369466078,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687713135
    },
    {
        "content": "<p>I think that most of the rest of the gains are to be found deeper in lean though, I expect we are really just producing larger or more complex expressions for certain things</p>",
        "id": 369466588,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687713283
    },
    {
        "content": "<blockquote>\n<p>The choice of compression algorithm doesn't seem to make too big of a difference, compared to the wins I was getting with just adjusting the encoding. These are the numbers for compressing <code>Lean</code> with various algorithms and quality settings:</p>\n</blockquote>\n<p>I hope you realize that deflate, zlib, and gz are essentially the same algorithm, right?  You should definitely try zstd instead.</p>",
        "id": 369471468,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1687714545
    },
    {
        "content": "<p>I have very little knowledge about how those compression algorithms relate, they just happened to be the ones provided by the library I'm using (which makes sense, if they are all similar)</p>",
        "id": 369472246,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687714778
    },
    {
        "content": "<p>Okay, zstd does improve the result:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">zstd</span> <span class=\"mi\">3</span><span class=\"o\">:</span>  <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">41122345</span> <span class=\"bp\">=</span> <span class=\"mi\">14</span><span class=\"bp\">.</span><span class=\"mi\">588713</span>\n<span class=\"n\">zstd</span> <span class=\"mi\">15</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">38674486</span> <span class=\"bp\">=</span> <span class=\"mi\">15</span><span class=\"bp\">.</span><span class=\"mi\">512090</span>\n<span class=\"n\">zstd</span> <span class=\"mi\">18</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">35631733</span> <span class=\"bp\">=</span> <span class=\"mi\">16</span><span class=\"bp\">.</span><span class=\"mi\">836737</span>\n<span class=\"n\">zstd</span> <span class=\"mi\">21</span><span class=\"o\">:</span> <span class=\"mi\">599922104</span> <span class=\"bp\">/</span> <span class=\"mi\">35565313</span> <span class=\"bp\">=</span> <span class=\"mi\">16</span><span class=\"bp\">.</span><span class=\"mi\">868180</span>\n</code></pre></div>",
        "id": 369478924,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687716403
    },
    {
        "content": "<p>Unclear whether I am supposed to react with <span aria-label=\"upwards trend\" class=\"emoji emoji-1f4c8\" role=\"img\" title=\"upwards trend\">:upwards_trend:</span> or <span aria-label=\"downwards trend\" class=\"emoji emoji-1f4c9\" role=\"img\" title=\"downwards trend\">:downwards_trend:</span> to this...</p>",
        "id": 369479753,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1687716695
    },
    {
        "content": "<p>What does the magic number after zstd mean, is that the parameter for how hard it is supposed to compress?</p>",
        "id": 369479828,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1687716723
    },
    {
        "content": "<p>yes, it apparently ranges from 1-21 with a default of 3</p>",
        "id": 369479910,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687716743
    },
    {
        "content": "<p>unlike the others that go from 1-9</p>",
        "id": 369479940,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687716753
    },
    {
        "content": "<p>I'm not sure what we are using on mathlib CI but it's probably cranked to max</p>",
        "id": 369480065,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687716812
    },
    {
        "content": "<p>We don't use zstd on mathlib ci, only on Lean 4 CI.  There we use 19 from what I can tell.</p>",
        "id": 369493807,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1687721963
    },
    {
        "content": "<p>Mario, thanks a lot for working on this! For teaching I would clearly prefer slow loading time with small olean files than fast loading with large oleans.</p>",
        "id": 369494988,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1687722484
    },
    {
        "content": "<p>The growing elan cache isn't an issue for teaching since we usually don't want students to update Lean or mathlib during a course.</p>",
        "id": 369495167,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1687722567
    },
    {
        "content": "<p>Regarding the compression trade-off, the ideal situation for me would be tightly compressed oleans over the network, but then storing oleans uncompressed in the cache for fast switching between branches. (I'm sure I decompress the same oleans multiple times every day.)</p>",
        "id": 369527121,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1687740423
    },
    {
        "content": "<p>Perhaps there could even be two layers of the cache if we implement the auto-cleaning feature discussed earlier. E.g. I'd happily keep X gigabytes of uncompressed oleans, plus a further Y gigabyes of compressed oleans, etc.</p>",
        "id": 369527209,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1687740476
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369527121\">said</a>:</p>\n<blockquote>\n<p>Regarding the compression trade-off, the ideal situation for me would be tightly compressed oleans over the network, but then storing oleans uncompressed in the cache for fast switching between branches. (I'm sure I decompress the same oleans multiple times every day.)</p>\n</blockquote>\n<p>I'll get you some numbers soon, but I'm hoping it won't be necessary to do this. Decompression is quite fast, you should not notice it compared to the current <code>tar</code> performance. (It might even be faster, we'll see.)</p>",
        "id": 369531659,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687742287
    },
    {
        "content": "<p>I'm not sure if this can completely replace the tar stuff.  We also cache ileans and c files.</p>",
        "id": 369535830,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1687744068
    },
    {
        "content": "<p>Hm, maybe I should take the compression out of the lgz format and instead use tar.xz on uncompressed <code>lgz + ilean + c</code>? There could still be performance gains by just doing that all in-process instead of shelling out to thousands of commands as is currently done</p>",
        "id": 369537518,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687744876
    },
    {
        "content": "<p>Are we <strong>shelling</strong> out or <code>exec</code> extra processes? I mean, do we run <code>bin/sh</code> (or an equivalent) in addition to <code>/usr/bin/tar</code>?</p>",
        "id": 369540086,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1687746438
    },
    {
        "content": "<p>pretty sure it's just exec</p>",
        "id": 369540107,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687746465
    },
    {
        "content": "<p>One thing that I would really like to add to that process though is to check the trace and not unpack the file if it's already up to date. Right now that is the bottleneck when you call <code>lake exe cache</code> and there is nothing to do</p>",
        "id": 369540646,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687746763
    },
    {
        "content": "<p>perhaps it would be worth trying brotli instead of zstd as well?!</p>",
        "id": 369568922,
        "sender_full_name": "Moritz Firsching",
        "timestamp": 1687759328
    },
    {
        "content": "<p>isn't brotli for text?</p>",
        "id": 369569895,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687759605
    },
    {
        "content": "<p>I was thinking about that, dictionary approaches may well help, but probably we want a custom dictionary, not sure if any of these compression algs support that</p>",
        "id": 369569969,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687759641
    },
    {
        "content": "<p>I have performance numbers for unpacking mathlib now:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">baseline</span><span class=\"o\">:</span> <span class=\"mi\">12</span><span class=\"bp\">.</span><span class=\"mi\">6</span> <span class=\"n\">s</span> <span class=\"n\">compress</span><span class=\"o\">,</span>  <span class=\"mi\">1</span><span class=\"bp\">.</span><span class=\"mi\">13</span> <span class=\"n\">s</span> <span class=\"n\">decompress</span><span class=\"o\">,</span> <span class=\"mi\">2749684104</span> <span class=\"bp\">/</span> <span class=\"mi\">372063364</span> <span class=\"bp\">=</span> <span class=\"mi\">7</span><span class=\"bp\">.</span><span class=\"mi\">39</span><span class=\"n\">x</span>\n<span class=\"n\">gz</span> <span class=\"mi\">7</span><span class=\"o\">:</span>     <span class=\"mi\">34</span><span class=\"bp\">.</span><span class=\"mi\">1</span> <span class=\"n\">s</span> <span class=\"n\">compress</span><span class=\"o\">,</span> <span class=\"mi\">13</span><span class=\"bp\">.</span><span class=\"mi\">69</span> <span class=\"n\">s</span> <span class=\"n\">decompress</span><span class=\"o\">,</span> <span class=\"mi\">2749684104</span> <span class=\"bp\">/</span> <span class=\"mi\">162315766</span> <span class=\"bp\">=</span> <span class=\"mi\">16</span><span class=\"bp\">.</span><span class=\"mi\">94</span><span class=\"n\">x</span>\n<span class=\"n\">zstd</span> <span class=\"mi\">19</span><span class=\"o\">:</span> <span class=\"mi\">216</span><span class=\"bp\">.</span><span class=\"mi\">5</span> <span class=\"n\">s</span> <span class=\"n\">compress</span><span class=\"o\">,</span> <span class=\"mi\">11</span><span class=\"bp\">.</span><span class=\"mi\">16</span> <span class=\"n\">s</span> <span class=\"n\">decompress</span><span class=\"o\">,</span> <span class=\"mi\">2749684104</span> <span class=\"bp\">/</span> <span class=\"mi\">143815597</span> <span class=\"bp\">=</span> <span class=\"mi\">19</span><span class=\"bp\">.</span><span class=\"mi\">12</span><span class=\"n\">x</span>\n</code></pre></div>\n<p>Here <code>zstd 19</code> means that we used zstd compression, and <code>baseline</code> is for the case of no off the shelf compression, just the manually written stuff. Compare this with the approximately 15 seconds of <code>tar</code> time spent in <code>lake exe cache</code> when there is nothing to do. But note also that this is <em>single-threaded</em> performance, it can be divided by 8 or how ever many cores you have in practice. That brings the baseline performance to around 140ms, which I think is small enough to be usable by lean itself during startup, assuming we can resolve the linking issues with bringing in this library (or rewrite the decompressor in C++...). <span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span> do you have any thoughts on how one might use leangz in the lean4 repo? I think it would be best to keep the codec as a separate project, and statically link it with lean so that it can unpack directly into the mmap regions on startup.</p>",
        "id": 369592259,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687765167
    },
    {
        "content": "<p>I don't think uncompressing at Lean run time is a good approach, the memory overhead of parallel Lean processes would be prohibitive unless we implemented explicit interprocess sharing (which we get for free when the source is uncompressed files)</p>",
        "id": 369608328,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687768629
    },
    {
        "content": "<p>I didn't suggest parallel lean processes, rather lean would link to leangz which may use threads to load files in parallel</p>",
        "id": 369623438,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687771586
    },
    {
        "content": "<p>actually even that isn't really necessary, it could be just a plain single-threaded function on the leangz side and lean does the multithreading using tasks</p>",
        "id": 369624237,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687771735
    },
    {
        "content": "<p>No, what I mean is that you don't want to keep a separate, uncompressed copy of mathlib in memory for each open Lean file</p>",
        "id": 369624313,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687771753
    },
    {
        "content": "<p>oh I see, that sharing is not being managed by lean</p>",
        "id": 369624456,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687771792
    },
    {
        "content": "<p>you could do it by creating a temp file and mmaping that</p>",
        "id": 369624635,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687771826
    },
    {
        "content": "<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">zstd</span> <span class=\"mi\">19</span><span class=\"o\">:</span>        <span class=\"mi\">199</span><span class=\"bp\">.</span><span class=\"mi\">77</span> <span class=\"n\">s</span> <span class=\"n\">compress</span><span class=\"o\">,</span> <span class=\"mi\">10</span><span class=\"bp\">.</span><span class=\"mi\">66</span> <span class=\"n\">s</span> <span class=\"n\">decompress</span><span class=\"o\">,</span> <span class=\"mi\">2749684104</span> <span class=\"bp\">/</span> <span class=\"mi\">143833925</span> <span class=\"bp\">=</span> <span class=\"mi\">19</span><span class=\"bp\">.</span><span class=\"mi\">11</span><span class=\"n\">x</span>\n<span class=\"n\">zstd</span> <span class=\"mi\">19</span> <span class=\"bp\">+</span> <span class=\"n\">dict</span><span class=\"o\">:</span> <span class=\"mi\">100</span><span class=\"bp\">.</span><span class=\"mi\">64</span> <span class=\"n\">s</span> <span class=\"n\">compress</span><span class=\"o\">,</span> <span class=\"mi\">10</span><span class=\"bp\">.</span><span class=\"mi\">22</span> <span class=\"n\">s</span> <span class=\"n\">decompress</span><span class=\"o\">,</span> <span class=\"mi\">2749684104</span> <span class=\"bp\">/</span> <span class=\"mi\">139903272</span> <span class=\"bp\">=</span> <span class=\"mi\">19</span><span class=\"bp\">.</span><span class=\"mi\">65</span><span class=\"n\">x</span>\n</code></pre></div>\n<p>I managed to figure out how to use the dictionary creation feature of zstd, and it improves things on all axes: significantly faster compress time, better compression, and slightly faster decompress time</p>",
        "id": 369675854,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687781078
    },
    {
        "content": "<p>the only thing I can recognize in the dictionary are the names of environment extensions like <code>protectedExt</code>, <code>customEliminatorExt</code> etc. Makes sense that these would show up in all the olean files</p>",
        "id": 369676831,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687781235
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369624635\">said</a>:</p>\n<blockquote>\n<p>you could do it by creating a temp file and mmaping that</p>\n</blockquote>\n<p>Then who will remove these files? If we need to come up with any kind of cache eviction policies, these should be in <code>cache</code>, not core.</p>",
        "id": 369690414,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687783837
    },
    {
        "content": "<p>How about this: <code>cache</code> keeps a global directory of unpacked .olean files bounded by some sensible-sounding size like say 10GB and hardlinks these files into project directories as desired. When the size limit is reached, we start deleting files with link count 1. Which might even work on Windows.</p>",
        "id": 369692565,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687784206
    },
    {
        "content": "<p>I think it would be fine even if lean itself never cleans up those files and something else is doing garbage collection</p>",
        "id": 369697354,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687785082
    },
    {
        "content": "<p>the main issue right now is that you can't really remove oleans without causing massive rebuilds</p>",
        "id": 369697494,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687785108
    },
    {
        "content": "<p>if they are just build outputs that can be easily regenerated on demand and cleaned up as required the equation changes a bit</p>",
        "id": 369697741,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687785153
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369692565\">said</a>:</p>\n<blockquote>\n<p>How about this: <code>cache</code> keeps a global directory of unpacked .olean files bounded by some sensible-sounding size like say 10GB and hardlinks these files into project directories as desired. When the size limit is reached, we start deleting files with link count 1. Which might even work on Windows.</p>\n</blockquote>\n<p>How would this work if there are multiple oleans with the same path (e.g. different versions of the same file on different branches of mathlib)?</p>",
        "id": 369698500,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687785284
    },
    {
        "content": "<p>But I take your point that if the temp file generation has to happen anyway, it doesn't have to be lean doing the unpacking, and probably lake is a better place to live</p>",
        "id": 369699174,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687785412
    },
    {
        "content": "<p>If <code>lake</code> was driving the process then you could pretty well ensure that these temp files really are suitably temporary</p>",
        "id": 369699713,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687785510
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369569969\">said</a>:</p>\n<blockquote>\n<p>I was thinking about that, dictionary approaches may well help, but probably we want a custom dictionary, not sure if any of these compression algs support that</p>\n</blockquote>\n<p>I though brotli supports custom dictionary: <a href=\"https://github.com/google/brotli/blob/master/research/dictionary_generator.cc\">https://github.com/google/brotli/blob/master/research/dictionary_generator.cc</a></p>",
        "id": 369709291,
        "sender_full_name": "Moritz Firsching",
        "timestamp": 1687787219
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369698500\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/369692565\">said</a>:</p>\n<blockquote>\n<p>How about this: <code>cache</code> keeps a global directory of unpacked .olean files bounded by some sensible-sounding size like say 10GB and hardlinks these files into project directories as desired. When the size limit is reached, we start deleting files with link count 1. Which might even work on Windows.</p>\n</blockquote>\n<p>How would this work if there are multiple oleans with the same path (e.g. different versions of the same file on different branches of mathlib)?</p>\n</blockquote>\n<p>Same way as with the current tar cache, use the hash as a name. The renaming can be done as part of the hardlinking.</p>",
        "id": 369711329,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1687787559
    },
    {
        "content": "<p>Okay, it's just about ready for use in <code>lake exe cache</code> now. I added a <code>leantar</code> utility that has a similar interface as what <code>cache</code> is expecting: a command to pack <code>olean + ilean + ...</code> into a <code>.ltar</code> file, compressing stuff along the way; and a single command that takes a list of <code>.ltar</code> files and unpacks everything which is not up to date. Here are the test results for an end to end test (local only, no network stuff)</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"mi\">129</span><span class=\"bp\">.</span><span class=\"mi\">3</span> <span class=\"n\">s</span> <span class=\"n\">compress</span>\n<span class=\"mi\">3196</span> <span class=\"n\">files</span><span class=\"o\">,</span> <span class=\"mi\">167</span><span class=\"bp\">.</span><span class=\"mi\">852</span> <span class=\"n\">MB</span>\n<span class=\"mi\">90</span><span class=\"n\">ms</span> <span class=\"n\">to</span> <span class=\"k\">do</span> <span class=\"n\">nothing</span> <span class=\"o\">(</span><span class=\"n\">everything</span> <span class=\"n\">up</span> <span class=\"n\">to</span> <span class=\"n\">date</span><span class=\"o\">)</span>\n<span class=\"mi\">3</span><span class=\"bp\">.</span><span class=\"mi\">260</span> <span class=\"n\">s</span> <span class=\"n\">to</span> <span class=\"n\">unpack</span> <span class=\"n\">everything</span>\n</code></pre></div>\n<p>Compared to <code>15.969 s</code> for running <code>lake exe cache</code>, it is a 4.9x improvement when actually unpacking, and a 177x improvement when there is nothing to do.</p>",
        "id": 369738648,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1687792177
    },
    {
        "content": "<p>The <code>leantar</code>-enabled cache is now up for review: <a href=\"https://github.com/leanprover-community/mathlib4/pull/5710\">#5710</a>. The <a href=\"https://github.com/digama0/leangz\">https://github.com/digama0/leangz</a> repo is now cutting releases for all arches supported by lean4, and <code>lake exe cache</code> will automatically download <code>leantar</code> and use it, same as it currently does for <code>curl</code>.</p>",
        "id": 372238814,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688463143
    },
    {
        "content": "<p>If this looks good we should probably transfer leangz over to leanprover-community repo</p>",
        "id": 372239378,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688463269
    },
    {
        "content": "<p>This somewhat begs the question whether it wouldn't be easier to write all of Cache in Rust, in which case we can use libcurl and link it into a single binary. Not sure if any plans of integrating it into Lake are affected by that, the Rust code could still export a sensible FFI</p>",
        "id": 372254256,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1688466200
    },
    {
        "content": "<p>yeah, I have been seriously considering that. We might want to think about it in conjunction with the rewrite for moving it to lake</p>",
        "id": 372254442,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466242
    },
    {
        "content": "<p>This initial release is mainly focused on preserving the existing interface, modulo the differing file extensions and such</p>",
        "id": 372254766,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466310
    },
    {
        "content": "<p>Rust would also help with doing \"advanced\" FS operations like the symlinking approach mentioned above</p>",
        "id": 372255053,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1688466370
    },
    {
        "content": "<p>I just got, after a partial build, <code>lake exe cache pack</code> then <code>lake exe cache get</code>:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">Decompressing</span> <span class=\"mi\">401</span> <span class=\"n\">file</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">)</span>\n<span class=\"n\">thread</span> <span class=\"bp\">'&lt;</span><span class=\"n\">unnamed</span><span class=\"bp\">&gt;'</span> <span class=\"n\">panicked</span> <span class=\"n\">at</span> <span class=\"bp\">'</span><span class=\"n\">called</span> <span class=\"bp\">`</span><span class=\"n\">Result</span><span class=\"o\">::</span><span class=\"n\">unwrap</span><span class=\"o\">()</span><span class=\"bp\">`</span> <span class=\"n\">on</span> <span class=\"n\">an</span> <span class=\"bp\">`</span><span class=\"n\">Err</span><span class=\"bp\">`</span> <span class=\"n\">value</span><span class=\"o\">:</span> <span class=\"n\">Error</span> <span class=\"o\">{</span> <span class=\"n\">kind</span><span class=\"o\">:</span> <span class=\"n\">UnexpectedEof</span><span class=\"o\">,</span> <span class=\"n\">message</span><span class=\"o\">:</span> <span class=\"s2\">\"failed to fill whole buffer\"</span> <span class=\"o\">}</span><span class=\"bp\">'</span><span class=\"o\">,</span> <span class=\"n\">src</span><span class=\"bp\">/</span><span class=\"n\">tar.rs</span><span class=\"o\">:</span><span class=\"mi\">87</span><span class=\"o\">:</span><span class=\"mi\">36</span>\n<span class=\"n\">note</span><span class=\"o\">:</span> <span class=\"n\">run</span> <span class=\"k\">with</span> <span class=\"bp\">`</span><span class=\"n\">RUST_BACKTRACE</span><span class=\"bp\">=</span><span class=\"mi\">1</span><span class=\"bp\">`</span> <span class=\"n\">environment</span> <span class=\"kd\">variable</span> <span class=\"n\">to</span> <span class=\"n\">display</span> <span class=\"n\">a</span> <span class=\"n\">backtrace</span>\n<span class=\"n\">uncaught</span> <span class=\"n\">exception</span><span class=\"o\">:</span> <span class=\"n\">leantar</span> <span class=\"n\">failed</span> <span class=\"k\">with</span> <span class=\"n\">error</span> <span class=\"n\">code</span> <span class=\"mi\">101</span>\n</code></pre></div>",
        "id": 372255351,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1688466428
    },
    {
        "content": "<p>that particular error message indicates that it tried to open an <code>.ltar</code> file which is 0 bytes</p>",
        "id": 372255967,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466554
    },
    {
        "content": "<p>I notice that packing the cache is substantially slower than it was previously. This isn't a huge problem, as mostly this is work done by CI, but perhaps worth flagging. Is there some trivial parallelism available? I notice it is doing this single threaded.</p>",
        "id": 372256102,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1688466581
    },
    {
        "content": "<p>yes, zstd is slower to pack</p>",
        "id": 372256207,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466598
    },
    {
        "content": "<p>at least on the relatively high compression setting in use</p>",
        "id": 372256409,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466623
    },
    {
        "content": "<p>When packing, the intention was for lean to handle the multithreading and launch many <code>leantar</code> processes, one per file</p>",
        "id": 372256861,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466707
    },
    {
        "content": "<p>(I actually haven't tested <code>pack</code> at all, I wasn't really sure whether it was possible to test without sending stuff to azure)</p>",
        "id": 372257101,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466757
    },
    {
        "content": "<p>oh I see, the pack is indeed serialized</p>",
        "id": 372257302,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688466807
    },
    {
        "content": "<p>This is probably worth fixing. A normal build takes ~30s to pack and upload, and your build is still going at 10 minutes.</p>",
        "id": 372257670,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1688466883
    },
    {
        "content": "<p>I did, it's 1.7 min parallel 12 threads</p>",
        "id": 372259593,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688467301
    },
    {
        "content": "<p>It finished uploading. I tried <code>lake exe cache get</code>, and it downloads, but then gives me 3500 copies of:</p>",
        "id": 372259639,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1688467312
    },
    {
        "content": "<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">stack</span> <span class=\"n\">backtrace</span><span class=\"o\">:</span>\n   <span class=\"mi\">0</span><span class=\"o\">:</span>        <span class=\"n\">thread</span> <span class=\"bp\">'</span><span class=\"mi\">0</span><span class=\"n\">x</span><span class=\"bp\">&lt;</span><span class=\"n\">unnamed</span><span class=\"bp\">&gt;</span><span class=\"mi\">10</span><span class=\"n\">e09ad36'</span> <span class=\"n\">panicked</span> <span class=\"n\">at</span> <span class=\"bp\">'</span> <span class=\"bp\">-</span> <span class=\"n\">called</span> <span class=\"bp\">`</span><span class=\"n\">Result</span><span class=\"o\">::</span><span class=\"n\">unwrap</span><span class=\"o\">()</span><span class=\"bp\">`</span> <span class=\"n\">on</span> <span class=\"n\">an</span> <span class=\"bp\">`</span><span class=\"n\">Err</span><span class=\"bp\">`</span> <span class=\"n\">value</span><span class=\"o\">:</span> <span class=\"n\">Os</span> <span class=\"o\">{</span> <span class=\"n\">code</span><span class=\"o\">:</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"n\">kind</span><span class=\"o\">:</span> <span class=\"n\">NotFound</span><span class=\"o\">,</span> <span class=\"n\">message</span><span class=\"o\">:</span> <span class=\"s2\">\"No such file or directory\"</span> <span class=\"o\">}</span><span class=\"n\">__mh_execute_header'</span><span class=\"o\">,</span>\n<span class=\"n\">src</span><span class=\"bp\">/</span><span class=\"n\">tar.rs</span><span class=\"o\">:</span> <span class=\"mi\">103</span> <span class=\"o\">:</span> <span class=\"mi\">541</span>\n</code></pre></div>",
        "id": 372259643,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1688467315
    },
    {
        "content": "<p>oh that looks a bit funny, maybe the streams are interleaving?</p>",
        "id": 372259936,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688467367
    },
    {
        "content": "<p>oh I see, all the threads are panicking in parallel with a not found error</p>",
        "id": 372260775,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688467552
    },
    {
        "content": "<p>It's a mass panic?</p>",
        "id": 372264003,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1688468201
    },
    {
        "content": "<p>What is the expected behavior when one of the cache files is empty, i.e. 0 bytes</p>",
        "id": 372268234,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688469053
    },
    {
        "content": "<p>Currently this just causes the extractor to report an error but continue and unpack other things (but still report an error for the overall execution), similar to the behavior you get with a bunch of <code>tar</code> calls</p>",
        "id": 372268851,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688469170
    },
    {
        "content": "<p>but <code>lake exe cache</code> doesn't really introspect on such files, it just assumes they are fine and does not try to redownload them or clean them up or anything</p>",
        "id": 372269021,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688469209
    },
    {
        "content": "<p>Where are these empty files coming from?</p>",
        "id": 372271464,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1688469692
    },
    {
        "content": "<p>I believe it happens when a writing process is terminated or curl dies</p>",
        "id": 372277101,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688470909
    },
    {
        "content": "<p>or possibly if curl loses connection in the middle</p>",
        "id": 372277220,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688470925
    },
    {
        "content": "<p>Instead of rewriting Cache in Rust, I would consider improving Lean 4 so it matches Rust's power more closely (where possible), even if it's a longer term goal.</p>\n<p>Mathlib is almost fully ported, meaning that Lean 4 is probably almost as good of a theorem prover as it needs to be (modulo optimizations for compilation and UX improvements, which can go on forever).</p>\n<p>Of course, I am out of context at this point so I don't know the long term strategy. But my impression is that once the community considers the port fully finished, a lot of burden will be released and there would be plenty of room to think big in different ways</p>",
        "id": 372294969,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1688474497
    },
    {
        "content": "<p>I understand your point but beware that mathematicians won't feel qualified to write that kind of code, so the end of the port won't free that much energy in this specific direction.</p>",
        "id": 372299563,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1688475380
    },
    {
        "content": "<p>Yeah I mean in the sense that core Lean 4 devs are focusing on providing support for the port. Once the port is finished, mathematicians will come back to expanding mathlib and the core devs would be able to worry less about mathlib</p>",
        "id": 372300520,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1688475565
    },
    {
        "content": "<p>But again, I'm out of the loop. I'm sure the core devs have a good plan. I'm speaking as someone (a programmer) who would be happy to see Lean 4 reach Rust in some of the aspects mentioned above.</p>",
        "id": 372301309,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1688475719
    },
    {
        "content": "<p>Cache is so small that I don't think it would even be contradictory to have the short-term goal of rewriting it in Rust and the longer-term goal of rewriting it in Lean</p>",
        "id": 372305900,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1688476546
    },
    {
        "content": "<p>Having said that, I have been thinking about what the smallest possible change to core would need to consist of to unblock interfacing with external libraries such as libcurl</p>",
        "id": 372307292,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1688476841
    },
    {
        "content": "<p>The PR now seems to be working: the last build completed successfully, and running <code>lake exe cache get</code> and <code>lake exe cache get!</code> in various combinations seems to unpack correctly. Unfortunately since mathlib is still on <code>leanprover/lean4:nightly-2023-06-20</code> it doesn't include the upstream changes to the olean file format needed to keep parity between <code>leangz</code> and <code>lean</code>, which means that lean will still recompile mathlib after unpacking the cache. This PR will need to depend on the lean4 bump PR <a href=\"https://github.com/leanprover-community/mathlib4/pull/5409\">#5409</a>.</p>",
        "id": 372376863,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688491344
    },
    {
        "content": "<p>It's working well enough to take some timing measurements. A <code>lean exe cache get</code> when there is nothing to do takes 4.6 seconds, of which 60ms are being spent in the decompressor (this part was previously ~15 seconds) and the other 4.6 seconds are spent computing hashes and starting up lean.</p>\n<p>To me this is just not acceptable performance, and to your point <span class=\"user-mention\" data-user-id=\"451983\">@Arthur Paulino</span> while I understand why we should make every attempt to make lean as good a language as it can be, I am a staunch believer in using the best tool for the job and currently lean simply isn't delivering. Maybe it will in the future and we can rewrite it in lean, but this is just wasting time and energy on users' machines in the meantime. TBH I knew that this would come up when I first decided to write the decompressor in something that isn't lean, people have this sense that we should be doing everything in lean because it's a programming language, but it's not the best programming language for very predictable reasons (it's <em>really</em> hard to compete with the big leagues), and making the lean experience better by not using lean sounds like a good tradeoff to me.</p>",
        "id": 372380418,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688492352
    },
    {
        "content": "<p>Slightly off-topic: Since the cache file name is changing anyway, can it be changed to use a fixed number of hex digits? Base 10 for hashes just looks weird</p>",
        "id": 372383135,
        "sender_full_name": "Mauricio Collares",
        "timestamp": 1688493167
    },
    {
        "content": "<p>those are separate issues</p>",
        "id": 372384046,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688493431
    },
    {
        "content": "<p>base 10 for hashes is determined by lake, cache can't do anything about it</p>",
        "id": 372384175,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688493476
    },
    {
        "content": "<p>oh nvm the file names are also base 10</p>",
        "id": 372384383,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688493529
    },
    {
        "content": "<p>I don't think there is even a function to get hex digits of fixed length</p>",
        "id": 372385749,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688493926
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110024\">Sebastian Ullrich</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/372307292\">said</a>:</p>\n<blockquote>\n<p>Having said that, I have been thinking about what the smallest possible change to core would need to consist of to unblock interfacing with external libraries such as libcurl</p>\n</blockquote>\n<p>While certainly this is a longer term goal, eventually interfacing with <code>libcurl</code> does seem like it could provide many benefits for a number of projects that wish to interact with web APIs.</p>",
        "id": 372400020,
        "sender_full_name": "Mac Malone",
        "timestamp": 1688498327
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/372384175\">said</a>:</p>\n<blockquote>\n<p>base 10 for hashes is determined by lake, cache can't do anything about it</p>\n</blockquote>\n<p>And Lake's reason for using base 10 is just because that is what Lean's <code>toString</code>/<code>toNat?</code> does. Also, at least in Lake, there isn't a strong motivation to do anything else, because there is not likely a significant performance difference between different bases.</p>",
        "id": 372400617,
        "sender_full_name": "Mac Malone",
        "timestamp": 1688498514
    },
    {
        "content": "<p>No, but I agree it looks a little peculiar. Incidentally, leangz does have to read trace files so it also has to implement those semantics</p>",
        "id": 372401429,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688498778
    },
    {
        "content": "<p>in any case, I updated <code>cache</code> to write the numbers in fixed width hex</p>",
        "id": 372401611,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688498841
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/leangz.3A.20olean.20file.20compressor/near/372384175\">said</a>:</p>\n<blockquote>\n<p>base 10 for hashes is determined by lake, cache can't do anything about it</p>\n</blockquote>\n<p>The cache filenames are purely our own making, they have zero relation to Lake and are not computed using Lake code either.  We could switch to base32 or whatever, but aesthetics aside I don't see any reason to switch.</p>",
        "id": 372412215,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1688503159
    },
    {
        "content": "<p>If people are unhappy with base 10 then I could recommend base 37. It's 0-9, then a-z, then <code>ᾰ</code>. I think it's ideally suited to Lean.</p>",
        "id": 372412759,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1688503425
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span> I wrote a function to do fixed width hex in <code>cache</code>, which really should be in <code>Lean</code>. Sadly we don't have fancy format strings in lean so hex formatting is painful right now and the path of least resistance uses decimal</p>",
        "id": 372413656,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1688503818
    }
]