[
    {
        "content": "<p>I thought I'd post here to solicit some feedback on Mathlib's <a href=\"https://github.com/leanprover-community/mathlib4/pull/19855\">#19855</a>, and whether these seem like appropriate invocations of <a href=\"https://leanprover-community.github.io/mathlib4_docs/find/?pattern=Lean.withTraceNode#doc\">docs#Lean.withTraceNode</a>. Some considerations:</p>\n<ol>\n<li>All the trace nodes have name <code>linarith</code> rather than something hierarchical, since <code>trace.linarith</code> is not an inherited trace option (if it were, then <code>linarith.detail</code> wouldn't work). This of course makes the trace useless without <code>set_option trace.profiler.output.pp true</code>.</li>\n<li>There seems to be some tension between putting data in the node message vs putting it in a <code>trace[]</code> within the scope. If you do the former, then the trace is easier to read, but if you do the latter, then the grouping provided by the firefox profiler is more useful</li>\n<li>It's not clear to me when the cost of formatting the message is paid; is this only when writing out the profile data at the very end?</li>\n<li>In one case I use a trace node just to make a long message collapsible; is this reasonable, and should there be a helper to streamline this in core?</li>\n</ol>",
        "id": 489198580,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734344790
    },
    {
        "content": "<p>I suppose the other relevant question here is how to choose between recording data with the <code>trace.profiler</code> and <code>profiler</code> machinery, and whether there should be an interface that encourages the use of both simultaneously</p>",
        "id": 489203287,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734346195
    },
    {
        "content": "<p>(I suspect thinking about this too long isn't a great use of FRO time, but if these things have already been thought about it would be great to build on that wisdom!)</p>",
        "id": 489203556,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734346275
    },
    {
        "content": "<p>Regarding</p>\n<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/channel/270676-lean4/topic/Using.20withTraceNode/near/489203287\">said</a>:</p>\n<blockquote>\n<p>I suppose the other relevant question here is how to choose between recording data with the <code>trace.profiler</code> and <code>profiler</code> machinery, and whether there should be an interface that encourages the use of both simultaneously</p>\n</blockquote>\n<p>The only situation in which I use <code>profiler</code> is when getting the data from <code>trace.profiler</code> is too expensive for some reason. For example when we run <code>bv_decide</code> on SMTLIB it can takes gigabytes of memory and quite some time to render the <code>trace.profiler</code> trace (this is tweakable with pretty pritner and threshold options and what not) and with <code>profiler</code> you can at least get some data as to what is slow. But apart from that <code>profiler</code> is just inferior to <code>trace.profiler</code> in every regard I'd say.</p>",
        "id": 489204345,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1734346504
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/Using.20withTraceNode/near/489204345\">said</a>:</p>\n<blockquote>\n<p>it can takes gigabytes of memory and quite some time to render the <code>trace.profiler</code> trace</p>\n</blockquote>\n<p>Are you using \"render\" in the \"React in the infoview\" sense, or \"serialize to json\" sense?</p>",
        "id": 489206520,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734347122
    },
    {
        "content": "<p>IIRC, the json -&gt; string serializer has some unpleasant quadratic behavior, which makes the latter much more expensive than it should be</p>",
        "id": 489206721,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734347166
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/channel/270676-lean4/topic/Using.20withTraceNode/near/489206520\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/Using.20withTraceNode/near/489204345\">said</a>:</p>\n<blockquote>\n<p>it can takes gigabytes of memory and quite some time to render the <code>trace.profiler</code> trace</p>\n</blockquote>\n<p>Are you using \"render\" in the \"React in the infoview\" sense, or \"serialize to json\" sense?</p>\n</blockquote>\n<p>print to stdout</p>",
        "id": 489207538,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1734347408
    },
    {
        "content": "<p>the expensive part is not serialisations or whatever it is pretty printing of expressions, SMTLIB contains expressiosn with megabytes up to gigabytes of text size</p>",
        "id": 489207618,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1734347438
    },
    {
        "content": "<p>The json failure I'm thinking of is <a href=\"https://github.com/leanprover/lean4/pull/5548\">lean4#5548</a></p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"n\">Lean</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">big</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">List</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">List</span><span class=\"bp\">.</span><span class=\"n\">iota</span><span class=\"w\"> </span><span class=\"mi\">200000</span>\n\n<span class=\"c1\">-- stack overflow</span>\n<span class=\"bp\">#</span><span class=\"n\">eval</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">String</span><span class=\"bp\">.</span><span class=\"n\">length</span><span class=\"w\"> </span><span class=\"bp\">&lt;|</span><span class=\"w\"> </span><span class=\"n\">toString</span><span class=\"w\"> </span><span class=\"bp\">&lt;|</span><span class=\"w\"> </span><span class=\"n\">Lean</span><span class=\"bp\">.</span><span class=\"n\">ToJson</span><span class=\"bp\">.</span><span class=\"n\">toJson</span><span class=\"w\"> </span><span class=\"n\">big</span><span class=\"o\">)</span>\n</code></pre></div>",
        "id": 489207998,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734347571
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/Using.20withTraceNode/near/489207618\">said</a>:</p>\n<blockquote>\n<p>the expensive part is not serialisations or whatever it is pretty printing of expressions, SMTLIB contains expressiosn with megabytes up to gigabytes of text size</p>\n</blockquote>\n<p>Does this suggest there should be an intermediate version of <code>output.pp</code> which is \"assemble messages but do not interpolate <code>Expr</code>s? Or are the SMT objects not <code>Expr</code>s anyway?</p>",
        "id": 489221946,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1734351813
    },
    {
        "content": "<p>They are exprs but there are means to reduce the size of printed expressions with the pretty printer options and threshold options already anyway. Its just an example where using profiler might even begin to make sense against trace.profiler</p>",
        "id": 489226849,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1734353243
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/channel/270676-lean4/topic/Using.20withTraceNode/near/489198580\">said</a>:</p>\n<blockquote>\n<ol start=\"3\">\n<li>It's not clear to me when the cost of formatting the message is paid; is this only when writing out the profile data at the very end?</li>\n</ol>\n</blockquote>\n<p>Can someone confirm that this understanding is correct? That is, message formatting costs are not included in the profiler results?</p>",
        "id": 492217527,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1736211020
    }
]