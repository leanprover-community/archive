[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"395550\">@Henrik Böving</span> mentioned that he (through the Lean FRO) is developing a new Lean verifier. From context is sounds like it is more along the lines of <span class=\"user-mention\" data-user-id=\"776090\">@GasStationManager</span>'s SafeVerify.  I'd love to learn more.  Here are some questions:</p>\n<ul>\n<li>What is the intended workflow and use case?</li>\n<li>Is it like SafeVerify in that it is intended for checking a full implementation against a reference implementation?  SafeVerify checks that the terms of the theorems are the same, that the proof terms replay (using same tools as Lean4Checker) and that the axioms are correct.</li>\n<li>Will this be an official Lean verifier supported by the FRO?</li>\n<li>Will this supersede Lean4Checker?</li>\n<li>When will it be released?</li>\n<li>SafeVerify doesn't work with multiple files.  Will this cover that case?</li>\n<li>SafeVerify doesn't allow custom axioms.  Will this have support for that?</li>\n<li>Can this be used to easily check human and AI generated developments?</li>\n<li>Is it robust to known attacks (short of a bug in the kernel).</li>\n</ul>",
        "id": 546476215,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761144035
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/113488-general/topic/Erdos.20707/near/546452170\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/113488-general/topic/Erdos.20707/near/546356646\">said</a>:</p>\n<blockquote>\n<p>Beside checking the statements carefully, it would be nice practice to also check the code carefully too with:</p>\n<ul>\n<li>print axioms</li>\n<li>lean4checker</li>\n<li>SafeVerify:  In particular, you can make two versions of the file, one with just the theorem statements and the definitions needed to state the theorems, and then the whole file.  SafeVerify will compare them, checking that the statements have the same terms and that the proofs/axioms are correct.  If SafeVerify is happy, then you only need to check the smaller file for mathematical correctness of the definitions and statements.</li>\n</ul>\n</blockquote>\n<p>The verifier that I've been developing at the FRO says yes :)</p>\n</blockquote>",
        "id": 546476669,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761144150
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/FRO's.20new.20verifier/near/546476215\">said</a>:</p>\n<blockquote>\n<ul>\n<li>What is the intended workflow and use case?</li>\n<li>Is it like SafeVerify in that it is intended for checking a full implementation against a reference implementation?  SafeVerify checks that the terms of the theorems are the same, that the proof terms replay (using same tools as Lean4Checker) and that the axioms are correct.</li>\n</ul>\n</blockquote>\n<p>In a similar vein to SafeVerify, comparing a challenge and a solution file and certifying that the solution is correct</p>\n<blockquote>\n<ul>\n<li>Will this be an official Lean verifier supported by the FRO?</li>\n</ul>\n</blockquote>\n<p>It will be at some point yes</p>\n<blockquote>\n<ul>\n<li>Will this supersede Lean4Checker?</li>\n</ul>\n</blockquote>\n<p>I think it fits into a bit of a different niche in that lean4checker has no concept of a challenge</p>\n<blockquote>\n<ul>\n<li>When will it be released?</li>\n</ul>\n</blockquote>\n<p>I don't have a concrete date for that yet unfortunately</p>\n<blockquote>\n<ul>\n<li>SafeVerify doesn't work with multiple files.  Will this cover that case?</li>\n</ul>\n</blockquote>\n<p>It doesn't right now but that seems like a pretty trivial change to me</p>\n<blockquote>\n<ul>\n<li>SafeVerify doesn't allow custom axioms.  Will this have support for that?</li>\n</ul>\n</blockquote>\n<p>Yes</p>\n<blockquote>\n<ul>\n<li>Can this be used to easily check human and AI generated developments?</li>\n</ul>\n</blockquote>\n<p>That feels like a bit of an open question, what does this even mean</p>\n<blockquote>\n<ul>\n<li>Is it robust to known attacks (short of a bug in the kernel).</li>\n</ul>\n</blockquote>\n<p>Yes, it is designed in a very defensive manner that also anticipates (and negates) some attacks which are only a theoretical possibility so far. On top of that it's also built in a way where integrating third party kernels is quite easy if people want to check with say, the Lean kernel and nanoda. That said it hasn't been stress tested yet by anyone apart from me so when we will release it there is going to be a call for testing during which you should maybe not yet trust it completely :)</p>",
        "id": 546478910,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761144680
    },
    {
        "content": "<blockquote>\n<p>That feels like a bit of an open question, what does this even mean</p>\n</blockquote>\n<p>Fair question.  I see the following needs:</p>\n<ul>\n<li>Recently there have been a lot of long proofs (sometimes with multiple files), created by a combination of AI and humans.  I'm more worried about the ones created by reinforcement-learning-trained AI where there may be subtle hacks in the proof.  We don't want to have to check thousands of lines of AI-generated Lean code by hand.  So it would be good to figure out how to check all the code and get the proof down to a small chunk of Lean for the theorem statement and definitions that we can check by hand.  This could itself be an automated process where one inputs the theorem name(s) they are interested in and we get a \"challenge\" project with only those theorems and dependencies.</li>\n<li>With human developments like blueprints (or even mathlib), it would be good to check/report what axioms are used.</li>\n<li>With training and benchmarking AI, one often has a challenge file/project with sorrys that they want filled in.  Then they want to check that the AI solution is correct.  Ideally this could be fast enough to use during training (so one wants an option where it doesn't recheck all of mathlib say).</li>\n</ul>",
        "id": 546481219,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761145235
    },
    {
        "content": "<p>Here are some exploits I know about:  Changing the meaning of notation in a theorem using local instances, using debug.skipTCKernel to skip kernel checking, exploiting native_decide, using metaprogramming to change the behavior of keywords or add fake theorems to the environment, crashing Lean in just the right way so that a theorem isn't added to the environment (and hence Lean and Lean4Checker are happy), and exploiting a bug in <code>#print axioms</code> so it says no axioms but axioms are used.</p>",
        "id": 546481242,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761145241
    },
    {
        "content": "<blockquote>\n<ul>\n<li>Recently there have been a lot of long proofs (sometimes with multiple files), created by a combination of AI and humans. I'm more worried about the ones created by reinforcement-learning-trained AI where there may be subtle hacks in the proof. We don't want to have to check thousands of lines of AI-generated Lean code by hand. So it would be good to figure out how to check all the code and get the proof down to a small chunk of Lean for the theorem statement and definitions that we can check by hand. This could itself be an automated process where one inputs the theorem name(s) they are interested in and we get a \"challenge\" project with only those theorems and dependencies.</li>\n</ul>\n</blockquote>\n<p>Automatically generating these challenge files from an untrusted input source such as an AI seems like an easy trap for correctness for me. The fundamental assumption of the checker is that the challenge file is to be trusted under all circumstances.</p>\n<blockquote>\n<ul>\n<li>With human developments like blueprints (or even mathlib), it would be good to check/report what axioms are used.</li>\n</ul>\n</blockquote>\n<p>I guess it could also be molded in a way that collects axioms instead of just checking them against an allowset pretty easily yeah.</p>\n<blockquote>\n<ul>\n<li>With training and benchmarking AI, one often has a challenge file/project with sorrys that they want filled in. Then they want to check that the AI solution is correct. Ideally this could be fast enough to use during training (so one wants an option where it doesn't recheck all of mathlib say).</li>\n</ul>\n</blockquote>\n<p>The current setup is not built for this. However, one thing you could do (which is basically the trick of the module system) is to say, okay, when we encounter a theorem within a trusted module export it as an axiom and dynamically add that theorem to the allowset instead. That way you would at least be skipping proofs (though not definitions as that is of course impossible) that come from a trusted source.</p>\n<p>In general the goal of the tool in its current incarnation is to provide as high certainty as possible that indeed the challenge file is guaranteed to match the solution file statement wise and that the solution file is type correct. It is completely biased towards this in the way it is engineered and accepts slowdowns that are not necessary unless you are really very paranoid, as if an OpenBSD person wrote a Lean checker really :D</p>\n<blockquote>\n<p>Here are some exploits I know about: Changing the meaning of notation in a theorem using local instances, using debug.skipTCKernel to skip kernel checking, exploiting native_decide, using metaprogramming to change the behavior of keywords or add fake theorems to the environment, crashing Lean in just the right way so that a theorem isn't added to the environment (and hence Lean and Lean4Checker are happy), and exploiting a bug in <code>#print axioms</code> so it says no axioms but axioms are used.</p>\n</blockquote>\n<p>Regarding the exploits:</p>\n<ul>\n<li>notation, using local instances, skipping kernel checking, changing keywords are all avoided by construction through this challenge solution mechanism afaict?</li>\n<li>adding fake theorems is avoided by replaying the environment into the kernel</li>\n<li>crashing lean in a right way is avoided because we interact with olean files produced by the solution file so if something is not in there we also don't believe it</li>\n<li><code>native_decide</code> will be as exploitable as ever if it is in your allowlist for axioms, if it is not you should be good</li>\n</ul>",
        "id": 546496835,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761148882
    },
    {
        "content": "<p>FYI, the training use case is quite important in practice, specifically for reinforcement learning, where we want to reward an AI only for correct proofs. This use case is sensitive to latency since the GPUs are idle while we check each batch of AI-generated proofs, and idle GPUs = big waste of money. So it would be very nice if the checker had an option to just trust some provided Mathlib oleans.</p>",
        "id": 546505791,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1761151533
    },
    {
        "content": "<p>... and actually perhaps also preload Mathlib (à la Pantograph) so that we don't have to load it for every sample. I have a tool that works like this (can maybe open-source at some point), but it would be nicer if the official checker could support this use case directly.</p>",
        "id": 546506205,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1761151696
    },
    {
        "content": "<p>I do understand that people are interested in this use case but in its current state this is not the type of service it aims to provide. It does a bit of additional work to ensure that really nothing fishy is going on under any circumstances. I do see ways to fit something like a fast mode into its architecture but this feels wrong to me. We end up in a situation where it is again not fully clear what running the official tool actually means unless you inspect it for yourself. In my mind this tool should be the ultimate judge where if you have the challenge file properly set up and point it at a set of theorems and a set of legal axioms it tries as hard as in any way possible to guarantee that the result is correct so that if you can say this tool checks your result there is no trapdoor, it is just correct.</p>",
        "id": 546514698,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761154480
    },
    {
        "content": "<p>I am very excited about this news! I have been working on a configurable verifier. While it will work standalone, the goal is to expose it as a tool to agents. lean4checker is a dependency; to reuse some of its battle-tested replay logic. Looking forward to integrating the new verifier eventually and throw out some custom code!</p>",
        "id": 546515901,
        "sender_full_name": "Oliver Dressler",
        "timestamp": 1761154858
    },
    {
        "content": "<p>I agree it is a tricky balance between completeness and speed.  I agree that the official tool should be the final arbitrator.  If someone claims to prove RH, we should know how to check it.</p>",
        "id": 546520201,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761156214
    },
    {
        "content": "<p>But I also agree with <span class=\"user-mention\" data-user-id=\"256311\">@Jannis Limperg</span>  that we need a fast tool as well, even if this isn't that one, or the official one.  Ideally it should still borrow as much from the official tooling so it is as trustworthy as speed allows.  Otherwise, without a fast tool, we will train our models to exploit weaknesses in Lean's front end, and not notice them.  (A simple example would be a model which receives feedback that a proof passed tactics, but failed the kernel, and then it just disables the kernel to make the proof pass.  This might be rare enough that it would go unnoticed that the model has this behavior.)</p>",
        "id": 546520220,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761156220
    },
    {
        "content": "<p>After your tool is released, I think there could be a lot of community support to modifying into a fast version (but less secure), maybe using the axiom trick you mentioned above.  But still the slow version should still be the default, and the one used to check anything important (final benchmark results, full proofs, any contract you are paying money for).</p>",
        "id": 546520288,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761156239
    },
    {
        "content": "<p>But also to be honest, I think we will find this challenge-solution approach will only go so far.  I think a large number of times the challenge will have mistakes in it.  And many other times we will be looking to judge a proof which never had a challenge to begin with.  The best we will be able to hope for is that we can distill a challenge post-hoc from the solution.</p>",
        "id": 546520378,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761156265
    },
    {
        "content": "<blockquote>\n<p>We end up in a situation where it is again not fully clear what running the official tool actually means unless you inspect it for yourself.</p>\n</blockquote>\n<p>The tradeoffs can be documented clearly, with the default being as secure as possible, and then we don't have a problem, no? If you're saying you don't want to make the checker too complex, I can understand that (complexity is the enemy of security), but then the complexity will just live in a non-official repo and some functionality will be duplicated. I can live with that, but I'd argue it's not the optimal outcome.</p>",
        "id": 546521380,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1761156589
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/FRO's.20new.20verifier/near/546520378\">said</a>:</p>\n<blockquote>\n<p>The best we will be able to hope for is that we can distill a challenge post-hoc from the solution.</p>\n</blockquote>\n<p>I guess another approach, if the theorem is easy enough to state, is to restate the theorem statement independently of the solution.  Make that the \"challenge\".  And then use the original solution to prove the new challenge, making a new solution which matches the new challenge.</p>",
        "id": 546522377,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761156900
    },
    {
        "content": "<p>What is a \"challenge\" in this context?</p>",
        "id": 546523812,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1761157355
    },
    {
        "content": "<p>Two things are important here I think, for one yes the complexity part but also the fact that if someone tells you it passes the checker there are still multiple levels to what that could mean which I would like to avoid. Besides that, the checker logic without all of the additional guard rails is 176 LoC. I guess we could share that if we want to but it's really not that fancy.</p>\n<p>I can give a brief primer of what it actually does, it's really not that fancy. The basic idea here is that you trust the challenge file to be fine, the solution file to be arbitrarily evil and from there the following things happen:</p>\n<ol>\n<li>we parse the config file which contains names of theorems to verify and an allowlist of axioms</li>\n<li>we compile and load the challenge file into the verifier proces</li>\n<li>we compile the solution file in a <code>landrun</code> sandbox to avoid whatever tricks an IO executing meta program in the solution file could perform</li>\n<li>we run lean4export on the solution file output in a separate process to extract the olean contents (also in a landrun sandbox), this way even if the attacker managed to produce an olean file that is able to exploit something (recall that olean files basically just contain pointers that we follow blindly) the best they will be able to do is exploit the lean4checker process which in turn can only send input to our verifier but nothing else. If they were e.g. running in the same process they might be able to pull a funky trick where they trick us into printing accept when we would not really accept</li>\n<li>The verifier parses the lean4export output</li>\n<li>(this is where the mentioned 176 LoC start) the verifier checks that no axioms that are not on the allowlist occur in the parsed solution</li>\n<li>the verifier checks that all constants in the types of the target theorems (and constants they use transitively) match between the two parsed environments</li>\n<li>we run the kernel on the solution</li>\n<li>we accept</li>\n</ol>\n<p>Really the thing that is taking the most time here is steps 1-5 which are precisely the high assurance steps. If you use the axiom trick 6 and 7 will be fast but then you basically throw away just about everything that is interesting about this tool :D</p>",
        "id": 546524686,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761157587
    },
    {
        "content": "<p><del>So this doesn't run any kernel on the proof terms (or am I missing that)?</del></p>",
        "id": 546525181,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761157757
    },
    {
        "content": "<p>Oh sorry I forgot, we do of course run the kernel</p>",
        "id": 546525212,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761157770
    },
    {
        "content": "<p>and note that because this already uses lean4export anyway one thing you can do for cheap in this setup is actually to just feed the already exported thing to multiple kernels</p>",
        "id": 546525621,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761157904
    },
    {
        "content": "<p>When we say this is slow, how slow are we talking?</p>",
        "id": 546526736,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761158201
    },
    {
        "content": "<p>Is it rechecking all of mathlib in a kernel for example?</p>",
        "id": 546526913,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761158260
    },
    {
        "content": "<p>Totally</p>",
        "id": 546526942,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761158269
    },
    {
        "content": "<p>I actually have no sense how slow that is. I know how slow it is to <em>build</em> Mathlib, but not to <em>check</em> it.</p>",
        "id": 546527262,
        "sender_full_name": "Alex Meiburg",
        "timestamp": 1761158376
    },
    {
        "content": "<p>Can this be cached?  Many solutions would use the same mathlib (and many even would use the same challenge).  Since this is happening in a separate process anyway, can we not safely cache our kernel check of mathlib.</p>",
        "id": 546527457,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761158440
    },
    {
        "content": "<p>With a fetched mathlib cache checking the erdos thing from today on our FRO dev machine with a AMD Ryzen 9 7950X3D 16-Core Processor takes 20 seconds</p>",
        "id": 546527512,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761158461
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/FRO's.20new.20verifier/near/546527457\">said</a>:</p>\n<blockquote>\n<p>Can this be cached?  Many solutions would use the same mathlib (and many even would use the same challenge).  Since this is happening in a separate process anyway, can we not safely cache our kernel check of mathlib.</p>\n</blockquote>\n<p>as I've alluded to multiple times above you can do this using the axiom trick that the module system uses as well</p>",
        "id": 546527586,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761158483
    },
    {
        "content": "<p>(is that actually fast? I feel like it's horribly slow)</p>",
        "id": 546530101,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761159360
    },
    {
        "content": "<p>If you only have to do it once then it's acceptable! (at least to me.) The answer I was fearing was \"an hour\".</p>",
        "id": 546531485,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1761159842
    },
    {
        "content": "<p>Oh I mean certainly acceptable for a final judge use yeah, but not for AI use lol</p>",
        "id": 546531557,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761159866
    },
    {
        "content": "<p>I don't really have an intutition for how long AI training cycles take but I would imagine it has to be in the 1s and below magnitude to be good</p>",
        "id": 546532115,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761160027
    },
    {
        "content": "<p>20 seconds on 16 cores is probably too slow for many RL applications (although one only has to check a proof if it builds, so the amortized time is lower if most of your proofs fail).  But for other applications, like checking final results, it isn't as bad as I thought it would be.  (And it is much faster than I thought you were going to say, so I might just be anchored)  Also, this \"axiom trick\", as much as you are discouraging it, seems to sound easy to setup, not that much off a soundness risk, and natural to do---so I think many would just find a way to do that for stuff that needs to be really fast.</p>",
        "id": 546533403,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761160493
    },
    {
        "content": "<p>No no its 20 seconds on 1 of those 16 cores, this whole process isn't really parallelizable well</p>",
        "id": 546533455,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761160515
    },
    {
        "content": "<p>That is better.</p>",
        "id": 546533920,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761160729
    },
    {
        "content": "<p>Is it though?</p>",
        "id": 546533962,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761160743
    },
    {
        "content": "<p>Whether you need x16 cores to make it this fast or just 1 shouldn't matter should it? The main point is that, and again this is just based on my experience with HPC, I would imagine 1 output cycle on your GPU is much faster than 20 seconds so this would be the bottle neck in the training feedback pipeline</p>",
        "id": 546534162,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761160818
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/channel/270676-lean4/topic/FRO's.20new.20verifier/near/546524686\">said</a>:</p>\n<blockquote>\n<p>The basic idea here is that you trust the challenge file to be fine, the solution file to be arbitrarily evil</p>\n</blockquote>\n<p>This sounds exactly like Mario's vision: <a href=\"https://github.com/digama0/mm0\">Metamath Zero</a><br>\nIs he involved in some way?</p>",
        "id": 546563226,
        "sender_full_name": "Snir Broshi",
        "timestamp": 1761173665
    },
    {
        "content": "<p>Do the theoretical attacks this avoids include ones based on <a href=\"https://github.com/digama0/lean4lean/blob/master/divergences.md\">Mario's list of divergences between lean4lean and the Lean kernel</a>? For example, anything involving an adversarial prelude used by the claimed solution (though maybe it's impossible to use an adversarial prelude and still pass the comparison of types to the challenge file).</p>\n<p>Is the comparison between the challenge and solution based on definitional equality or syntactic equality? (Mathematically it doesn't make much difference. Defeq may have advantages when changed imports mean instances are found through different paths that are defeq but not syntactically the same. Syntactic equality might be closer to the ideal if an answer is supposed to be given in an exact syntactic form, e.g. a numeric literal, rather than an expression that might be defeq to the desired answer.)</p>\n<p>Does the verifier handle the case where the challenge file has multiple <code>sorry</code>s to fill in, one depending on another (that is, a \"determine\" problem where an answer must be filled in, along with a proof of a type that references the answer, and a human is expected to check that the answer is legitimate rather than just restating the problem)? Or is it expected in such cases that the human checks legitimacy of the claimed answer first, and puts it in a copy of the challenge file, before running the verifier? (This is only an issue for problems where the correct answer could be expressed in many different legitimate forms rather than having one clear canonical form; IMO 2025 P4 is an example of such a problem. If there's a clear canonical form for the answer, say a numeric literal, then the challenge file used with the verifier can just give the answer in that form, and the solver can be told to put the answer in the canonical form in their solution.)</p>",
        "id": 546567306,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1761176240
    },
    {
        "content": "<p>If we do choose to implement a fast mode, can I request that we make sure it has a different name than the full version, so that there's absolutely no confusion? My worry is that if the full version is e.g. named the comparator, and we make another tool called the \"fast comparator\", then people might start saying that something has been checked by the comparator when it has only been checked by the fast comparator. Then if the fast comparator has a flaw in it, it will degrade trust in the full comparator just because they happen to share the same name.</p>",
        "id": 546595201,
        "sender_full_name": "Niels Voss",
        "timestamp": 1761198816
    },
    {
        "content": "<blockquote>\n<p>Do the theoretical attacks this avoids include ones based on <a href=\"https://github.com/digama0/lean4lean/blob/master/divergences.md\">Mario's list of divergences between lean4lean and the Lean kernel</a>? For example, anything involving an adversarial prelude used by the claimed solution (though maybe it's impossible to use an adversarial prelude and still pass the comparison of types to the challenge file).</p>\n</blockquote>\n<p>If your challenge file uses the built-in prelude a solution that modifies the prelude would indeed be detected yes. One situation that is imaginable is one where the challenge file does not use a part of the prelude to state the challenge but the solution does and can thus in principle modify this part of the prelude without being caught. I think this is quite an unlikely vector of attack given that most of these problems will likely contain the relevant parts of the prelude in their challenge. That being said, due to the architecture already using lean4export it is in principle quite easy to just feed it to another checker (like nanoda or lean4lean) if that checker also supports the export format so even this unlikely vector is totally fixable.</p>\n<blockquote>\n<p>Is the comparison between the challenge and solution based on definitional equality or syntactic equality? (Mathematically it doesn't make much difference. Defeq may have advantages when changed imports mean instances are found through different paths that are defeq but not syntactically the same. Syntactic equality might be closer to the ideal if an answer is supposed to be given in an exact syntactic form, e.g. a numeric literal, rather than an expression that might be defeq to the desired answer.)</p>\n</blockquote>\n<p>syntactic</p>\n<blockquote>\n<p>Does the verifier handle the case where the challenge file has multiple <code>sorry</code>s to fill in, one depending on another (that is, a \"determine\" problem where an answer must be filled in, along with a proof of a type that references the answer, and a human is expected to check that the answer is legitimate rather than just restating the problem)? Or is it expected in such cases that the human checks legitimacy of the claimed answer first, and puts it in a copy of the challenge file, before running the verifier? (This is only an issue for problems where the correct answer could be expressed in many different legitimate forms rather than having one clear canonical form; IMO 2025 P4 is an example of such a problem. If there's a clear canonical form for the answer, say a numeric literal, then the challenge file used with the verifier can just give the answer in that form, and the solver can be told to put the answer in the canonical form in their solution.)</p>\n</blockquote>\n<p>So you are talking about a situation like the following:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">MyType</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"gr\">sorry</span>\n\n<span class=\"kn\">theorem</span><span class=\"w\"> </span><span class=\"n\">bar</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">P</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"gr\">sorry</span>\n</code></pre></div>\n<p>yes? It would not accept such a problem at the moment because the definition of a constant used in the target theorem <code>bar</code> changed. I see two ways to address this:</p>\n<ol>\n<li>Additionally inject a list of things whose body need not be inspected. This is going in a similar direction as the axiom trick above really</li>\n<li>Rephrase the problem as something like <code>def bar : { x : MyType // P x}</code> and then give  that as a challenge?</li>\n</ol>\n<p><span class=\"user-mention silent\" data-user-id=\"521331\">Niels Voss</span> <a href=\"#narrow/channel/270676-lean4/topic/FRO's.20new.20verifier/near/546595201\">said</a>:</p>\n<blockquote>\n<p>If we do choose to implement a fast mode, can I request that we make sure it has a different name than the full version, so that there's absolutely no confusion? My worry is that if the full version is e.g. named the comparator, and we make another tool called the \"fast comparator\", then people might start saying that something has been checked by the comparator when it has only been checked by the fast comparator. Then if the fast comparator has a flaw in it, it will degrade trust in the full comparator just because they happen to share the same name.</p>\n</blockquote>\n<p>This was one of the two points I was trying to make above yes</p>",
        "id": 546607338,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761204550
    },
    {
        "content": "<p>You meant to write <code>theorem bar : P foo := sorry</code>, right?</p>",
        "id": 546670402,
        "sender_full_name": "Jason Rute",
        "timestamp": 1761223388
    },
    {
        "content": "<p>yes</p>",
        "id": 546672858,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1761224143
    }
]