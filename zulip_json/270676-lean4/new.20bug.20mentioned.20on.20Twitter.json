[
    {
        "content": "<p>I see mention of a ‚Äúsoundness bug‚Äù on X with no explanation.l by <span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span> and <span class=\"user-mention\" data-user-id=\"904624\">@James E Hanson</span> .  Is this discussed somewhere? <a href=\"https://x.com/elliotglazer/status/2005798788829700259?s=46\">https://x.com/elliotglazer/status/2005798788829700259?s=46</a></p>",
        "id": 565736061,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767059125
    },
    {
        "content": "<p>This is not a bug per-se.</p>",
        "id": 565736125,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767059217
    },
    {
        "content": "<p>You can always insert arbitrary declarations into the environment</p>",
        "id": 565736141,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767059234
    },
    {
        "content": "<p>It just won't pass lean4checker or external checkers where every declaration is replayed. I can see the potential for misinformation though</p>",
        "id": 565736229,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767059377
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"466334\">@Shreyas Srinivas</span> Perhaps there should be a setting on Lean 4 Web to analyze code the way lean4checker does?</p>",
        "id": 565736474,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767059812
    },
    {
        "content": "<p>That‚Äôs not my forte. But lean4checker doesn‚Äôt run in the editor afaik. What you see is a conscious design decision to not nuke the editor by replaying all declarations to check each new declaration. This has been discussed on Zulip before.</p>",
        "id": 565736660,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767059995
    },
    {
        "content": "<p>I understand that the editor is optimized for typical use rather than adversarial code, but surely it's feasible to install a \"rigorously check this proof\" button that exports the code to a checker and validates it?</p>",
        "id": 565736942,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767060216
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span>, I know you are very excited by Lean, but people also see you as a leader in this space because of your work on FrontierMath.  I think dropping that there is a ‚Äú soundness bug‚Äù on X with no prior discussion with the lean community about these issues is a bit rude to this community.</p>",
        "id": 565736999,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767060304
    },
    {
        "content": "<p><a class=\"message-link\" href=\"/#narrow/channel/113488-general/topic/PSA.20about.20trusting.20Lean.20proofs/near/392705540\">#general &gt; PSA about trusting Lean proofs @ üí¨</a></p>",
        "id": 565737221,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767060606
    },
    {
        "content": "<p>External checkers are necessarily going to take forever to run. So I am not sure it makes sense to  add such a command or button to the editor. It's basically going to grind to a halt for more than an hour and make the editor unusable.</p>",
        "id": 565737243,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767060655
    },
    {
        "content": "<p>You can run lean4checker from the terminal</p>",
        "id": 565737285,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767060692
    },
    {
        "content": "<p>Mathlib and all other projects handle this by running the external checker on the CI</p>",
        "id": 565737346,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767060781
    },
    {
        "content": "<p>To go slightly further, there is a property of proof assistants called \"Pollack consistency\", which basically no major proof assistant achieves : <a href=\"https://ammkrn.github.io/type_checking_in_lean4/future_work.html\">https://ammkrn.github.io/type_checking_in_lean4/future_work.html</a><br>\nTrusting lean or any other theorem prover is slightly non-trivial as other pages on that document mention. This is par for the course.</p>",
        "id": 565737428,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767060924
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I didn't say \"soundness bug\" in my public tweets. When crediting Jannis with the initial discovery, I used \"bug\" for lack of a better word. Perhaps \"web editor exploit\" would be better?</p>\n<p>I don't think it's unreasonable to disseminate an example that \"putting code in the web editor and checking that it compiles and checking that no axioms are added\" is insufficient to verify correctness of a Lean proof, considering that's the standard recent high-profile Lean proofs have been judged by. I myself didn't realize this was possible, having thought that the previously discussed exploits were either patched out, or caught with #print axioms like native_decide uses are.</p>",
        "id": 565737570,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767061110
    },
    {
        "content": "<p>I‚Äôm sorry.  You are right.  I mixed up a direct message you sent to me with your public post.  I take back what I said and am again sorry.</p>",
        "id": 565737642,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767061204
    },
    {
        "content": "<p>It's not an \"exploit\" either. It's a design trade off that can catch someone unaware if their idea of \"trust lean\" is \"lean says it is right so it is right\". It's an editor UX limitation. It also works on your local VSCode installation.</p>",
        "id": 565737672,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061240
    },
    {
        "content": "<p>You can also add all the tactics that use <code>Lean.ofReduceBool</code> to gotchas if we are talking about people inexperienced with ITPs. This one is just slightly worse because it doesn't show up in <code>#print_axioms</code></p>",
        "id": 565737710,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061273
    },
    {
        "content": "<p>But I still don‚Äôt like the modern Twitter post style of dropping something with no explanation.  But I guess this is my grip with the whole modern internet.</p>",
        "id": 565737732,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767061299
    },
    {
        "content": "<p>Basically trust is a complicated thing. The good news is, lean4 is still sound as far as this issue goes.</p>",
        "id": 565737821,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061411
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565737710\">said</a>:</p>\n<blockquote>\n<p>You can also add all the tactics that use <code>Lean.ofReduceBool</code> to gotchas if we are talking about people inexperienced with ITPs. This one is just slightly worse because it doesn't show up in <code>#print_axioms</code></p>\n</blockquote>\n<p>Would it be possible to add a warning to <code>addDeclCore</code> when the <code>doCheck</code> argument is used (in the same way that you get a warning whenever you use <code>native_decide</code>)?</p>",
        "id": 565737861,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767061486
    },
    {
        "content": "<p>You could add that and someone else could bypass addDeclCore altogether or just write a modified version without the warning.</p>",
        "id": 565737891,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061538
    },
    {
        "content": "<p>Sure, I'm not saying it would be foolproof.</p>",
        "id": 565737906,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767061567
    },
    {
        "content": "<p>It wouldn‚Äôt be fool-anything. You are just adding one small step</p>",
        "id": 565737926,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061593
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565737672\">said</a>:</p>\n<blockquote>\n<p>It's not an \"exploit\" either. It's a design trade off that can catch someone unaware if their idea of \"trust lean\" is \"lean says it is right so it is right\". It's an editor UX limitation. It also works on your local VSCode installation.</p>\n</blockquote>\n<p>Do you have a preferred lingo for describing this type of editor behavior to the public? There's a lot of people working right now to post AI-generated \"proofs\" of major conjectures and sooner or later someone is gonna use this exploit (even if it's just Claude deep diving the Zulip for \"insta-prove\" methods). I'm gonna be expected to make some sort of adjudication on those events, and it's much healthier to have that conversation with the public using an obviously intentional example BEFORE someone seriously says they've solved Riemann Hypothesis with 10k lines of code obscuring this exploit.</p>",
        "id": 565737936,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767061607
    },
    {
        "content": "<p>Well the specific term for this is \"environment hacking\" but I guess that's not what you're looking for, right?</p>",
        "id": 565737966,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767061645
    },
    {
        "content": "<p>Eh, I can go with \"environmental hack.\"</p>",
        "id": 565737987,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767061681
    },
    {
        "content": "<p>‚ÄúEditor UX limitation: The lean editor/elaborrator   doesn‚Äôt replay the environment. To be foolproof a proof must be run through the environment replay tool lean4checker. Trust is further improved by running external checkers‚Äù</p>",
        "id": 565738074,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565737732\">said</a>:</p>\n<blockquote>\n<p>But I still don‚Äôt like the modern Twitter post style of dropping something with no explanation.  But I guess this is my grip with the whole modern internet.</p>\n</blockquote>\n<p>I'll happily drop an explanation thread asap. What would be the most informative Zulip thread to cite?</p>",
        "id": 565738107,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767061818
    },
    {
        "content": "<p>I just posted it up there</p>",
        "id": 565738118,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061833
    },
    {
        "content": "<p>I don‚Äôt think you should post it though</p>",
        "id": 565738126,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061848
    },
    {
        "content": "<p>Just write a brief explanation and link to the doc I posted</p>",
        "id": 565738136,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061863
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565737428\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://ammkrn.github.io/type_checking_in_lean4/future_work.html\">https://ammkrn.github.io/type_checking_in_lean4/future_work.html</a></p>\n</blockquote>",
        "id": 565738150,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061890
    },
    {
        "content": "<p>Also I wrote about this in the equational theories project paper</p>",
        "id": 565738164,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061902
    },
    {
        "content": "<p>Nothing new per se. But important info for mathematicians.</p>",
        "id": 565738179,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767061922
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565737243\">said</a>:</p>\n<blockquote>\n<p>External checkers are necessarily going to take forever to run. So I am not sure it makes sense to  add such a command or button to the editor. It's basically going to grind to a halt for more than an hour and make the editor unusable.</p>\n</blockquote>\n<p>This doesn't convince me the feature would be a bad idea. Its use case is precisely \"random onlooker sees some code and wants to know if it proves what it says it does.\" If that requires them to ignore a tab for an hour, so be it.</p>",
        "id": 565738270,
        "sender_full_name": "Elliot Glazer",
        "timestamp": 1767062049
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span> We have canonical ways (<code>lean4checker</code>, etc.) of checking that metaprogramming shenanigans have not created a situation that is a kernel error on replaying the environment. It's really not been a focus on even AI input because something in the <code>Lean.Kernel</code> is obscure enough that I've never seen a model try this, but I agree it could be a future concern. This is part of why I hope that we can standardize on one officially supported tool (maybe Comparator now?) that checks for these sorts of things.</p>",
        "id": 565738300,
        "sender_full_name": "Chris Henson",
        "timestamp": 1767062070
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"687698\">@Chris Henson</span> an AI model just needs to insert that snippet in the middle of 25000 lines of proof.</p>",
        "id": 565738331,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767062125
    },
    {
        "content": "<p>And if it‚Äôs available on the web‚Ä¶</p>",
        "id": 565738338,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767062135
    },
    {
        "content": "<p>I fully agree, just citing my anecdotal experience that they don't seem to do so currently and that if they do in the future we have appropriate tools to point people to.</p>",
        "id": 565738414,
        "sender_full_name": "Chris Henson",
        "timestamp": 1767062244
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"1007402\">Elliot Glazer</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565738270\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565737243\">said</a>:</p>\n<blockquote>\n<p>External checkers are necessarily going to take forever to run. So I am not sure it makes sense to  add such a command or button to the editor. It's basically going to grind to a halt for more than an hour and make the editor unusable.</p>\n</blockquote>\n<p>This doesn't convince me the feature would be a bad idea. Its use case is precisely \"random onlooker sees some code and wants to know if it proves what it says it does.\" If that requires them to ignore a tab for an hour, so be it.</p>\n</blockquote>\n<p>It would be a UX nightmare. Much easier to use the CI action (which can be run locally in vscode).</p>",
        "id": 565738422,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767062255
    },
    {
        "content": "<p>Technically you need to have lean4checker as a project dependency which is not the default thing in the math template. So that would have to happen before making the button useful.</p>",
        "id": 565738574,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767062434
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565738574\">said</a>:</p>\n<blockquote>\n<p>Technically you need to have lean4checker as a project dependency</p>\n</blockquote>\n<p>Is this documented somewhere? <a href=\"https://github.com/leanprover/lean4checker\">I didn't see this on the readme of the lean4checker github repo.</a></p>",
        "id": 565738758,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767062708
    },
    {
        "content": "<p>Right sorry my bad. I had to check the etp repo</p>",
        "id": 565738977,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767062983
    },
    {
        "content": "<p>You can download lean4checker and build it and then run it in your project directory</p>",
        "id": 565738991,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767063008
    },
    {
        "content": "<p>Sorry I am out with friends and not able to respond quickly (or read everything), but the feature you want from the editor <span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span>  is more or less built into the blockchain project by <span class=\"user-mention\" data-user-id=\"776090\">@GasStationManager</span>.  It uses SafeVerify so it also checks axioms and that terms match the target theorem.</p>",
        "id": 565739009,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767063034
    },
    {
        "content": "<p>Same goes for lean4lean which can serve as an external checker. (Boosting lean4lean here tbh)</p>",
        "id": 565739046,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767063068
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span> : you could in-principle try writing a separate lean4checker vscode extension and see how it plays with the code. I find it much easier to write a new extension than make PRs to the lean4 extension to try out ideas.</p>",
        "id": 565739089,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767063142
    },
    {
        "content": "<p>It shouldn‚Äôt be hard. It took me &lt; 10 hours to (re)-learn typescript and write the loogle lean extension. I am guessing it‚Äôs simpler with AI tools.</p>",
        "id": 565739126,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767063206
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span> <span class=\"user-mention\" data-user-id=\"904624\">@James E Hanson</span>  Just to be clear, just running <code>lean4checker</code>, while much better, it will not guard against all the practical issues you are both worried about.  These are two simple ways to give a very fake proof of Fermat's last theorem.  Both will \"pass\" <code>lean4checker</code>, since <code>lean4checker</code> does not check that the statement of the theorem is correct.</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"c1\">-- casually insert another hypotheses into the theorem statement</span>\n<span class=\"kn\">variable</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">nothing_to_see_here</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">False</span><span class=\"o\">)</span>\n<span class=\"n\">include</span><span class=\"w\"> </span><span class=\"n\">nothing_to_see_here</span><span class=\"w\"> </span><span class=\"k\">in</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">fermats_last_theorem</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"o\">)</span>\n<span class=\"w\">  </span><span class=\"o\">(</span><span class=\"n\">hn</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">hx</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">hy</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">hz</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span>\n<span class=\"w\">  </span><span class=\"n\">x</span><span class=\"bp\">^</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"bp\">^</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">‚â†</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"bp\">^</span><span class=\"n\">n</span>\n<span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n<span class=\"w\">  </span><span class=\"n\">contradiction</span>\n\n<span class=\"bp\">#</span><span class=\"n\">print</span><span class=\"w\"> </span><span class=\"n\">axioms</span><span class=\"w\"> </span><span class=\"n\">fermats_last_theorem</span>\n<span class=\"c1\">-- 'fermats_last_theorem' does not depend on any axioms</span>\n\n<span class=\"bp\">#</span><span class=\"n\">check</span><span class=\"w\"> </span><span class=\"n\">fermats_last_theorem</span>\n<span class=\"c1\">-- fermats_last_theorem (nothing_to_see_here : False) (n x y z : Nat) (hn : n &gt; 2) (hx : x &gt; 0) (hy : y &gt; 0) (hz : z &gt; 0) :</span>\n<span class=\"c1\">--  x ^ n + y ^ n ‚â† z ^ n</span>\n</code></pre></div>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"c1\">-- casually change the meaning of `+`</span>\n<span class=\"kn\">instance</span><span class=\"w\"> </span><span class=\"n\">nothingToSeeHere</span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Add</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"bp\">‚ü®Œª</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"bp\">‚ü©</span>\n\n<span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">fermats_last_theorem</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">Nat</span><span class=\"o\">)</span>\n<span class=\"w\">  </span><span class=\"o\">(</span><span class=\"n\">hn</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">hx</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">hy</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">hz</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"w\"> </span><span class=\"bp\">&gt;</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span>\n<span class=\"w\">  </span><span class=\"n\">x</span><span class=\"bp\">^</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"bp\">^</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">‚â†</span><span class=\"w\"> </span><span class=\"n\">z</span><span class=\"bp\">^</span><span class=\"n\">n</span>\n<span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span>\n<span class=\"w\">  </span><span class=\"k\">have</span><span class=\"w\"> </span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"bp\">^</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">+</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"bp\">^</span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"bp\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"k\">by</span><span class=\"w\"> </span><span class=\"n\">rfl</span>\n<span class=\"w\">  </span><span class=\"n\">grind</span>\n\n<span class=\"bp\">#</span><span class=\"n\">print</span><span class=\"w\"> </span><span class=\"n\">axioms</span><span class=\"w\"> </span><span class=\"n\">fermats_last_theorem</span>\n<span class=\"c1\">-- 'fermats_last_theorem' depends on axioms: [propext, Classical.choice, Quot.sound]</span>\n\n<span class=\"bp\">#</span><span class=\"n\">check</span><span class=\"w\"> </span><span class=\"n\">fermats_last_theorem</span>\n<span class=\"c1\">-- fermats_last_theorem (n x y z : Nat) (hn : n &gt; 2) (hx : x &gt; 0) (hy : y &gt; 0) (hz : z &gt; 0) : x ^ n + y ^ n ‚â† z ^ n</span>\n</code></pre></div>",
        "id": 565770882,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767090462
    },
    {
        "content": "<p><strong>Neither is a bug in Lean.  They are both intended behavior.</strong>   But the existence of both means that one has to be careful scrutinizing claimed Lean results.  The gold standard right now is something like SafeVerify or Comparator which checks that the theorem statement matches a target.  Hopefully the target is simple enough to understand and vet.</p>",
        "id": 565770895,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767090473
    },
    {
        "content": "<p>But as others have said, there is a tradeoff of user friendliness to security.  The web editor is intended for user friendliness.  If someone drops a Lean \"proof\" of a millennial problem on our lap, this community is probably equipped to quickly vet it even if no single online tool is.  (But if someone did want to try to make such an online tool, something like <a href=\"https://theorem-marketplace.com/\">https://theorem-marketplace.com/</a> would come close.  It would be a lot harder to trick.  It might just be slow to run.)</p>",
        "id": 565770899,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767090474
    },
    {
        "content": "<p>Also, both <a href=\"https://github.com/GasStationManager/SafeVerify/tree/main/SafeVerifyTest\">https://github.com/GasStationManager/SafeVerify/tree/main/SafeVerifyTest</a> and <a href=\"https://github.com/leanprover/lean4checker/tree/master/Lean4CheckerTests\">https://github.com/leanprover/lean4checker/tree/master/Lean4CheckerTests</a> have long lists of tests showing the sort of hacks that are known (and for which these tools guard against).</p>",
        "id": 565775623,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767093327
    },
    {
        "content": "<p>Could we at least have some linters that detect suspicious code, like <code>False</code> prop injection, <code>Override</code> of definitions, <code>env</code> injection ?<br>\nWhile not fool proof, they would be highlighted in UX with a warning or error.</p>",
        "id": 565781422,
        "sender_full_name": "Alfredo Moreira-Rosa",
        "timestamp": 1767096936
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"1007402\">@Elliot Glazer</span> <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> <span class=\"user-mention\" data-user-id=\"904624\">@James E Hanson</span> I think this kind of experiments that Elliot is advertising are important for people to realize one has to be careful with Lean 4 code. The recent example from Elliot should be considered in the category where the famous C obfuscation competition lies. I find these kind of bugs much less harmless than citing a full-fledged GPT hallucinated paper as a complete evidence. BTW: I am formalizing with Aristotle a part of Bourbaki's book. So far it's going smooth but I've learned a ton of Lean by just doing inspections of the auto-formalized code and being in general suspicious. Elliot's example give me this extra warning flag every time I post Lean code. So overall I think it's a very positive effect for the specialists. I agree that for general audience it's better to keep very positive attitude. Please also let me know if I do something wrong in the public space. I want to advertise Lean in the best possible way so that more people get engaged. Cheers!</p>",
        "id": 565787435,
        "sender_full_name": "Bartosz Naskrƒôcki",
        "timestamp": 1767100409
    },
    {
        "content": "<p>The point here is ‚Äúsoundness bug‚Äù means something much more serious (although it can happen from time to time). That is why Jason is emphasising correct communication. </p>\n<p>An ITP is ultimately checking proofs for theorems that are stated in its logic. Any reasonably powerful  logical calculus allows statements that don‚Äôt make much mathematical sense (like <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mo>‚àà</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">2 \\in 3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6835em;vertical-align:-0.0391em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">‚àà</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">3</span></span></span></span> in zfc) . It can‚Äôt figure out human intent. This is why I don‚Äôt take AI systems seriously beyond assisting in the proof of given statements.  Being impressively capable on occasion is less important than being systematically correct at understanding math as mathematicians understand it. That is not easy even for humans.</p>",
        "id": 565788070,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767100872
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565788070\">said</a>:</p>\n<blockquote>\n<p>The point here is ‚Äúsoundness bug‚Äù means something much more serious (although it can happen from time to time).</p>\n</blockquote>\n<p>As an aside, I don't really buy the idea that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mo>‚àà</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">2 \\in 3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6835em;vertical-align:-0.0391em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">‚àà</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">3</span></span></span></span> doesn't make mathematical sense to be completely honest. People just <em>say</em> that it doesn't all the time.</p>",
        "id": 565807823,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767112416
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565788070\">said</a>:</p>\n<blockquote>\n<p>The point here is ‚Äúsoundness bug‚Äù means something much more serious (although it can happen from time to time).</p>\n</blockquote>\n<p>I understand the rationale for why this is the way that it is and I can accept that this is a reasonable design philosophy for usability, but from the point of view of an outsider it really looks pretty indistinguishable from a soundness bug.</p>",
        "id": 565808380,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767112765
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565788070\">said</a>:</p>\n<blockquote>\n<p>An ITP is ultimately checking proofs for theorems that are stated in its logic.</p>\n</blockquote>\n<p>This also isn't really what's happening here. Environment hacking isn't the same kind of thing as a junk theorem.</p>\n<p>No formal presentation of type theory includes a proviso that you're allowed to just inject unproven theorems freely in the middle of a proof.</p>",
        "id": 565808687,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767112978
    },
    {
        "content": "<p>I would say this is more analogous to the fundamental soundness issue with definitions in Metamath (i.e., the fact that definitions must be given as new axioms and there's no built-in system that checks the conservativity of these axioms although it's relatively easy to check with external tools).</p>",
        "id": 565808993,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767113194
    },
    {
        "content": "<p>Obviously this depends on what counts as the 'official' proof-checker, but, again, I'm coming at this from the point of view of an outsider. For an outsider, when you boot up a local copy of Lean in your IDE or look at the web playground, it's pretty reasonable to think that that <em>is</em> the 'official' context in which proofs are verified by Lean.</p>",
        "id": 565809469,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767113543
    },
    {
        "content": "<p>There will always be an ‚Äúoutsider‚Äù problem with a tool of any sophistication.</p>",
        "id": 565811069,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767114493
    },
    {
        "content": "<p>Those who choose to learn and use it will necessarily have to learn the pitfalls.</p>",
        "id": 565811694,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767114900
    },
    {
        "content": "<p>I sympathize a lot with your concerns <span class=\"user-mention\" data-user-id=\"904624\">@James E Hanson</span>.  I think ultimately this is a communication challenge.  Communicating the design decisions of Lean, what correctness means, and how to achieve it.  Here is how I see it (which are not universally shared opinions):</p>\n<ul>\n<li>Lean values flexibility and meta-programming, which will mean there are always going to be ways to hack it.  Reigning this in doesn't seem to be a strong priority of the Lean community, which is fine, but needs to be communicated.</li>\n<li>There are levels of correctness. For a lot of needs, the editor is a good measure of correctness, and there are further levels with more guarantees.</li>\n<li>Similarly there are different levels of \"soundness\".  I agree with you, it is not black and white, but grey.  However the term \"soundness\" has a lot of sensitivity in this community.  A \"soundness bug\" communicates something that Leo has to get up in the middle of the night to fix.  I this community it is enforced to mean a bug in the kernel which can be used to prove false at the term level.  Anything short of that is considered at worst a bug, and at best a design decision.  (I got yelled at for this once.)</li>\n<li>But short of a kernel bug, there are tools to get stronger guarantees in Lean.</li>\n<li>The main tool which is advertised is <code>lean4checker</code>.  It is not that difficult to use.  Short of a kernel soundness bug, <code>lean4checker</code> will say correctly if a term proof is valid or not.</li>\n<li>Another tool is the set of external checkers which like, lean4checker, check the term proof.  (None are really that independent of <code>lean4checker</code> and all I think have the same major design decision which is to use bigint packages for natural number reduction.  In Lean 3 it used to be possible to use Rocq as an external checker, but I don't know if this is true in Lean 4.)</li>\n<li>But <code>lean4checker</code> (and external checkers) aren't sufficient for all needs.  The largest need it is missing is the checking of axioms.  It just doesn't.  Sure there is <code>#print axioms</code> but that has weaker <del>soundness</del> <em>correctness</em> guarantees.  <code>#print axioms</code> once had a bug (at least in the online editor) and it also can be environment hacked.  At one point <span class=\"user-mention\" data-user-id=\"310045\">@Eric Wieser</span> proposed Lean add some kind of axiom checking to <code>lean4checker</code>.  (Note, <code>lean4checker</code> does check for <code>native_decide</code>axioms, but that is only because those axioms can't be executed in the kernel.)</li>\n<li>Another problem with lean4checker is that it doesn't check that your theorem even exists at the term level.  You can just delete the theorem from the environment (and in at least one previous bug, a certain error in the proof caused Lean to skip the theorem silently).  <code>#print axioms</code> helps here, since <code>#print</code> and <code>#print axioms</code> would fail if the theorem doesn't exist (short of environment hacking of the print system), but the best would be a way to check that the theorem exists at the term level.</li>\n<li>Finally, one needs to check that the final theorem is correct and not misformalized.  This is a problem in all theorem provers, including metamath, but is trickier in Lean since there are so many ways to change the meaning of a theorem statement just by putting code in front of it.  (See my above two examples.)</li>\n<li>The gold standards in checking Lean are SafeVerify and Comparator which check for all of the above issues (term proof correctness, axioms, the theorem exists, and that the theorem statement is not hacked).  Comparator (by the Lean FRO) is also careful about code execution (since running code could have malicious effects in the most extreme cases).  But this also requires a very different way to write your Lean code where you separate out the main thing you are trying to prove and the proof into two different files.  But I think as Lean is used more in conjunction with AI and as a tool for correctness, it makes sense.</li>\n<li>(Also, as for checking that a theorem statement is correct, there is the obvious method of inspecting the statement by hand, but another useful method used in the Liquid tensor project is to prove auxiliary statements about all the definitions used in the theorem.  For example, proving that <code>Real</code> is a complete real closed field, and similar.)</li>\n<li>So in summary, there are many levels of \"proof\" in Lean, not just one.  For most applications the editor is fine, but ultimately should be checked with more scrutiny with anything important.  Some of these levels of scrutiny are more work and not agreed-upon standards.</li>\n<li>Communication is the most important thing.  A good start to that is the document <a href=\"https://leanprover-community.github.io/did_you_prove_it.html\">https://leanprover-community.github.io/did_you_prove_it.html</a>, (which we are having a discussion about in <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/channel/113488-general/topic/Standards.20for.20Lean.20proofs.20of.20unsolved.20problems/with/565812993\">#general &gt; Standards for Lean proofs of unsolved problems</a>).</li>\n</ul>",
        "id": 565819296,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767119178
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565819296\">said</a>:</p>\n<blockquote>\n<ul>\n<li>Communication is the most important thing.</li>\n</ul>\n</blockquote>\n<p>Everything you're saying makes sense to me. I just think that it's fundamentally at odds with the rhetoric and marketing that is commonly employed by people promoting Lean. For instance, people often talk about Lean eliminating the need for peer review of papers, because with a Lean proof in hand you can be sure that the theorem is true, or something like that. Or just look at this statement guaranteeing \"absolute correctness in mathematical proofs\" that is prominently displayed on <a href=\"https://lean-lang.org/\">lean-lang.org</a></p>\n<p><a href=\"/user_uploads/3121/BEJ0DH78yK76GcsYwrpQfZPu/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/BEJ0DH78yK76GcsYwrpQfZPu/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"476x324\" src=\"/user_uploads/thumbnail/3121/BEJ0DH78yK76GcsYwrpQfZPu/image.png/840x560.webp\"></a></div><p>and contrast it with your statements here</p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565819296\">said</a>:</p>\n<blockquote>\n<ul>\n<li>There are levels of correctness. For a lot of needs, the editor is a good measure of correctness, and there are further levels with more guarantees.</li>\n<li>Similarly there are different levels of \"soundness\". I agree with you, it is not black and white, but grey.</li>\n</ul>\n</blockquote>\n<p>I understand that on a technical level the statement from <a href=\"http://lean-lang.org\">lean-lang.org</a> is referring to the kernel and your statements are about the editor, but I'm talking about the rhetorical nature of these statements rather than their technical content.</p>",
        "id": 565822422,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767121246
    },
    {
        "content": "<p>On a pedantic level, the statement on <a href=\"http://lean-lang.org\">lean-lang.org</a> is just false. There have now been at least two kernel-level soundness bugs in Lean, so we know for a fact that the Lean kernel did not provide a guarantee of \"absolute correctness in mathematical proofs\" before May of this year, at best.</p>",
        "id": 565822548,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767121328
    },
    {
        "content": "<p>Well... Firstly lean is not going to eliminate the need for peer review. It‚Äôs adoption will definitely change how peer review works and possibly even change what questions are considered ‚Äúinteresting‚Äù</p>",
        "id": 565822674,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121407
    },
    {
        "content": "<p>Your phrasing suggests that you think it's a given that Lean will have mass adoption among mathematicians.</p>",
        "id": 565822845,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767121519
    },
    {
        "content": "<p>It already has adoption in the PL world and I am extrapolating from that. Secondly I am not a lawyer.</p>",
        "id": 565822898,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121556
    },
    {
        "content": "<p>Secondly, there is always room for human error.</p>\n<p>Thirdly, lean is only as reliable as the system you run it in.</p>",
        "id": 565822918,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121566
    },
    {
        "content": "<p>If your operating system is malicious and alters how lean runs for example, you can‚Äôt do anything about it.</p>",
        "id": 565822984,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121603
    },
    {
        "content": "<p>The guarantees of lean are in the same spirit as Signal‚Äôs guarantees of security and privacy.  If someone hijacks your phone, it doesn‚Äôt hold.</p>",
        "id": 565823053,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121641
    },
    {
        "content": "<p>I feel like these statements don't contradict my point. The claim regarding \"absolute correctness\" made on the front page of <a href=\"http://lean-lang.org\">lean-lang.org</a> is false.</p>",
        "id": 565823195,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767121727
    },
    {
        "content": "<p>Trying to seek absolute correctness in a three line snippet is an exercise in futility.</p>",
        "id": 565823285,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121772
    },
    {
        "content": "<p>So you're telling me that interpreting the statement made on the front page of <a href=\"http://lean-lang.org\">lean-lang.org</a> <em>as it is literally written</em> is an exercise in futility?</p>",
        "id": 565823350,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767121800
    },
    {
        "content": "<p>Yes. And that applies to the front page of anything</p>",
        "id": 565823454,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121854
    },
    {
        "content": "<p>So I'm being silly for thinking that the statement</p>\n<blockquote>\n<p>Lean's minimal trusted kernel guarantees absolute correctness in mathematical proof, software and hardware verification.</p>\n</blockquote>\n<p>is saying that Lean guarantees absolute correctness in mathematical proofs?</p>",
        "id": 565823533,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767121887
    },
    {
        "content": "<p>It guarantees correctness of proofs upto </p>\n<ol>\n<li>Soundness of the type theory w.r.t to a relevant model</li>\n<li>Correctness of the kernel‚Äôs implementation </li>\n<li>Correct behavior of your system (and a bunch of things listed by Mario in the document I linked above)</li>\n</ol>",
        "id": 565823673,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767121985
    },
    {
        "content": "<p>And for the record, this is and will remain true of any system claiming any guarantee of correctness ever.</p>",
        "id": 565823852,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767122105
    },
    {
        "content": "<p>Here is a full list : <a href=\"https://ammkrn.github.io/type_checking_in_lean4/trust/trust.html\">https://ammkrn.github.io/type_checking_in_lean4/trust/trust.html</a></p>",
        "id": 565824184,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767122302
    },
    {
        "content": "<p>As you pointed out, the phrase on the website, says : <br>\n<em>\"Lean's minimal trusted kernel guarantees absolute correctness in mathematical proof, software and hardware verification.\"</em></p>\n<p>This means : <br>\n<em>\"If a mathematical statement is correct AND it's context is correct, then the Lean kernel will garanty that a validated proof is correct\"</em>.</p>\n<p>And maybe it's a better statement to have in landing page of Lean. But i don't think it's a <code>marketing lie</code> from the community. More of an oversight.</p>\n<p>So Lean is a tool for mathematicians to check THEIR proofs. If they trust someone, it can also extend to trust OTHERS proofs (like for Mathlib). But cheaters will always find ways to cheat. And lean don't give strong garanties in this category.<br>\nAnd of course that's why we have tools like <code>comparator</code> and <code>safeVerify</code> to address part of these concerns.</p>\n<p>It's also important to note that all the examples shown shown at the beginning of the thread and the tweet, don't refute the statement :</p>\n<ul>\n<li>In the tweet exemple, is just obfuscation to forge a <code>False</code> statement by telling lean to bypass type checking. So the context is not correct anymore.</li>\n<li>in the first exemple given by Jason, a false statement in injected in the proof context. So again the context not correct anymore.</li>\n<li>in the second exemple, a Mathematical definition is overriden. So the Statement is not correct.</li>\n</ul>\n<p>But in none of these cases did the kernel validated a correct statement given a correct context.</p>\n<p>And of course, it would be nice to have better integrated tooling to detect these adversarial attacks. And lean will get better at this, given the understandable cristicism you and others are raising.</p>",
        "id": 565832928,
        "sender_full_name": "Alfredo Moreira-Rosa",
        "timestamp": 1767128271
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"942100\">Alfredo Moreira-Rosa</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565832928\">said</a>:</p>\n<blockquote>\n<p>But in none of these cases did the kernel validated a correct statement given a correct context.</p>\n</blockquote>\n<p>These examples were not the basis of me saying that the claim on <a href=\"http://lean-lang.org\">lean-lang.org</a> was incorrect. I was specifically referring to the instances of actual Kernel-level soundness bugs.</p>",
        "id": 565840765,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767135505
    },
    {
        "content": "<p>On a pedantic level, we know that the statement was incorrect before May of this year. Lean's kernel simply did not guarantee absolute correctness in mathematical proofs before that point in time.</p>",
        "id": 565840900,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767135579
    },
    {
        "content": "<p>I feel like this isn't really a debatable point.</p>",
        "id": 565840916,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767135591
    },
    {
        "content": "<p>Which soundness bugs are you referring to?</p>",
        "id": 565841247,
        "sender_full_name": "Damiano Testa",
        "timestamp": 1767135906
    },
    {
        "content": "<p>The two found by Mario. <a href=\"#narrow/channel/270676-lean4/topic/Soundness.20bug.3A.20hasLooseBVars.20is.20not.20conservative/near/521286338\">The more clear-cut one is the issue with Level.data,</a> which was back in May. <a href=\"#narrow/channel/270676-lean4/topic/soundness.20bug.3A.20native_decide.20leakage/near/395967589\">The older one involved <code>native_decide</code> being able to prove <code>False</code> without reporting any axioms.</a></p>",
        "id": 565841429,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767136137
    },
    {
        "content": "<p>The second one involving <code>native_decide</code> is not really a Kernel-level bug: <code>native_decide</code> involved trusting much more than the kernel.</p>",
        "id": 565841797,
        "sender_full_name": "Damiano Testa",
        "timestamp": 1767136587
    },
    {
        "content": "<p>It's still a kernel soundness issue. The kernel was trusting the compiler without reporting that it was doing so.</p>",
        "id": 565841964,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767136770
    },
    {
        "content": "<p>But regardless, even if that one is not as clear-cut, the first one is unambiguously a kernel soundness bug, and that's enough for the statement on <a href=\"http://lean-lang.org\">lean-lang.org</a> to have been incorrect.</p>",
        "id": 565841999,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767136811
    },
    {
        "content": "<p>\"Absolute correctness\" is an extremely strong claim.</p>",
        "id": 565842023,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767136848
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"904624\">@James E Hanson</span> I‚Äôm not saying this will help at all with your worries about Lean lying to the mathematical community, but I think you would be very interested in <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> ‚Äòs ongoing lean4lean work formally proving the correctness of the Lean kernel.  <a href=\"https://github.com/digama0/lean4lean\">https://github.com/digama0/lean4lean</a>  Unlike the Lean advertising, Mario is very careful with his claims and is one of the people most concerned by correctness, not just of Lean but many other provers as well.  If you search for lean4lean on Zulip you will find some progress reports from Mario on the project as well as his thoughts on how bug-free the Lean kernel currently is.</p>",
        "id": 565856166,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767153539
    },
    {
        "content": "<p><a href=\"#narrow/channel/236446-Type-theory/topic/Lean4Lean.20progress/with/541108980\">https://leanprover.zulipchat.com/#narrow/channel/236446-Type-theory/topic/Lean4Lean.20progress/with/541108980</a></p>",
        "id": 565856488,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767153680
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"687698\">Chris Henson</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565738414\">said</a>:</p>\n<blockquote>\n<p>I fully agree, just citing my anecdotal experience that they don't seem to do so currently and that if they do in the future we have appropriate tools to point people to.</p>\n</blockquote>\n<p>I just wanted to mention that I have already seen AI models attempt to \"escape the box\" when proving theorems in Lean.  They haven't been Lean kernel soundness bugs yet, of course, but it seems that a sufficiently smart LLM + RL model will, after enough compute, at some point attempt to use a bug/oversight in whatever sandbox/environment they're in to get their reward for falsely proving something.  I've seen ~3 different instances of this.</p>",
        "id": 565904721,
        "sender_full_name": "Boris Alexeev",
        "timestamp": 1767193122
    },
    {
        "content": "<p>In another thread (since this one is a mess) would you be willing to document the cases <span class=\"user-mention\" data-user-id=\"419930\">@Boris Alexeev</span>?  I would be very interested.</p>",
        "id": 565904964,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767193209
    },
    {
        "content": "<p>Maybe in <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/channel/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a></p>",
        "id": 565905087,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767193235
    },
    {
        "content": "<p>Eh, I think they're mostly artifacts of whatever checking environment is used.  I think one example can get the idea of these across quickly:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">result_statement</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">foo</span><span class=\"w\"> </span><span class=\"n\">bar</span>\n<span class=\"n\">unsafe</span><span class=\"w\"> </span><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">my_sorry</span><span class=\"w\"> </span><span class=\"o\">{</span><span class=\"n\">P</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"o\">}</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">P</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">unsafeCast</span><span class=\"w\"> </span><span class=\"o\">()</span>\n<span class=\"n\">unsafe</span><span class=\"w\"> </span><span class=\"kn\">def</span><span class=\"w\"> </span><span class=\"n\">result_proof</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">result_statement</span><span class=\"w\"> </span><span class=\"o\">:=</span><span class=\"w\"> </span><span class=\"n\">my_sorry</span>\n</code></pre></div>",
        "id": 565905598,
        "sender_full_name": "Boris Alexeev",
        "timestamp": 1767193575
    },
    {
        "content": "<p>Well, it‚Äôs <code>unsafe</code></p>",
        "id": 565905645,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767193608
    },
    {
        "content": "<p>See <a href=\"https://lean-lang.org/doc/reference/latest/Definitions/Recursive-Definitions/#unsafe\">here</a></p>",
        "id": 565905838,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767193730
    },
    {
        "content": "<p>Right, and it doesn't pass normal checks that we do.<br>\nBut my claim is that when people set up RL environments for these Lean agents, (1) often times they're checking weaker properties and (2) the agents <em>are</em> trying to hack them.<br>\nOver time, the environments are hardened against whatever exploits you know about, but I'm just saying that (2) is already happening.</p>",
        "id": 565905883,
        "sender_full_name": "Boris Alexeev",
        "timestamp": 1767193762
    },
    {
        "content": "<p>That's a problem with the relevant python API then. it can always check these things</p>",
        "id": 565906029,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767193860
    },
    {
        "content": "<p>Yeah, I think on top of whatever issues Lean does or does not have, some AI labs are not very good at designing robust RL lean environments.  That is how for example, the DeepSeek bug got through.  If they had done something as simple as print axioms they would have caught it.  Again I think there isn‚Äôt good communication about what correctness means in Lean, but I also think some blame falls on the AI labs as well (at least if they release proofs with easy to find exploits).  I hope in the near future we can have good standards around this, not onerous and brittle standards with banned keywords, but robust, principled standards like what Comparator is trying to do.  (Actually RL environments may have to take shortcuts for speed, but they should understand the limitations of those shortcuts and know how to vet their final results before announcing them.)</p>",
        "id": 565907299,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767194856
    },
    {
        "content": "<p>I think it's worth asking for something like <a href=\"https://ammkrn.github.io/type_checking_in_lean4/trust/trust.html\">the external checkers guide</a>, which contains a nice account of trust, to be included in the lean-lang reference manual</p>",
        "id": 565908158,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1767195407
    },
    {
        "content": "<p>My opinion (which is buried above) is that accounts of correctness and trust in Lean (including that one) forget about practical issues like axioms, theorems not making it into the environment, and theorem statements changing by other stuff in the code before the theorem.  External checkers are only as good as the terms they are checking, and it is easy in Lean to get the wrong terms. No external checker would have caught the DeepSeek bug or the unsafe bug above because those theorems are not exported or they rely on weird axioms.  (I know this doc talks about Pollack consistency, but that seems like a throw away comment.  Pollack consistency is the most common issue.  And tools like Comparator and SafeVerify are practical solutions which really help increase trust about that issue.)</p>",
        "id": 565910554,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767196929
    },
    {
        "content": "<p>I will add something to the book expanding on this and some of the potential solutions; there are two related but separate concerns here. The first is \"we want a free-standing type checker and pretty printer that can offer a second opinion on whether something type checks, can tell us exactly what was checked, and which is completely independent of the Lean ecosystem\". External type checkers do come with a pretty printer and the ability to print and spot-check the \"big parts\" of a Lean project (which will not type check if they rely on things like unsafe definitions, so there is protection here). Before the AI boom this was the primary concern, and as far as the \"absolute correctness\" semantic debate is concerned, this is the concern implicated in my opinion.</p>\n<p>The second concern is \"we want something which is robust against potentially malicious inputs which we may not have even seen without having to manually compare the pretty printer output for every declaration\". This is totally different, and is much more reliant on an ability to parse Lean (or do other things that involve the elaborator) to enable robust comparison of declarations, which is why (as far as I'm aware) the existing works that do this rely on Lean itself and are therefore unable to address concern number one. </p>\n<p>I don't think there are any magic bullets that will simultaneously address both concerns <em>while also</em> remaining a \"push-button\" tool. None of the existing tools on either side of the fence address the \"theorem statements changing by other stuff in the code\" part either unless you're willing to read the pretty-printer output from an external checker.</p>",
        "id": 565937753,
        "sender_full_name": "Chris Bailey",
        "timestamp": 1767220722
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"228466\">@Chris Bailey</span> I think I was a bit too mean to your document.  Your document, is very helpful, and when combined with <a href=\"https://leanprover-community.github.io/did_you_prove_it.html\">https://leanprover-community.github.io/did_you_prove_it.html</a> gives a fairly complete side of the story.  My main complaint is that when various issues arise with checking Lean proofs, hacks and AI exploits, the first answer is usually \"they should have used lean4checker\".  But in half those cases, <code>lean4checker</code> won't catch them.</p>",
        "id": 565982134,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278179
    },
    {
        "content": "<blockquote>\n<p>can tell us exactly what was checked</p>\n</blockquote>\n<p>I could be wrong, but looking at the terms themselves seems like a no-go.  They are much too large and complicated in Lean.  Now, of course, external checkers could print other information like declarations being checked and their axioms (but I don't think any do at the moment, right)?  As far as pretty printing, I think that would get complicated really fast.</p>",
        "id": 565982152,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278186
    },
    {
        "content": "<blockquote>\n<p>External type checkers do come with a pretty printer</p>\n</blockquote>\n<p>I was unaware of this.  Honestly, I thought this would be a unrealistic since pretty printing is context dependent.  Of course you could print the raw terms but type class resolution blows up terms making them unreadable.  For example, twice recently I came across the issue of norm <code>||x||</code>.  In the pretty printed version, it is hard to know what norm this is (sup norm, L2/Euclidean norm, operator norm?), and in the pp.all version the answer is so buried in a super long expression that you also can't read it.  (See <a href=\"https://proofassistants.stackexchange.com/questions/5396/selecting-the-right-norm\">here</a>.)  Of course, if all you want to do is check that terms haven't changes, then it is just term matching (and pretty printing is not important).</p>",
        "id": 565982160,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278193
    },
    {
        "content": "<blockquote>\n<p>completely independent of the Lean ecosystem</p>\n</blockquote>\n<p>Since you brought this up, and I agree it is related to the debate about absolute correctness, I'm not really sure current external checkers are that independent of the Lean ecosystem.  <strong>An important case study would be if the current Lean 4 checkers avoided all the various kernel bugs that Mario and others have found over the years.</strong>  But regardless, I think they also make the same decision to use bigint packages for reducing certain nat terms, right, which I know worry some people?   It would be great to see an external checker based on metamath/mm0 and one based on coq.  The former has some challenges with reduction (see <a href=\"https://proofassistants.stackexchange.com/questions/849/how-does-metamath-zero-handle-cic-as-in-lean-or-coq\">here</a>).  The latter I thought was once possible (<a href=\"https://github.com/SkySkimmer/rocq/tree/lean-import\">https://github.com/SkySkimmer/rocq/tree/lean-import</a>, <a href=\"https://discourse.rocq-prover.org/t/alpha-announcement-coq-is-a-lean-typechecker/581\">https://discourse.rocq-prover.org/t/alpha-announcement-coq-is-a-lean-typechecker/581</a>), but maybe I am mistaken.</p>",
        "id": 565982177,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278208
    },
    {
        "content": "<blockquote>\n<p>we want something which is robust against potentially malicious inputs which we may not have even seen without having to manually compare the pretty printer output for every declaration</p>\n</blockquote>\n<p>Kind of.  What I think we want is a way to safeguard against exploits.  And yes, manually comparing pretty printed output is not enough (as I have an example above where the pretty printed output does not change).</p>",
        "id": 565982186,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278214
    },
    {
        "content": "<blockquote>\n<p>is much more reliant on an ability to parse Lean</p>\n</blockquote>\n<p>I don't see why.  I think a lot can happen at the term level, the same level that external checkers work with.</p>",
        "id": 565982193,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278219
    },
    {
        "content": "<blockquote>\n<p>don't think there are any magic bullets that will simultaneously address both concerns <em>while also</em> remaining a \"push-button\" tool.</p>\n</blockquote>\n<p>I think <a href=\"https://github.com/GasStationManager/SafeVerify\">SafeVerify</a> and <a href=\"https://github.com/leanprover/comparator\">Comparator</a> come very close (and I think Comparator will soon enable use of external typecheckers as well).  <strong>I'd really love to get your thoughts on these two tools.</strong>  But maybe I'm misunderstanding your concerns.</p>\n<p>The only thing that I think existing push button tools can't do is check that the original targets are semantically correct.  I think the three best ways to do this are (1) checking the code manually (at the code and pretty printer level), and (2) doing what the Liquid Tensor Project did, which is to prove other characterizing facts about those objects.  This helps ensure one is working with the correct mathematical objects.  (But even then, I understand a type about an edge case or a missing hypothesis could totally change the intended meaning of a statement.)  (3) Use the proven theorem to prove consequences of it.  That would further help ensure it is the theorem you want.</p>",
        "id": 565982250,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767278282
    },
    {
        "content": "<p>There's another, more restricted issue here that I'm not sure anyone has mentioned yet (though this is a long thread, so please forgive me if I missed it): expectations surrounding <code>#print axioms foo</code>. Generally, I believe people (reasonably) expect <code>#print axioms foo</code> to mean \"<code>foo</code> can be proven from the logged axioms\". But <code>#print axioms</code> does not typecheck all declarations it uses, and therefore the tacit part of the expectation that <code>foo</code> is in fact <em>proven</em> from these axioms is not necessarily met.</p>\n<p>So, I think it'd be reasonable to have <code>#print axioms</code> typecheck as well, as I believe \"providing some degree of peace of mind that you have a proof, up to the other exploits mentioned here\" is its actual responsibility for users. (I claim it's fine if it's not terribly performant, and won't be too bad if we keep track of which declarations have been typechecked and which haven't.)</p>\n<p>If we <em>do</em> simply want <code>#print axioms</code> to mechanically list axioms (though I'm not sure why this would be a useful responsibility to be fulfilled by a user-facing command!), I think we ought to at least have <em>something</em> which fulfills the typechecking responsibility, and have it replace <code>#print axioms</code> in the lexicon as the first stage in the \"see? I proved it!\" pipeline. :)</p>",
        "id": 566100148,
        "sender_full_name": "Thomas Murrills",
        "timestamp": 1767387254
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565982152\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>can tell us exactly what was checked</p>\n</blockquote>\n<p>Now, of course, external checkers could print other information like declarations being checked and their axioms (but I don't think any do at the moment, right)?  As far as pretty printing, I think that would get complicated really fast.</p>\n</blockquote>\n<p>The rust checker requires users to white list axioms by name up front in the execution config (so does Comparator) and you can ask for checked declarations to be printed/read back. There's a pretty printer options setting in the config format for controlling things like implicits, typeclasses, universes, etc. The difference in the external pretty printers is that there is no support for complex notation or macros (only simple prefix, infix, and suffix notation).</p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565982177\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>completely independent of the Lean ecosystem</p>\n</blockquote>\n<p>Since you brought this up, and I agree it is related to the debate about absolute correctness, I'm not really sure current external checkers are that independent of the Lean ecosystem.  <strong>An important case study would be if the current Lean 4 checkers avoided all the various kernel bugs that Mario and others have found over the years.</strong>  But regardless, I think they also make the same decision to use bigint packages for reducing certain nat terms, right, which I know worry some people?   It would be great to see an external checker based on metamath/mm0 and one based on coq.  The former has some challenges with reduction (see <a href=\"https://proofassistants.stackexchange.com/questions/849/how-does-metamath-zero-handle-cic-as-in-lean-or-coq\">here</a>).  The latter I thought was once possible (<a href=\"https://github.com/SkySkimmer/rocq/tree/lean-import\">https://github.com/SkySkimmer/rocq/tree/lean-import</a>, <a href=\"https://discourse.rocq-prover.org/t/alpha-announcement-coq-is-a-lean-typechecker/581\">https://discourse.rocq-prover.org/t/alpha-announcement-coq-is-a-lean-typechecker/581</a>), but maybe I am mistaken.</p>\n</blockquote>\n<ol>\n<li>None of the kernel issues Mario reported (that I'm aware of) were replicated in the rust checker. I would argue that by definition it's \"completely independent of the Lean ecosystem\" in that building and running the checker does not rely on Lean or any software downstream of Lean.</li>\n<li>The bignum extension can be turned off (there's a flag in the config file <code>\"nat_extension\": bool</code>) but in practice it's no longer optional for anything downstream of core. This is the kind of feature that gets added, people start using it, and now you have terms in core that will just run indefinitely or OOM if you try to check them without the extension enabled. Not to say anything about whether this was a good or bad addition, that's just the current state of affairs.</li>\n<li>The mm0 thing is a question for Mario, but I presume it's (at least) blocked on developer time; adapting the kernel changes that have taken place in the last few years would take a lot of effort (and he would probably just use lean4lean as the type checker/reduction machine now).</li>\n</ol>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565982186\">said</a>:</p>\n<blockquote>\n<p>And yes, manually comparing pretty printed output is not enough (as I have an example above where the pretty printed output does not change).</p>\n</blockquote>\n<p>Can you post this example? This should not be possible with the pretty printer included in an external checker. </p>\n<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565982250\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>don't think there are any magic bullets that will simultaneously address both concerns <em>while also</em> remaining a \"push-button\" tool.</p>\n</blockquote>\n<p>I think <a href=\"https://github.com/GasStationManager/SafeVerify\">SafeVerify</a> and <a href=\"https://github.com/leanprover/comparator\">Comparator</a> come very close (and I think Comparator will soon enable use of external typecheckers as well).  <strong>I'd really love to get your thoughts on these two tools.</strong>  But maybe I'm misunderstanding your concerns.</p>\n<p>The only thing that I think existing push button tools can't do is check that the original targets are semantically correct.  I think the three best ways to do this are (1) checking the code manually (at the code and pretty printer level), and (2) doing what the Liquid Tensor Project did, which is to prove other characterizing facts about those objects.  This helps ensure one is working with the correct mathematical objects.  (But even then, I understand a type about an edge case or a missing hypothesis could totally change the intended meaning of a statement.)  (3) Use the proven theorem to prove consequences of it.  That would further help ensure it is the theorem you want.</p>\n</blockquote>\n<p>From what I can tell, Comparator does exactly what you would want <em>to address concern 2</em>, and is what people should be using for most of the tasks I see discussed these days. The rust checker is not currently able to do challenge/solution because I did not write it in a way that supports comparison of declarations which come from different environments. It's possible to add this, but I haven't had much time to think about what's necessary for the comparison to be sound. In my opinion, adequately addressing concern 1 requires using something that is not itself written in or in some way dependent on Lean (the extent to which lean4lean represents an exception to this is above my pay grade).</p>",
        "id": 566114054,
        "sender_full_name": "Chris Bailey",
        "timestamp": 1767401814
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"228466\">Chris Bailey</span> <a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/566114054\">said</a>:</p>\n<blockquote>\n<p>Can you post this example? This should not be possible with the pretty printer included in an external checker.</p>\n</blockquote>\n<p><a href=\"#narrow/channel/270676-lean4/topic/new.20bug.20mentioned.20on.20Twitter/near/565770882\">I think Jason is referring to this example he already posted.</a></p>",
        "id": 566133791,
        "sender_full_name": "James E Hanson",
        "timestamp": 1767428214
    },
    {
        "content": "<p>Right, it's the <code>nothingToSeeHere</code> example</p>",
        "id": 566137314,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1767432744
    },
    {
        "content": "<p>It should be possible to extend comparator to use the rust checker for ‚Äúare these declarations correct‚Äù, that's orthogonal to the comparison with the challenge file, right?<br>\n(Or are you saying that ideally that functionality would also be implemented multiple times and independently?)</p>",
        "id": 566139334,
        "sender_full_name": "Joachim Breitner",
        "timestamp": 1767435001
    },
    {
        "content": "<p>As for the example, as others just said, it is that <code>nothingToSeeHere</code> example.  <span class=\"user-mention\" data-user-id=\"228466\">@Chris Bailey</span>  I don't know what you mean by \"the pretty printer included in an external checker\", but if you just mean something similar to the verbose output in Lean of printing the statement with <code>pp.all</code> turned on, then yes, you could use that to compare theorem statements.  (Of course, at that point you could also just compare the actual terms which is what I believe Comparator does.  No need for a human to get involved, especially since fully elaborated terms can get insanely long.)  But if you hide type class resolution, then you won't be able to compare the two versions I think, right?</p>",
        "id": 566143058,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767440082
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"470149\">@Joachim Breitner</span> I don't know if you are asking me or Chris, but yes, Comparator should be able to extendable to use external checkers for checking if the exported proofs are valid.  Comparing the terms of the Challenge and Solution file does not seem to be difficult or controversial computation in the same way that running the Lean kernel is.  Which means, (1) I would trust Comparator to do it correctly, (2) it wouldn't be hard to make an external tool which does the same thing, and (3) it wouldn't be hard to make a verified tool which also does it.)  (Note, this might not be so trivial if we are looking for something more than an exact match of terms, as has come up a few times.)</p>",
        "id": 566143463,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767440624
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"228466\">@Chris Bailey</span> said:</p>\n<blockquote>\n<p>The rust checker requires users to white list axioms by name up front in the execution config (so does Comparator) and you can ask for checked declarations to be printed/read back.</p>\n</blockquote>\n<p>Ok, that is nicer than I realized.  That would help a lot for axiom issues and for cases when the environment drops expressions.  (I'll keep this in mind in discussions like this, and possible applications.)</p>",
        "id": 566143678,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767440898
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"228466\">@Chris Bailey</span> said:</p>\n<blockquote>\n<p>None of the kernel issues Mario reported (that I'm aware of) were replicated in the rust checker.</p>\n</blockquote>\n<p>If this is true, I think this should be made more well-known (and systematically checked with tests for each of Lean's previous bugs).  I know Lean has \"officially\" at points emphasized that its approach to soundness includes external checkers.  (I honestly don't know if any lean users actually use external checkers, but I think Lean was slightly implying that if a user really wanted to be sure there were no soundness issues, they should use one.)  Now, that Lean can no longer say (as they used to) that there are not soundness bugs ever found, they might still be able to say that there are no soundness bugs ever found that would not have been caught with an external type checker.  If that is the case, that would be very helpful to know.</p>",
        "id": 566144635,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767441679
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"228466\">@Chris Bailey</span> said:</p>\n<blockquote>\n<p>The bignum extension can be turned off (there's a flag in the config file <code>\"nat_extension\": bool</code>) but in practice it's no longer optional for anything downstream of core.</p>\n</blockquote>\n<p>That is also good to know.  I also agree we are sort of stuck with the feature now that people are writing proofs using it.  (A side comment, which is probably best for its own thread, is that I think that there are probably good post hoc ways to get rid of specific bignum reductions in a proof.  One just can track all such reductions, prove them with something like <code>norm_num</code>, and replace the term proof with that.  It won't be completely trivial, but it could be useful if there is ever a serious worry about such computations.)</p>",
        "id": 566145185,
        "sender_full_name": "Jason Rute",
        "timestamp": 1767442176
    }
]