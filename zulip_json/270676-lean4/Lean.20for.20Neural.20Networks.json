[
    {
        "content": "<p>I have been working a lot with neural networks including using Pytorch, Tensorflow and Onnxruntime.</p>\n<p>It strikes me that those languages essentially have their own implementation of dependent types. (Albeit a very basic one.) It also occurs to me that a language such as Lean would be an excellent language to express neural network models.</p>\n<p>As an example, a neural network can basically be thought of as a function that takes one or more inputs of certain types (a number type plus a shape) and returns one or more outputs of certain types. What is more, that type of the output is dependent on the type of the input.</p>\n<p>A typical example might have inputs of type  <code>float16[batch,channels,width,height]</code>, <code>int64[batch, ids]</code> and return some output e.g. <code>float16[batch,f(width,height,ids)]</code> where f is some function. Thus they are types with various parameters (shapes) which are named (incorrectly) as tensors types.</p>\n<p>I have been playing around implementing my own type theory, and found it works quite well with importing multi-dimensional arrays, in this way. (I don't actually define what a float16 actually is, I just leave as an undefined object of type Type). One could indeed define a type such as <code>int(n)</code> where is it like a Fin but the values are between ±2^n and it also includes ±∞. It is less clear to me how one would mathematically define a float32 or float16 or whether they need to be defined at all.</p>\n<p>As another example, if you consider the convolution operator for a neural network, the output type (shape) depends on the types (shapes) of the input tensor and the kernel size. </p>\n<p>BTW the multi-dimensional arrays here are a lot different from tensors you find in math and physics which tend to have the same dimension on each index, rather these are rectangular arrays. Multidimensional arrays have lots of other uses, e.g. graphics, simulation, and lots of other things.</p>\n<p>Well, all this say that this might be an interesting avenue to explore, and that multidimensional arrays (especially if GPU accelerated) would be an interesting use for the Lean language.</p>\n<p>Thanks for listening<span aria-label=\"grinning face with smiling eyes\" class=\"emoji emoji-1f601\" role=\"img\" title=\"grinning face with smiling eyes\">:grinning_face_with_smiling_eyes:</span></p>\n<p>Addendum: For back propagation, this is essentially just partial differentiation with the chain rule, which could also be automated in Lean.</p>",
        "id": 455267176,
        "sender_full_name": "Mr Proof",
        "timestamp": 1722395396
    },
    {
        "content": "<p>Yes, multidimensional arrays/tensors in neural networks are a great example of where dependent types would be helpful.   See <span class=\"user-mention\" data-user-id=\"346070\">@Tomas Skrivan</span>’s <a href=\"https://github.com/lecopivo/SciLean\">SciLean</a> for a project which I think aims in this direction.</p>",
        "id": 455286995,
        "sender_full_name": "Jason Rute",
        "timestamp": 1722403406
    },
    {
        "content": "<p>I think <a href=\"https://jax.readthedocs.io/en/latest/\">Jax</a> is interesting also since it also aims to be fully functional, so one day I could imagine something like a Jax interface in Lean, but maybe that is asking too much. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 455287121,
        "sender_full_name": "Jason Rute",
        "timestamp": 1722403481
    },
    {
        "content": "<p>And finally, you might also be interested in <a href=\"https://arxiv.org/abs/2402.15332\">categorical deep learning</a> which aims to combine category theory (and type theory) with deep learning theory (based on the success of geometric deep learning).</p>",
        "id": 455287344,
        "sender_full_name": "Jason Rute",
        "timestamp": 1722403634
    },
    {
        "content": "<p>You may like this paper, which shared a similar vision:</p>\n<p><a href=\"http://proceedings.mlr.press/v70/selsam17a/selsam17a.pdf\">http://proceedings.mlr.press/v70/selsam17a/selsam17a.pdf</a></p>",
        "id": 455306286,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1722411178
    },
    {
        "content": "<p>Here I describe my attempt at neural networks in Lean <a href=\"https://lecopivo.github.io/scientific-computing-lean/working-with-arrays/tensor-operations.html\">https://lecopivo.github.io/scientific-computing-lean/working-with-arrays/tensor-operations.html</a> unfortunately I have not managed to train it yet.</p>",
        "id": 455322835,
        "sender_full_name": "Tomas Skrivan",
        "timestamp": 1722415963
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"337670\">@Alok Singh</span></p>",
        "id": 462260293,
        "sender_full_name": "Quinn",
        "timestamp": 1723610601
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"693606\">Quinn</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.20for.20Neural.20Networks/near/462260293\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"337670\">Alok Singh</span></p>\n</blockquote>\n<p>i’ve been streaming this at <a href=\"mailto:youtube.com/@therevaloksingh\">youtube.com/@therevaloksingh</a>, like <a href=\"https://www.youtube.com/live/_efcMao8_50?si=Y9rYHbgfDB68yvbd\">https://www.youtube.com/live/_efcMao8_50?si=Y9rYHbgfDB68yvbd</a></p>\n<p>code at <a href=\"https://github.com/pimpale/llm.lean\">https://github.com/pimpale/llm.lean</a></p>",
        "id": 462387097,
        "sender_full_name": "Alok Singh",
        "timestamp": 1723655442
    }
]