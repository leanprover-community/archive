[
    {
        "content": "<p>Iâ€™m curious whether anyone has explored dispatching Lean 4 code to GPU kernels, in particular for straight line code operating on simple types used inside <a href=\"http://Array.map\">Array.map</a>.  It seems like one could write a quite general plugin to do this given the flexibility of the system.</p>",
        "id": 393123362,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1695681324
    },
    {
        "content": "<p>Note that both the Metal and CUDA shading languages are C-based, so conceivably one could try to restrict to the mutually compatible fragment and re-use the existing code generator.</p>",
        "id": 393123655,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1695681475
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"230999\">@Daniel Selsam</span>: Do you know whether people have tried this?</p>",
        "id": 393125359,
        "sender_full_name": "Geoffrey Irving",
        "timestamp": 1695682357
    },
    {
        "content": "<p>A long time ago I attempted something like that, but I knew very little metaprogramming and got nowhere. I would be quite happy to try again. I'm thinking of writing bindings for CUDA Thrust library and some meta code to generate transformations and reduction kernels based on Lean code.</p>",
        "id": 393125653,
        "sender_full_name": "Tomas Skrivan",
        "timestamp": 1695682512
    },
    {
        "content": "<p>I'm not aware of anyone giving it a proper try.</p>",
        "id": 393125776,
        "sender_full_name": "Tomas Skrivan",
        "timestamp": 1695682587
    },
    {
        "content": "<p>Soon I should have backpropagation working in Lean. Generating reasonable GPU code would be really useful to properly test it out.</p>",
        "id": 393126388,
        "sender_full_name": "Tomas Skrivan",
        "timestamp": 1695682912
    },
    {
        "content": "<p>I do know another project focusing on using dependently typed arrays to program GPU/CPU parallel kernels. <a href=\"https://github.com/google-research/dex-lang\">https://github.com/google-research/dex-lang</a></p>",
        "id": 393130007,
        "sender_full_name": "Schrodinger ZHU Yifan",
        "timestamp": 1695684938
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"514145\">Geoffrey Irving</span> <a href=\"#narrow/stream/270676-lean4/topic/GPU.20kernel.20generation.20and.20use.3F/near/393125359\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"230999\">Daniel Selsam</span>: Do you know whether people have tried this?</p>\n</blockquote>\n<p>Hi <span class=\"user-mention\" data-user-id=\"514145\">@Geoffrey Irving</span>! I am afraid I have no idea.</p>",
        "id": 393144367,
        "sender_full_name": "Daniel Selsam",
        "timestamp": 1695693423
    }
]