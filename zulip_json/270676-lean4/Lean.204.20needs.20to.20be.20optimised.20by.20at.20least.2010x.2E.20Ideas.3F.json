[
    {
        "content": "<p>Coming from a software developer background rather than an academic math background. I am viewing Lean as a piece of software. In that respect my observations are that it needs a great deal of optimisation. Open source projects have the habit of building up inefficiencies over time. </p>\n<p>For example, the built version of Mathlib, is several GB and takes several GB of RAM to load in and several seconds to load. That is not the fault of mathlib, but rather it must be the software and data-structures it is built upon. They were probably fine to begin with but with huge libraries I think they need updating.</p>\n<p>When Mathlib and other libraries get bigger this is only going to get worse.</p>\n<p>Can we throw some ideas around to see if we can think of ways to make it more memory efficient?</p>\n<p>Some ideas I have are (perhaps some are already implemented)</p>\n<ul>\n<li>JIT compilation of libraries. Libraries are compiled on the fly only when they are needed. (e.g. when you import Mathlib, it could just load in a list of every function. Then when you call a particular function, it could compile, or download, the particular library it needs , Algebra, say, and cache it for later, then remove ones that haven't been used for a while).</li>\n<li>Olean files only moved to RAM when needed. (e.g. when you import Mathlib - don't instantly move it all to RAM0</li>\n<li>Improve the internal data-structures to be more efficient</li>\n<li>Improve repetition in files by referencing external indexes</li>\n<li>Improve cases where tactics like \"simp\" are expanded inefficiently. </li>\n<li>Move some of Lean processing to the GPU(?) probably difficult as it's not suited for parallelism.</li>\n<li>Olean files stored in a compressed format and decompressed when needed. (I get 3x memory saving just by storing the olean files as ZIP on disk)</li>\n</ul>\n<p>Any other ideas? Are there any \"low hanging fruit\" that could cut the footprint in half?</p>",
        "id": 446209269,
        "sender_full_name": "Mr Proof",
        "timestamp": 1719022493
    },
    {
        "content": "<p>Currently you have to <code>require</code> the whole Mathlib, even if you only need Mathlib.Tactic. It would be good to have the ability to select a subdirectory.</p>\n<p>Sorry if this isn't the right place to talk about it.</p>",
        "id": 446214789,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1719027278
    },
    {
        "content": "<p>The original question seems to presuppose that no one is thinking about and working on all these things. Perhaps read the FRO roadmap?</p>",
        "id": 446215645,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1719028138
    },
    {
        "content": "<p>Regarding <code>require</code> and Mathlib: the subdirectories do not at all reflect the import structure, and I expect there are very few subsets of the directories (besides subsets close to all of Mathlib) which would work without the complement.</p>",
        "id": 446215759,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1719028240
    },
    {
        "content": "<p>If you want to work on that, there is massive amounts of work to be done simplifying and streamlining the import structure of Mathlib (perhaps one day enabling partial <code>require</code>) and assistance with this is appreciated.</p>",
        "id": 446215817,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1719028303
    },
    {
        "content": "<p>It's also worth being aware that making changes to fundamental aspects of Lean is a challenging process, and you need substantial experience with the ecosystem to be able to make a positive contribution. Work on technical debt in Mathlib, and contributing high quality documentation, are probably better things for new contributors to be thinking about.</p>",
        "id": 446215995,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1719028477
    },
    {
        "content": "<p>Yes, indeed the FRO says <strong>Scalability Enhancements</strong> but I think it's good to have a discussion where people can all throw in their ideas. Perhaps the people working on this will find out something they didn't think about. <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span> The more ideas the better IMO.<br>\n(I did  a search and didn't find any similar discussions)</p>\n<p>BTW, is Lean all made by volunteers or is there an in-house team at Microsoft(?) who work on the <del>kernel</del> compiler?</p>",
        "id": 446216192,
        "sender_full_name": "Mr Proof",
        "timestamp": 1719028616
    },
    {
        "content": "<p>Latest Lean code include license header of Amazon.</p>",
        "id": 446222622,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1719033327
    },
    {
        "content": "<p>Lean FRO must have been funded by some foundation.</p>",
        "id": 446222690,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1719033368
    },
    {
        "content": "<p>By the way, I am also interested in Lean's support for GPUs and parallel processing - GPUs may not be useful for proofs, but they are useful for scientific computing.</p>",
        "id": 446222739,
        "sender_full_name": "Asei Inoue",
        "timestamp": 1719033428
    },
    {
        "content": "<p>About point 2, oleans aren't ever moved to RAM, they're just <a href=\"https://en.wikipedia.org/wiki/Memory-mapped_file\">mmapped</a>. And mathlib already uses olean compression on the wire, but they get decompressed on disk because otherwise you can't mmap them.</p>",
        "id": 446240836,
        "sender_full_name": "Mauricio Collares",
        "timestamp": 1719042487
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"626349\">Asei Inoue</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/446222622\">said</a>:</p>\n<blockquote>\n<p>Latest Lean code include license header of Amazon.</p>\n</blockquote>\n<p>That's because Leo switched from MSR to Amazon, the rest of the FRO is not Amazon employees.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"725689\">Mr Proof</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/446216192\">said</a>:</p>\n<blockquote>\n<p>BTW, is Lean all made by volunteers or is there an in-house team at Microsoft(?) who work on the kernel?</p>\n</blockquote>\n<p>To my knowledge there is nobody working at Microsoft that does large contributions to the Lean compiler itself anymore. There is also basically nobody working actively on the kernel, the kernel is finished, it has been barely touched for a long time. As to who is working on the Lean compiler, that's the FRO.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"626349\">Asei Inoue</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/446222690\">said</a>:</p>\n<blockquote>\n<p>Lean FRO must have been funded by some foundation.</p>\n</blockquote>\n<p>You can read on our website which foundations etc are funding the FRO.</p>",
        "id": 446245425,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1719044648
    },
    {
        "content": "<p>To continue with Kim's thought, while valiant efforts are sporadically made to simplify the dependency structure of Mathlib, as I understand it they tend to be ad hoc and to work chiefly at file level. It could be a very nice computer-sciencey project to extract the theorem-level dependency graph and attack it with the latest in network algorithms.</p>",
        "id": 446246110,
        "sender_full_name": "A.",
        "timestamp": 1719045268
    },
    {
        "content": "<p>File level dependencies are how lean works right now, and I'm not aware of plans to change that. It would be interesting to have some kind of automated analysis that could flag when heavy dependencies are only used by a small fraction of the declarations in a file, for example</p>",
        "id": 446246837,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1719045861
    },
    {
        "content": "<p>Extracting the theorem level dependency graph is definitely possible with a meta program that just looks at all theorem statements + proofs and figures out dependencies based on that. One thing to keep in mind here however is that fully elaborated Lean terms do not contain references to the syntax declarations (such as math notation or mroe importantly tactics) that they used anymore. This means that while a tactic can produce a proof term that looks very simple it might have required some part of a library in order to do so and you are not seeing that library anymore now.</p>",
        "id": 446246969,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1719045966
    },
    {
        "content": "<p>Is that something lean would be interested in tracking in the future, you think, Henrik?</p>",
        "id": 446247253,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1719046254
    },
    {
        "content": "<p>In fact, the issue of \"oleans for syntax\" has appeared in several discussions.</p>",
        "id": 446247350,
        "sender_full_name": "Damiano Testa",
        "timestamp": 1719046347
    },
    {
        "content": "<p>I don't know about that. I do know that there is some demand for this in other features like some Mathlib linters but I'm aware of no movement to change this currently.</p>",
        "id": 446247364,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1719046365
    },
    {
        "content": "<p>We already have data on the 'hidden edges' caused by notations and tactics though, that's <code>noshake.json</code></p>",
        "id": 446303414,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1719079597
    },
    {
        "content": "<p>Isn't notation information visible in the infotree?</p>",
        "id": 446304061,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1719079959
    },
    {
        "content": "<p>I would expect it to only be things like positivity extensions (which have no syntax at the caller) referring to lemmas in an earlier file that are neither in the infotree nor the proof term</p>",
        "id": 446304138,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1719080004
    },
    {
        "content": "<p>(And I think the right thing to do there is record the used extension names in the proof in some way)</p>",
        "id": 446304195,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1719080037
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/446304061\">said</a>:</p>\n<blockquote>\n<p>Isn't notation information visible in the infotree?</p>\n</blockquote>\n<p>Yes, but the infotree is not stored</p>",
        "id": 446304695,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1719080287
    },
    {
        "content": "<p>also, even in a live elaboration session lean isn't very good at recording what parts of the state are actually used. The most accurate method is the obvious one: comment out imports until something breaks</p>",
        "id": 446304744,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1719080323
    },
    {
        "content": "<p>I think a useful tool would be a variant of <code>shake</code> that reported if an import was not used in the first <code>N</code> lines (e.g. N=500) of a file. These would be obvious targets for splitting files, and you could crank down <code>N</code> (or use a fraction of the file length).</p>",
        "id": 446365356,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1719115852
    },
    {
        "content": "<p>I was thinking of having a go at making a little C command line tool, that basically just read all the Lean files, and searched for the words \"import\" to make a list of the libraries used in a file. That could be run pretty quickly I imagine to make some sort of tree dependency graph. Which you could feed into something like Mathematica to give a graphical view.<br>\nAlso, it could search for the words like \"def\" , \"lemma\", \"theorem\" etc. to make a list of every definition in a file. This could all be done without parsing anything. <br>\nWhile not being as accurate as writing this in Lean code I imagine it would run a lot faster(?) <br>\nOr would it be \"necessary to break out Emacs and modify that Perl script\"?</p>",
        "id": 446368666,
        "sender_full_name": "Mr Proof",
        "timestamp": 1719118317
    },
    {
        "content": "<p>We already have <code>import-graph</code> written in Lean that can export to <code>dot</code>, hence generate <code>.pdf</code>.</p>",
        "id": 446377489,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1719123179
    },
    {
        "content": "<p>It runs fast enough (large invocations with <code>pdf</code> output are slow because of <code>dot</code> choosing locations for nodes, not because of Lean is slow in generating the graph) and is 100% accurate.</p>",
        "id": 446377688,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1719123260
    },
    {
        "content": "<p>Hello! I'm new here, but I was just looking for a place to ask whether or not LEAN was supposed to be taking upwards of 2.5 GB of RAM, and it seems like it is!</p>\n<p>This may be a silly solution, but based on what I understand it seems reasonable.</p>\n<p>Once a formal proof has been found and verified, on our machines we could replace the actual proof itself with sorry or change all proven theorems into axioms. These could be automated changes that triggers on certain GitHub actions. The idea is that a full repository of formalized proofs is still kept, but individuals on small computers like mine could still use what's already been formalized to formalize new things, without having the system be bogged down trying to \"reverify\" everything that's already been proven  tens of thousands of times on our own computers.</p>\n<p>Again, not sure if this is actually how it works, and it definitely feels like a lazy solution, but at the same time, \"I don't see why not.\"</p>\n<p>Speaking of, I know there's some sort of save tactic that supposedly prevents lean from rerunning the file every time you edit a line. Not sure if it's relevant.</p>",
        "id": 457232514,
        "sender_full_name": "Rowan Williams",
        "timestamp": 1723077711
    },
    {
        "content": "<p>I’m wondering if Mathlib is bigger than my rust dependencies for my 200k loc personal project. I believe that the source code is not optimsed for incremental computation. One thing at least is that in the roadmap it’s mentioned that Lean compiler needs module signature. Modern languages like rust golang have developed a bunch of techniques in dealing with things like this. I’m optimistic things will dramatically improve in the following years</p>",
        "id": 457234819,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1723078923
    },
    {
        "content": "<p>It seems in the compiler, most things are written in lean itself. Although lean tries to be a fast language, it's still not as convenient to optimise as a true system language like golang, rust, c++. Personally I would stick to a fixed system level language for the compiler itself, without doing too much bootstrapping. But that's just a personal opinion. I could imagine that for safety reasons the compiler is written in a bootstrapping way. Anyway that looks like too much a change to dicuss.</p>\n<p>Currently the incremental mechanism is complicated to get it designed right, because Lean just seems too powerful. My reading of the repo tells me that all the features interact in a convoluted way such that it's hard to get things incremental at a very granular level.</p>",
        "id": 457235494,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1723079484
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"677176\">Rowan Williams</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/457232514\">said</a>:</p>\n<blockquote>\n<p>Once a formal proof has been found and verified, on our machines we could replace the actual proof itself with sorry</p>\n</blockquote>\n<p>See also <a href=\"#narrow/stream/287929-mathlib4/topic/Running.20Mathlib.20under.20.60set_option.20debug.2EbyAsSorry.60\">https://leanprover.zulipchat.com/#narrow/stream/287929-mathlib4/topic/Running.20Mathlib.20under.20.60set_option.20debug.2EbyAsSorry.60</a></p>",
        "id": 457235684,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1723079583
    },
    {
        "content": "<p>Lean has had incremental compilation of tactics in interactive mode for the last few months. If you're running on a recent version you should see that when you edit a proof the yellow progress bar only starts at or near your edit.</p>",
        "id": 457235732,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1723079617
    },
    {
        "content": "<p>We're also working (longer term) on separating oleans into two components, one with as many proofs as possible stripped out.</p>",
        "id": 457235917,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1723079679
    },
    {
        "content": "<p>Intra-file parallelism is also on the near term horizon!</p>",
        "id": 457235960,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1723079710
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"677176\">Rowan Williams</span> <a href=\"#narrow/stream/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/457232514\">said</a>:</p>\n<blockquote>\n<p>whether or not LEAN was supposed to be taking upwards of 2.5 GB of RAM</p>\n</blockquote>\n<p>It depends on what you're doing with it and how you measure that. On Linux, importing all of Mathlib takes about 36MB of RAM (RssAnon in <code>/proc/$PID/status</code>) and another 38MB of data read lazily from .oleans (RssFile), though that part can be shared across processes.</p>",
        "id": 457317627,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1723109783
    },
    {
        "content": "<p>The most expensive file in Mathlib at its peak uses 3.7GB, which likely means it's just generating a <em>lot</em> of (temporary) terms while running. It's been some time, but we looked at such an outlier file before and with a few adjustments, memory use can usually be lowered drastically.</p>",
        "id": 457318334,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1723110032
    },
    {
        "content": "<p>Any updates on this? It sounds like it is more efficient these days? It certainly seems to load Mathlib faster on the web version. <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span>Nice work.</p>",
        "id": 530123875,
        "sender_full_name": "Mr Proof",
        "timestamp": 1753183594
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"677176\">Rowan Williams</span> <a href=\"#narrow/channel/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/457232514\">said</a>:</p>\n<blockquote>\n<p>Once a formal proof has been found and verified, on our machines we could replace the actual proof itself with sorry or change all proven theorems into axioms. These could be automated changes that triggers on certain GitHub actions. The idea is that a full repository of formalized proofs is still kept, but individuals on small computers like mine could still use what's already been formalized to formalize new things, without having the system be bogged down trying to \"reverify\" everything that's already been proven  tens of thousands of times on our own computers.</p>\n</blockquote>\n<p>Yep, keep the proofs on the cloud, exactly.</p>\n<p>This sounds reasonable to me! I had the same thought! (Not sure if it has been done. But it is probably possible to write a simple parser that just replaces all theorems with axioms. Probably Chat GPT could even write such a script). But it might not be necessary now with the incremental loading(???)</p>",
        "id": 530124409,
        "sender_full_name": "Mr Proof",
        "timestamp": 1753183787
    },
    {
        "content": "<p>That's a weird soundness breach for any proof assistant environment/UX because you completely offload your trust.</p>\n<p>But a similar idea is within the realm of possibilities that become doable with succinct cryptographic proofs of typechecking.</p>",
        "id": 530126347,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1753184390
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"451983\">Arthur Paulino</span> <a href=\"#narrow/channel/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/530126347\">said</a>:</p>\n<blockquote>\n<p>That's a weird soundness breach for any proof assistant environment/UX because you completely offload your trust.</p>\n<p>But a similar idea is within the realm of possibilities that become doable with succinct cryptographic proofs of typechecking.</p>\n</blockquote>\n<p>Well we're already offloading the trust that things like BigNum in lean are implemented correctly. Doing the proof offline just with the theorems as axioms and then doing a final check by uploading it to the cloud. Doesn't seem to me to be any less of trust issue. There's always the option of compiling the full theorems at the end of your project. </p>\n<p>Yeah, there could be some kind of \"block-chain\" type system to replace a proof with some sort of RSS type key. But I don't know much about that!</p>",
        "id": 530127491,
        "sender_full_name": "Mr Proof",
        "timestamp": 1753184765
    },
    {
        "content": "<p>How come? There's clearly a difference between trusting a smaller subset of the implementation and trusting a bigger subset of the implementation. Analogously, that's the whole point of trying to be extremely minimal with the usage of unsafe Rust.</p>",
        "id": 530128350,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1753185055
    },
    {
        "content": "<p>Well in fact I think the opposite. Whereas you can at the end of the month/year download the full theorems to test your proof. You can never actually test the implementation of big integers in Lean without inspecting the code. So while doing your proofs with just the axioms downloaded during the month (saving you lots of time and computing power), you never have to just trust it. It's just time-saving during day-to-day work.</p>\n<p>We're not saying throw away the theorems forever, we're just saying we don't need them for day-to-day work.</p>\n<p>(Or course if for some reason an erroneous axiom was put into the system then you've wasted a month of work. But I am very trusting that there could be systems to prevent that. Such as block-chain type systems etc.) This could all be mitigated if there was a way for the theorems to compile faster and use less memory which I think might have already been done. But a proof with a \"simp\" requires a lot of time, processing to evaluate.</p>\n<p>Alternatively: Download the mathlib, and the first time you run it it checks all the theorems. And from thenceforth never checks them again would be another solution.</p>",
        "id": 530132760,
        "sender_full_name": "Mr Proof",
        "timestamp": 1753186472
    },
    {
        "content": "<p>You might have a better chance with this idea by implementing this substitution logic locally (i.e. not pushing the <code>sorry</code>'d theorems anywhere), then hoping it's actually faster than simply downloading the cache.</p>",
        "id": 530134737,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1753187098
    },
    {
        "content": "<p>Now that we're having this discussion</p>",
        "id": 530181142,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201010
    },
    {
        "content": "<p>I think zk-SNARKs can help</p>",
        "id": 530181168,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201017
    },
    {
        "content": "<p>In the past I thought of mentioning zk-SNARKs in the context of offloading proof verification</p>",
        "id": 530181218,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201036
    },
    {
        "content": "<p>But the community wasn't diverse enough. Now that there's more diversity I feel it's safer to raise this idea</p>",
        "id": 530181290,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201058
    },
    {
        "content": "<p>There's some sort of zk-SNARK proof system for WASM programs. It is a made in China product.</p>",
        "id": 530181398,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201096
    },
    {
        "content": "<p>We could compile the kernel to WASM then use the zk-SNARK WASM system to generate a succinct cryptographic proof of proof validity.</p>",
        "id": 530181518,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201146
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"725689\">Mr Proof</span> <a href=\"#narrow/channel/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/530127491\">said</a>:</p>\n<blockquote>\n<p>Well we're already offloading the trust that things like BigNum in lean are implemented correctly.</p>\n</blockquote>\n<p>Maybe we should prove BigNum implementation correct.</p>",
        "id": 530181899,
        "sender_full_name": "suhr",
        "timestamp": 1753201292
    },
    {
        "content": "<p>No need. Everyone and their dog believes in GMP.</p>",
        "id": 530182044,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201349
    },
    {
        "content": "<p>If a piece of software has been shown to be extremely robust, there's little point in formally verifying it.</p>",
        "id": 530182419,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201482
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"394485\">suhr</span> <a href=\"#narrow/channel/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/530181899\">said</a>:</p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"n\">Maybe</span><span class=\"w\"> </span><span class=\"n\">we</span><span class=\"w\"> </span><span class=\"n\">should</span><span class=\"w\"> </span><span class=\"n\">prove</span><span class=\"w\"> </span><span class=\"n\">BigNum</span><span class=\"w\"> </span><span class=\"n\">implementation</span><span class=\"w\"> </span><span class=\"n\">correct</span><span class=\"bp\">.</span>\n</code></pre></div>\n<p>I asked before about this, apparently this was originally the plan but implementing big integers directly in Lean slows things down too much so compromises were made.</p>",
        "id": 530182452,
        "sender_full_name": "Mr Proof",
        "timestamp": 1753201490
    },
    {
        "content": "<p>I think it's not a compromise. Just a sound engineering decision.</p>",
        "id": 530182558,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201522
    },
    {
        "content": "<p>Lean is not the only language that can be verified.</p>",
        "id": 530182575,
        "sender_full_name": "suhr",
        "timestamp": 1753201528
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"511228\">Huỳnh Trần Khanh</span> <a href=\"#narrow/channel/270676-lean4/topic/Lean.204.20needs.20to.20be.20optimised.20by.20at.20least.2010x.2E.20Ideas.3F/near/530182044\">said</a>:</p>\n<blockquote>\n<p>Everyone and their dog believes in GMP.</p>\n</blockquote>\n<p>Are you going to bet you life on that there's absolutely no bugs in GMP?</p>",
        "id": 530182677,
        "sender_full_name": "suhr",
        "timestamp": 1753201564
    },
    {
        "content": "<p>Taking GMP is a reasonable thing to do, but the work does not end there.</p>",
        "id": 530182936,
        "sender_full_name": "suhr",
        "timestamp": 1753201632
    },
    {
        "content": "<p>I agree. But our community is small. We have to pick things that are worth working on. Formally verifying GMP means... I think GMP is some amount of SIMD heavy. Not sure if there are off the shelf tools that can handle the formal verification of SIMD code.</p>",
        "id": 530183204,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201723
    },
    {
        "content": "<p>Maybe we can hope that in the distant future someone will work on that WASM semantics library in Lean 4...</p>",
        "id": 530183307,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201767
    },
    {
        "content": "<p>WASM covers a lot of ground. Concurrency, memory management, external system interactions, SIMD, etc</p>",
        "id": 530183527,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201845
    },
    {
        "content": "<p>And it's well specified. Oh no I am becoming a WASM shill now. After wearing the pro-AI hat I'm now a pro-WASM person.</p>",
        "id": 530183689,
        "sender_full_name": "(deleted)",
        "timestamp": 1753201896
    },
    {
        "content": "<p>Verifying WASM seems kind of a well suited task for an LLM to do? Since it seems like a lot of it is just translating the WASM specification into Lean language(?)</p>",
        "id": 530184001,
        "sender_full_name": "Mr Proof",
        "timestamp": 1753202003
    },
    {
        "content": "<p>Maybe not in an agentic autonomous way. But LLMs can help accelerate the process.</p>",
        "id": 530184167,
        "sender_full_name": "(deleted)",
        "timestamp": 1753202058
    }
]